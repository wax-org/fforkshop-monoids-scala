Robot Visions - Isaac Asimov

ISAAC ASIMOV
ROBOT VISIONS

ILLUSTRATIONS BY
RALPH McQUARRIE


To Gardner Dozois and Stan Schmidt, colleagues and friends

CONTENTS

Introduction: The Robot Chronicles

STORIES

Robot Visions
Too Bad!
Robbie
Reason
Liar!
Runaround
Evidence
Little Lost Robot
The Evitable Conflict
Feminine Intuition
The Bicentennial Man
Someday
Think!
Segregationist
Mirror Image
Lenny
Galley Slave
Christmas Without Rodney

ESSAYS

Robots I Have Known
The New Teachers
Whatever You Wish
The Friends We Make
Our Intelligent Tools
The Laws Of Robotics
Future Fantastic
The Machine And The Robot
The New Profession
The Robot As Enemy?
Intelligences Together
My Robots
The Laws Of Humanics
Cybernetic Organism
The Sense Of Humor
Robots In Combination


Introduction:
The Robot Chronicles

What is a robot? We might define it most briefly and comprehensively as an artificial object that resembles a human being.
When we think of resemblance, we think of it, first, in terms of appearance. A robot looks like a human being.
It could, for instance, be covered with a soft material that resembles human skin. It could have hair, and eyes, and a voice, and all the features and appurtenances of a human being, so that it would, as far as outward appearance is concerned, be indistinguishable from a human being.
This, however, is not really essential. In fact, the robot, as it appears in science fiction, is almost always constructed of metal, and has only a stylized resemblance to a human being.
Suppose, then, we forget about appearance and consider only what it can do. We think of robots as capable of performing tasks more rapidly or more efficiently than human beings. But in that case any machine is a robot. A sewing machine can sew faster than a human being, a pneumatic drill can penetrate a hard surface faster than an unaided human being can, a television set can detect and organize radio waves as we cannot, and so on.
We must apply the term robot, then, to a machine that is more specialized than an ordinary device. A robot is a computerized machine that is capable of performing tasks of a kind that are too complex for any living mind other than that of a man, and of a kind that no non-computerized machine is capable of performing.
In other words to put it as briefly as possible:

robot = machine + computer

Clearly, then, a true robot was impossible before the invention of the computer in the 1940s, and was not practical (in the sense of being compact enough and cheap enough to be put to everyday use) until the invention of the microchip in the 19708.
Nevertheless, the concept of the robot-an artificial device that mimics the actions and, possibly, the appearance of a human being-is old, probably as old as the human imagination.
The ancients, lacking computers, had to think of some other way of instilling quasi-human abilities into artificial objects, and they made use of vague supernatural forces and depended on god-like abilities beyond the reach of mere men.
Thus, in the eighteenth book of Homers Iliad, Hephaistos, the Greek god of the forge, is described as having for helpers, a couple of maids...made of gold exactly like living girls; they have sense in their heads, they can speak and use their muscles, they can spin and weave and do their work.... Surely, these are robots.
Again, the island of Crete, at the time of its greatest power, was supposed to possess a bronze giant named Talos that ceaselessly patrolled its shores to fight off the approach of any enemy.
Throughout ancient and medieval times, learned men were supposed to have created artificially living things through the secret arts they had learned or uncoveredarts by which they made use of the powers of the divine or the demonic.


The medieval robot-story that is most familiar to us today is that of Rabbi Loew of sixteenth-century Prague. He is supposed to have formed an artificial human beinga robotout of clay, just as God had formed Adam out of clay. A clay object, however much it might resemble a human being, is an unformed substance (the Hebrew word for it is golem), since it lacks the attributes of life. Rabbi Loew, however, gave his golem the attributes of life by making use of the sacred name of God, and set the robot to work protecting the lives of Jews against their persecutors.
There was, however, always a certain nervousness about human beings involving themselves with knowledge that properly belongs to gods or demons. There was the feeling that this was dangerous, that the forces might escape human control. This attitude is most familiar to us in the legend of the sorcerers apprentice, the young fellow who knew enough magic to start a process going but not enough to stop it when it had outlived its usefulness.
The ancients were intelligent enough to see this possibility and be frightened by it. In the Hebrew myth of Adam and Eve, the sin they commit is that of gaining knowledge (eating of the fruit of the tree of knowledge of good and evil; i.e., knowledge of everything) and for that they were ejected from Eden and, according to Christian theologians, infected all of humanity with that original sin.
In the Greek myths, it was the Titan, or Prometheus, who supplied fire (and therefore technology) to human beings and for that he was dreadfully punished by the infuriated Zeus, who was the chief god.

In early modern times, mechanical clocks were perfected, and the small mechanisms that ran them (clockwork)the springs, gears, escapements, ratchets, and so oncould also be used to run other devices.
The 1700s was the golden age of automatons. These were devices that could, given a source of power such as a wound spring or compressed air, carry out a complicated series of activities. Toy soldiers were built that would march; toy ducks that would quack, bathe, drink water, eat grain and void it; toy boys that could dip a pen into ink and write a letter (always the same letter, of course). Such automata were put on display and proved extremely popular (and, sometimes, profitable to the owners).
It was a dead-end sort of thing, of course, but it kept alive the thought of mechanical devices that might do more than clockwork tricks, that might be more nearly alive.
Whats more, science was advancing rapidly, and in 1798, the Italian anatomist, Luigi Galvani, found that under the influence of an electric spark, dead muscles could be made to twitch and contract as though they were alive. Was it possible that electricity was the secret of life?
The thought naturally arose that artificial life could be brought into being by strictly scientific principles rather than by reliance on gods or demons. This thought led to a book that some people consider the first piece of modern science fictionFrankenstein by Mary Shelley, published in 1818.
In this book, Victor Frankenstein, an anatomist, collects fragments of freshly dead bodies and, by the use of new scientific discoveries (not specified in the book), brings the whole to life, creating something that is referred to only as the Monster in the book. (In the movie, the life principle was electricity.)
However, the switch from the supernatural to science did not eliminate the fear of the danger inherent in knowledge. In the medieval legend of Rabbi Loews golem, that monster went out of control and the rabbi had to withdraw the divine name and destroy him. In the modern tale of Frankenstein, the hero was not so lucky. He abandoned the Monster in fear, and the Monster, with an anger that the book all but justifies, in revenge killed those Frankenstein loved and, eventually, Frankenstein himself.
This proved a central theme in the science fiction stories that have appeared since Frankenstein. The creation of robots was looked upon as the prime example of the overweening arrogance of humanity, of its attempt to take on, through misdirected science, the mantle of the divine. The creation of human life, with a soul, was the sole prerogative of God. For a human being to attempt such a creation was to produce a soulless travesty that inevitably became as dangerous as the golem and as the Monster. The fashioning of a robot was, therefore, its own eventual punishment, and the lesson, there are some things that humanity is not meant to know, was preached over and over again.

No one used the word robot, however, until 1920 (the year, coincidentally, in which I was born). In that year, a Czech playwright, Karel Capek, wrote the play R.U.R., about an Englishman, Rossum, who manufactured artificial human beings in quantity. These were intended to do the arduous labor of the world so that real human beings could live lives of leisure and comfort.
Capek called these artificial human beings robots, which is a Czech word for forced workers, or slaves. In fact, the title of the play stands for Rossums Universal Robots, the name of the heros firm.
In this play, however, what I call the Frankenstein complex was made several notches more intense. Where Mary Shelleys Monster destroyed only Frankenstein and his family, Capeks robots were presented as gaining emotion and then, resenting their slavery, wiping out the human species.
The play was produced in 1921 and was sufficiently popular (though when I read it, my purely personal opinion was that it was dreadful) to force the word robot into universal use. The name for an artificial human being is now robot in every language, as far as I know.
Through the 1920s and 1930s, R U.R. helped reinforce the Frankenstein complex, and (with some notable exceptions such as Lester del Reys Helen OLoy and Eando Binders Adam Link series) the hordes of clanking, murderous robots continued to be reproduced in story after story.
I was an ardent science fiction reader in the 1930s and I became tired of the ever-repeated robot plot. I didnt see robots that way. I saw them as machinesadvanced machines but machines. They might be dangerous but surely safety factors would be built in. The safety factors might be faulty or inadequate or might fail under unexpected types of stresses, but such failures could always yield experience that could be used to improve the models.
After all, all devices have their dangers. The discovery of speech introduced communicationand lies. The discovery of fire introduced cookingand arson. The discovery of the compass improved navigationand destroyed civilizations in Mexico and Peru. The automobile is marvelously usefuland kills Americans by the tens of thousands each year. Medical advances have saved lives by the million&and intensified the population explosion.
In every case, the dangers and misuses could be used to demonstrate that there are some things humanity was not meant to know, but surely we cannot be expected to divest ourselves of all knowledge and return to the status of the australopithecines. Even from the theological standpoint, one might argue that God would never have given human beings brains to reason with if He hadnt intended those brains to be used to devise new things, to make wise use of them, to install safety factors to prevent unwise useand to do the best we can within the limitations of our imperfections.
So, in 1939, at the age of nineteen, I determined to write a robot story about a robot that was wisely used, that was not dangerous, and that did the job it was supposed to do. Since I needed a power source I introduced the positronic brain. This was just gobbledygook but it represented some unknown power source that was useful, versatile, speedy, and compactlike the as-yet uninvented computer.
The story was eventually named Robbie, and it did not appear immediately, but I proceeded to write other stories along the same linein consultation with my editor, John W. Campbell, Jr., who was much taken with this idea of mineand eventually, they were all printed.
Campbell urged me to make my ideas as to the robot safeguards explicit rather than implicit, and I did this in my fourth robot story, Runaround, which appeared in the March 1942 issue of Astounding Science Fiction. In that issue, on page 100, in the first column, about one-third of the way down (I just happen to remember) one of my characters says to another, Now, look, lets start with the Three Fundamental Rules of Robotics.
This, as it turned out, was the very first known use of the word robotics in print, a word that is the now-accepted and widely used term for the science and technology of the construction, maintenance, and use of robots. The Oxford English Dictionary, in the 3rd Supplementary Volume, gives me credit for the invention of the word.
I did not know I was inventing the word, of course. In my youthful innocence, I thought that was the word and hadnt the faintest notion it had never been used before.
The three fundamental Rules of Robotics mentioned at this point eventually became known as Asimovs Three Laws of Robotics, and here they are:
1. A robot may not injure a human being, or, through inaction, allow a human being to come to harm.
2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.
3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.
Those laws, as it turned out (and as I could not possibly have foreseen), proved to be the most famous, the most frequently quoted, and the most influential sentences I ever wrote. (And I did it when I was twenty-one, which makes me wonder if Ive done anything since to continue to justify my existence.)
My robot stories turned out to have a great effect on science fiction. I dealt with robots unemotionallythey were produced by engineers, they presented engineering problems that required solutions, and the solutions were found. The stories were rather convincing portrayals of a future technology and were not moral lessons. The robots were machines and not metaphors.
As a result, the old-fashioned robot story was virtually killed in all science fiction stories above the comic-strip level. Robots began to be viewed as machines rather than metaphors by other writers, too. They grew to be commonly seen as benevolent and useful except when something went wrong, and then as capable of correction and improvement. Other writers did not quote the Three Lawsthey tended to be reserved for mebut they assumed them, and so did the readers.
Astonishingly enough, my robot stories also had an important effect on the world outside.
It is well known that the early rocket-experimenters were strongly influenced by the science fiction stories of H. a. Wells. In the same way, early robot-experimenters were strongly influenced by my robot stories, nine of which were collected in 1950 to make up a book called I, Robot. It was my second published book and it has remained in print in the four decades since.
Joseph F. Engelberger, studying at Columbia University in the 1950s, came across I, Robot and was sufficiently attracted by what he read to determine that he was going to devote his life to robots. About that time, he met George C. Devol, Jr., at a cocktail party. Devol was an inventor who was also interested in robots.
Together, they founded the firm of Unimation and set about working out schemes for making robots work. They patented many devices, and by the mid-1970s, they had worked out all kinds of practical robots. The trouble was that they needed computers that were compact and cheapbut once the microchip came in, they had it. From that moment on, Unimation became the foremost robot firm in the world and Engelberger grew rich beyond anything he could have dreamed of.
He has always been kind enough to give me much of the credit. I have met other roboticists such as Marvin Minsky and Shimon Y. Nof, who also admitted, cheerfully, the value of their early reading of my robot stories. Nof, who is an Israeli, had first read I, Robot in a Hebrew translation.
The roboticists take the Three Laws of Robotics seriously and they keep them as an ideal for robot safety. As yet, the types of industrial robots in use are so simple, essentially, that safety devices have to be built in externally. However, robots may confidently be expected to grow more versatile and capable and the Three Laws, or their equivalent, will surely be built in to their programming eventually.
I myself have never actually worked with robots, never even as much as seen one, but I have never stopped thinking about them. I have to date written at least thirty-five short stories and five novels that involve robots, and I dare say that if I am spared, I will write more.
My robot stories and novels seem to have become classics in their own right and, with the advent of the Robot City series of novels, have become the wider literary universe of other writers as well. Under those circumstances, it might be useful to go over my robot stories and describe some of those which I think are particularly significant and to explain why I think they are.
1. Robbie: This is the first robot story I wrote. I turned it out between May 10 and May 22 of 1939, when I was nineteen years old and was just about to graduate from college. I had a little trouble placing it, for John Campbell rejected it and so did Amazing Stories. However, Fred Pohl accepted it on March 25, 1940, and it appeared in the September 1940 issue of Super Science Stories, which he edited. Fred Pohl, being Fred Pohl, changed the title to Strange Playfellow, but I changed it back when I included it in my book I, Robot and it has appeared as Robbie in every subsequent incarnation.
Aside from being my first robot story, Robbie is significant because in it, George Weston says to his wife in defense of a robot that is fulfilling the role of nursemaid, He just cant help being faithful and loving and kind. Hes a machinemade so. This is the first indication, in my first story, of what eventually became the First Law of Robotics, and of the basic fact that robots were made with built-in safety rules.
2. Reason: Robbie would have meant nothing in itself if I had written no more robot stories, particularly since it appeared in one of the minor magazines. However, I wrote a second robot story, Reason, and that one John Campbell liked. After a bit of revision, it appeared in the April 1941 issue of Astounding Science Fiction, and there it attracted notice. Readers became aware that there was such a thing as the positronic robots, and so did Campbell. That made everything afterward possible.
3. Liar!: In the very next issue of Astounding, that of May 1941, my third robot story, Liar! appeared. The importance of this story was that it introduced Susan Calvin, who became the central character in my early robot stories. This story was originally rather clumsily done, largely because it dealt with the relationship between the sexes at a time when I had not yet had my first date with a young lady. Fortunately, Im a quick learner, and it is one story in which I made significant changes before allowing it to appear in I, Robot.
4. Runaround: The next important robot story appeared in the March 1942 issue of Astounding. It was the first story in which I listed the Three Laws of Robotics explicitly instead of making them implicit. In it, I have one character, Gregory Powell, say to another, Michael Donovan, Now, look, lets start with the Three Fundamental Rules of Roboticsthe three rules that are built most deeply into a robots positronic brain. He then recites them.
Later on, I called them the Laws of Robotics, and their importance to me was threefold:
a) They guided me in forming my plots and made it possible to write many short stories, as well as several novels, based on robots. In these, I constantly studied the consequences of the Three Laws.
b) It was by all odds my most famous literary invention, quoted in season and out by others. If all I have written is someday to be forgotten, the Three Laws of Robotics will surely be the last to go.
c) The passage in Runaround quoted above happens to be the very first time the word robotics was used in print in the English language. I am therefore credited, as I have said, with the invention of that word (as well as of robotic, positronic, and psychohistory) by the Oxford English Dictionary, which takes the troubleand the spaceto quote the Three Laws. (All these things were created by my twenty-second birthday and I seem to have created nothing since, which gives rise to grievous thoughts within me.)
5. Evidence: This was the one and only story I wrote while I spent eight months and twenty-six days in the Army. At one point I persuaded a kindly librarian to let me remain in the locked library over lunch so that I could work on the story.It is the first story in which I made use of a humanoid robot. Stephen Byerley, the humanoid robot in question (though in the story I dont make it absolutely clear whether he is a robot or not), represents my first approach toward R. Daneel Olivaw, the humaniform robot who appears in a number of my novels. Evidence appeared in the September 1946 issue of Astounding Science Fiction.
6. Little Lost Robot: My robots tend to be benign entities. In fact, as the stories progressed, they gradually gained in moral and ethical qualities until they far surpassed human beings and, in the case of Daneel, approached the god-like. Nevertheless, I had no intention of limiting myself to robots as saviors. I followed wherever the wild winds of my imagination led me, and I was quite capable of seeing the uncomfortable sides of the robot phenomena.
It was only a few weeks ago (as I write this) that I received a letter from a reader who scolded me because, in a robot story of mine that had just been published, I showed the dangerous side of robots. He accused me of a failure of nerve.
That he was wrong is shown by Little Lost Robot in which a robot is the villain, even though it appeared nearly half a century ago. The seamy side of robots is not the result of a failure in nerve that comes of my advancing age and decrepitude. It has been a constant concern of mine all through my career.
7. The Evitable Conflict: This was a sequel to Evidence and appeared in the June 1950 issue of Astounding. It was the first story I wrote that dealt primarily with computers (I called them Machines in the story) rather than with robots per se. The difference is not a great one. You might define a robot as a computerized machine or as a mobile computer.  You might consider a computer as an immobile robot. In any case, I clearly did not distinguish between the two, and although the Machines, which dont make an actual physical appearance in the story, are clearly computers, I included the story, without hesitation, in my robot collection, I, Robot, and neither the publisher nor the readers objected. To be sure, Stephen Byerley is in the story, but the question of his roboticity plays no role.
8. Franchise: This was the first story in which I dealt with computers as computers, and I had no thought in mind of their being robots. It appeared in the August 1955 issue of It Worlds of Science Fiction, and by that time I had grown familiar with the existence of computers. My computer is Multivac, designed as an obviously larger and more complex version of the actually existing Univac.  In this story, and in some others of the period that dealt with Multivac, I described it as an enormously large machine, missing the chance of predicting the miniaturization and etherealization of computers.
9. The Last Question: My imagination didnt betray me for long, however. In The Last Question, which appeared first in the November 1956 issue of Science Fiction Quarterly, I discussed the miniaturization and etherealization of computers and followed it through a trillion years of evolution (of both computer and man) to a logical conclusion that you will have to read the story to discover. It is, beyond question, my favorite among all the stories I have written in my career.
10. The Feeling of Power: The miniaturization of computers played a small role as a side issue in this story. It appeared in the February 1958 issue of If and is also one of my favorites. In this story I dealt with pocket computers, which were not to make their appearance in the marketplace until ten to fifteen years after the story appeared. Moreover, it was one of the stories in which I foresaw accurately a social implication of technological advance rather than the technological advance itself.
The story deals with the possible loss of ability to do simple arithmetic through the perpetual use of computers. I wrote it as a satire that combined humor with passages of bitter irony, but I wrote more truly than I knew. These days I have a pocket computer and I begrudge the time and effort it would take me to subtract 182 from 854. I use the darned computer. The Feeling of Power is one of the most frequently anthologized of my stories.
In a way, this story shows the negative side of computers, and in this period I also wrote stories that showed the possible vengeful reactions of computers or robots that are mistreated. For computers, there is Someday, which appeared in the August 1956 issue of Infinity Science Fiction, and for robots (in automobile form) see Sally, which appeared in the May-June 1953 issue of Fantastic.
11. Feminine Intuition: My robots are almost always masculine, though not necessarily in an actual sense of gender. After all, I give them masculine names and refer to them as he. At the suggestion of a female editor, JudyLynn del Rey, I wrote Feminine Intuition, which appeared in the October 1969 issue of The Magazine of Fantasy and Science Fiction. It showed, for one thing, that I could do a feminine robot, too. She was still metal, but she had a narrower waistline than my usual robots and had a feminine voice, too. Later on, in my book Robots and Empire, there was a chapter in which a humanoid female robot made her appearance. She played a villainous role, which might surprise those who know of my frequently displayed admiration of the female half of humanity.
12. The Bicentennial Man: This story, which first appeared in 1976 in a paperback anthology of original science fiction, Stellar #2, edited by Judy-Lynn del Rey, was my most thoughtful exposition of the development of robots. It followed them in an entirely different direction from that in The Last Question.  What it dealt with was the desire of a robot to become a man and the way in which he carried out that desire, step by step. Again, I carried the plot all the way to its logical conclusion. I had no intention of writing this story when I started it. It wrote itself, and turned and twisted in the typewriter. It ended as the third favorite of mine among all my stories. Ahead of it come only The Last Question, mentioned above, and The Ugly Little Boy, which is not a robot story.
13. The Caves of Steel: Meanwhile, at the suggestion of Horace L. Gold, editor of Galaxy, I had written a robot novel. I had resisted doing so at first for I felt that my robot ideas only fit the short story length. Gold, however, suggested I write a murder mystery dealing with a robot detective. I followed the suggestion partway. My detective was a thoroughly human Elijah Baley (perhaps the most attractive character I ever invented, in my opinion), but he had a robot sidekick, R. Daneel Olivaw. The book, I felt, was the perfect fusion of mystery and science fiction. It appeared as a three-part serial in the October, November, and December 1953 issues of Galaxy, and Doubleday published it as a novel in 1954.
What surprised me about the book was the reaction of the readers. While they approved of Lije Baley, their obvious interest was entirely with Daneel, whom I had viewed as a mere subsidiary character. The approval was particularly intense in the case of the women who wrote to me. (Thirteen years after I had invented Daneel, the television series Star Trek came out, with Mr. Spock resembling Daneel quite closely in charactersomething which did not bother meand I noticed that women viewers were particularly interested in him, too. I wont pretend to analyze this.)
14. The Naked Sun: The popularity of Lije and Daneel led me to write a sequel, The Naked Sun, which appeared as a three-part serial in the October, November, and December 1956 issues of Astounding and was published as a novel by Doubleday in 1957. Naturally, the repetition of the success made a third novel seem the logical thing to do. I even started writing it in 1958, but things got in the way and, what with one thing and another, it didnt get written till 1983.
15. The Robots of Dawn: This, the third novel of the Lije Baley/R. Daneel series, was published by Doubleday in 1983. In it, I introduced a second robot, R. Giskard Reventlov, and this time I was not surprised when he turned out to be as popular as Daneel.
16. Robots and Empire: When it was necessary to allow Lije Baley to die (of old age), I felt I would have no problem in doing a fourth book in the series, provided I allowed Daneel to live. The fourth book, Robots and Empire, was published by Doubleday in 1985. Lijes death brought some reaction, but nothing at all compared to the storm of regretful letters I received when the exigencies of the plot made it necessary for R. Giskard to die.
Of the short stories I have listed as notable you may have noticed that threeFranchise, The Last Question, and The Feeling of Powerare not included in the collection you are now holding. This is not an oversight, nor is it any indication that they are not suitable for collection. The fact is that each of the three is to be found in an earlier collection, Robot Dreams, that is a companion piece for this one. It wouldnt be fair to the reader to have these stories in both collections.
To make up for that, I have included in Robot Visions nine robot stories that are not listed above as notable. This in no way implies that these nine stories are inferior, merely that they broke no new ground.
Of these nine stories, Galley Slave is one of my favorites, not only because of the word-play in the title, but because it deals with a job I earnestly wish a robot would take off my hands. Not many people have gone through more sets of galleys than I have.
Lenny shows a human side of Susan Calvin that appears in no other story, while Someday is my foray into pathos. Christmas Without Rodney is a humorous robot story, while Think! is a rather grim one. Mirror Image is the only short story I ever wrote that involves R. Daneel Olivaw, the co-hero of my robot novels. Too Bad! and Segregationist are both robot stories based on medical themes. And, finally, Robot Visions is written specifically for this collection.
So it turns out that my robot stories have been almost as successful as my Foundation books, and if you want to know the truth (in a whisper, of course, and please keep this confidential) I like my robot stories better.
Finally, a word about the essays in this book. The first essay was written in 1956. All the others have appeared in 1974 and thereafter. Why the eighteen-year gap?
Easy. I wrote my first robot story when I was nineteen, and I wrote them, on and off, for over thirty years without really believing that robots would ever come into existence in any real senseat least not in my lifetime. The result was that I never once wrote a serious essay on robotics. I might as well expect myself to have written serious essays on Galactic empires and psychohistory. In fact, my 1956 piece is not a serious discussion of robotics but merely a consideration of the use of robots in science fiction.
It was not till the mid-1970s, with the development of the microchip, that computers grew small enough, versatile enough, and cheap enough to allow computerized machinery to become practical for industrial use. Thus, the industrial robot arrivedextremely simple compared to my imaginary robots, but clearly en route.
And, as it happened, in 1974, just as robots were becoming real, I began to write essays on current developments in science, first for American Way magazine and then for the Los Angeles Times Syndicate. It became natural to write an occasional piece on real robotics. In addition, Byron Preiss Visual Publications, Inc., began to put out a remarkable series of books under the general title of Isaac Asimovs Robot City, and I was asked to do essays on robotics for each of them. So it came about that before 1974, I wrote virtually no essays on robotics, and after 1974 quite a few. Its not my fault, after all, if science finally catches up to my simpler notions.*


*You are now ready to plunge into the book itself. Please remember that the stories, written at different times over a period of half a century, may be mutually inconsistent here and there. As for the concluding essayswritten at different times for different outletsthey are repetitious here and there. Please forgive me in each case.


Robot Visions

I suppose I should start by telling you who I am. I am a very junior member of the Temporal Group. The Temporalists (for those of you who have been too busy trying to survive in this harsh world of 2030 to pay much attention to the advance of technology) are the aristocrats of physics these days.
They deal with that most intractable of problemsthat of moving through time at a speed different from the steady temporal progress of the Universe. In short, they are trying to develop time-travel.
And what am I doing with these people, when I myself am not even a physicist, but merely a? Well, merely a merely.
Despite my lack of qualification, it was actually a remark I made some time before that inspired the Temporalists to work out the concept of VPIT (virtual paths in time).
You see, one of the difficulties in traveling through time is that your base does not stay in one place relative to the Universe as a whole. The Earth is moving about the Sun; the Sun about the Galactic center; the Galaxy about the center of gravity of the Local Groupwell, you get the idea. If you move one day into the future or the pastjust one dayEarth has moved some 2.5 million kilometers in its orbit about the Sun. And the Sun has moved in its journey, carrying Earth with it, and so has everything else.
Therefore, you must move through space as well as through time, and it was my remark that led to a line of argument that showed that this was possible; that one could travel with the space-time motion of the Earth not in a literal, but in a virtual way that would enable a time-traveler to remain with his base on Earth wherever he went in time. It would be useless for me to try to explain that mathematically if you have not had Temporalist training. Just accept the matter.
It was also a remark of mine that led the Temporalists to develop a line of reasoning that showed that travel into the past was impossible. Key terms in the equations would have to rise beyond infinity when the temporal signs were changed.
It made sense. It was clear that a trip into the past would be sure to change events there at least slightly, and no matter how slight a change might be introduced into the past, it would alter the present; very likely drastically. Since the past should seem fixed, it makes sense that travel back in time is impossible.
The future, however, is not fixed, so that travel into the future and back again from it would be possible.
I was not particularly rewarded for my remarks. I imagine the Temporalist team assumed I had been fortunate in my speculations and it was they who were entirely the clever ones in picking up what I had said and carrying it through to useful conclusions. I did not resent that, considering the circumstances, but was merely very gladdelighted, in factsince because of that (I think) they allowed me to continue to work with them and to be part of the project, even though I was merely awell, merely.
Naturally, it took years to work out a practical device for time travel, even after the theory was established, but I dont intend to write a serious treatise on Temporality. It is my intention to write of only certain parts of the project, and to do so for only the future inhabitants of the planet, and not for our contemporaries.
Even after inanimate objects had been sent into the futureand then animalswe were not satisfied. All objects disappeared; all, it seemed, traveled into the future. When we sent them short distances into the futurefive minutes or five daysthey eventually appeared again, seemingly unharmed, unchanged, and, if alive to begin with, still alive and in good health.
But what was wanted was to send something far into the future and bring it back.
Wed have to send it at least two hundred years into the future, said one Temporalist. The important point is to see what the future is like and to have the vision reported back to us. We have to know whether humanity will survive and under what conditions, and two hundred years should be long enough to be sure. Frankly, I think the chances of survival are poor. Living conditions and the environment about us have deteriorated badly over the last century.
(There is no use in trying to describe which Temporalist said what. There were a couple of dozen of them altogether, and it makes no difference to the tale I am telling as to which one spoke at anyone time, even if I were sure I could remember which one said what. Therefore, I shall simply say said a Temporalist, or one said, or some of them said, or another said, and I assure you it will all be sufficiently clear to you. Naturally, I shall specify my own statements and that of one other, but you will see that those exceptions are essential.)
Another Temporalist said rather gloomily, I dont think I want to know the future, if it means finding out that the human race is to be wiped out or that it will exist only as miserable remnants.
Why not? said another. We can find out in shorter trips exactly what happened and then do our best to so act, out of our special knowledge, as to change the future in a preferred direction. The future, unlike the past, is not fixed.
But then the question arose as to who was to go. It was clear that the Temporalists each felt himself or herself to be just a bit too valuable to risk on a technique that might not yet be perfected despite the success of experiments on objects that were not alive; or, if alive, objects that lacked a brain of the incredible complexity that a human being owned. The brain might survive, but, perhaps, not quite all its complexity might.
I realized that of them all I was least valuable and might be considered the logical candidate. Indeed, I was on the point of raising my hand as a volunteer, but my facial expression must have given me away for one of the Temporalists said, rather impatiently, Not you. Even you are too valuable. (Not very complimentary.) The thing to do, he went on, is to send RG-32.
That did make sense. RG-32 was a rather old-fashioned robot, eminently replaceable. He could observe and reportperhaps without quite the ingenuity and penetration of a human beingbut well enough. He would be without fear, intent only on following his orders, and he could be expected to tell the truth.
Perfect!
I was rather surprised at myself for not seeing that from the start, and for foolishly considering volunteering myself. Perhaps, I thought, I had some sort of instinctive feeling that I ought to put myself into a position where I could serve the others. In any case, it was RG-32 that was the logical choice; indeed, the only one.
In some ways, it was not difficult to explain what we needed. Archie (it was customary to call a robot by some common perversion of his serial number) did not ask for reasons, or for guarantees of his safety. He would accept any order he was capable of understanding and following, with the same lack of emotionality that he would display if asked to raise his hand. He would have to, being a robot.
The details took time, however.
Once you are in the future, one of the senior Temporalists said, you may stay for as long as you feel you can make useful observations. When you are through, you will return to your machine and come back with it to the very moment that you left by adjusting the controls in a manner which we will explain to you. You will leave and to us it will seem that you will be back a split-second later, even though to yourself it may have seemed that you had spent a week in the future, or five years. Naturally, you will have to make sure the machine is stored in a safe place while you are gone, which should not be difficult since it is quite light. And you will have to remember where you stored the machine and how to get back to it.
What made the briefing even longer lay in the fact that one Temporalist after another would remember a new difficulty. Thus, one of them said suddenly, How much do you think the language will have changed in two centuries?
Naturally, there was no answer to that and a great debate grew as to whether there might be no chance of communication whatever, that Archie would neither understand nor make himself understood.
Finally, one Temporalist said, rather curtly, See here, the English language has been becoming ever more nearly universal for several centuries and that is sure to continue for two more. Nor has it changed significantly in the last two hundred years, so why should it do so in the next two hundred? Even if it has, there are bound to be scholars who would be able to speak what they might call ancient English.  And even if there were not, Archie would still be able to make useful observations. Determining whether a functioning society exists does not necessarily require talk.
Other problems arose. What if he found himself facing hostility? What if the people of the future found and destroyed the machine, either out of malevolence or ignorance?
One Temporalist said, It might be wise to design a Temporal engine so miniaturized that it could be carried in ones clothing. Under such conditions one could at any time leave a dangerous position very quickly.
Even if it were possible at all, snapped another, it would probably take so long to design so miniaturized a machine that weor rather our successorswould reach a time two centuries hence without the necessity of using a machine at all. No, if an accident of some sort takes place, Archie simply wont return and well just have to try again.


This was said with Archie present, but that didnt matter, of course. Archie could contemplate being marooned in time, or even his own destruction, with equanimity, provided he were following orders. The Second Law of Robotics, which makes it necessary for a robot to follow orders, takes precedence over the Third, which makes it necessary for him to protect his own existence.
In the end, of course, all had been said, and no one could any longer think of a warning, or an objection, or a possibility that had not been thoroughly aired.
Archie repeated all he had been told with robotic calmness and precision, and the next step was to teach him how to use the machine. And he learned that, too, with robotic calmness and precision.
You must understand that the general public did not know, at that time, that time-travel was being investigated. It was not an expensive project as long as it was a matter of working on theory, but experimental work had punished the budget and was bound to punish it still more. This was most uncomfortable for scientists engaged in an endeavor that seemed totally blue-sky.
If there was a large failure, given the state of the public purse, there would be a loud outcry on the part of the people, and the project might be doomed. The Temporalists all agreed, without even the necessity of debate, that only success could be reported, and that until such a success was recorded, the public would have to learn very little, if anything at all. And so this experiment, the crucial one, was heart-stopping for everyone.
We gathered at an isolated spot of the semi-desert, an artfully protected area given over to Project Four. (Even the name was intended to give no real hint of the nature of the work, but it always struck me that most people thought of time as a kind of fourth dimension and that someone ought therefore guess what we were doing. Yet no one ever did, to my knowledge.)
Then, at a certain moment, at which time there was a great deal of breath-holding, Archie, inside the machine, raised one hand to signify he was about to make his move. Half a breath laterif anyone had been breathingthe machine flickered.
It was a very rapid flicker. I wasnt sure that I had observed it. It seemed to me that I had merely assumed it ought to flicker, if it returned to nearly the instant at which it leftand I saw what I was convinced I ought to see. I meant to ask the others if they, too, had seen a flicker, but I always hesitated to address them unless they spoke to me first. They were very important people, and I was merelybut Ive said that. Then, too, in the excitement of questioning Archie, I forgot the matter of the flicker. It wasnt at all important.
So brief an interval was there between leaving and returning that we might well have thought that he hadnt left at all, but there was no question of that. The machine had definitely deteriorated. It had simply faded.
Nor was Archie, on emerging from the machine, much better off. He was not the same Archie that had entered that machine. There was a shopworn look about him, a dullness to his finish, a slight unevenness to his surface where he might have undergone collisions, an odd manner in the way he looked about as though he were re-experiencing an almost forgotten scene. I doubt that there was a single person there who felt for one moment that Archie had not been absent, as far as his own sensation of time was concerned, for a long interval.
In fact, the first question he was asked was, How long have you been away?
Archie said, Five years, sir. It was a time interval that had been mentioned in my instructions and I wished to do a thorough job.
Come, thats a hopeful fact, said one Temporalist. If the world were a mass of destruction, surely it would not have taken five years to gather that fact.
And yet not one of them dared say: well, Archie, was the Earth a mass of destruction?
They waited for him to speak, and for a while, he also waited, with robotic politeness, for them to ask. After a while, however, Archies need to obey orders, by reporting his observations, overcame whatever there was in his positronic circuits that made it necessary for him to seem polite.
Archie said,  All was well on the Earth of the future. The social structure was intact and working well.
Intact and working well? said one Temporalist, acting as though he were shocked at so heretical a notion. Everywhere?
The inhabitants of the world were most kind. They took me to every part of the globe. All was prosperous and peaceful.
The Temporalists looked at each other. It seemed easier for them to believe that Archie was wrong, or mistaken, than that the Earth of the future was prosperous and peaceful. It had seemed to me always that, despite all optimistic statements to the contrary, it was taken almost as an article of faith, that Earth was on the point of social, economic, and, perhaps, even physical destruction.
They began to question him thoroughly. One shouted, What about the forests? Theyre almost gone.
There was a huge project, said Archie, for the reforestation of the land, sir. Wilderness has been restored where possible. Genetic engineering has been used imaginatively to restore wildlife where related species existed in zoos or as pets. Pollution is a thing of the past. The world of 2230 is a world of natural peace and beauty.
You are sure of all this? asked a Temporalist.
No spot on Earth was kept secret. I was shown all I asked to see.
Another Temporalist said, with sudden severity,  Archie, listen to me. It may be that you have seen a ruined Earth, but hesitate to tell us this for fear we will be driven to despair and suicide. In your eagerness to do us no harm, you may be lying to us. This must not happen, Archie. You must tell us the truth.
Archie said, calmly, I am telling the truth, sir. If I were lying, no matter what my motive for it might be, my positronic potentials would be in an abnormal state. That could be tested.
Hes right there, muttered a Temporalist.
He was tested on the spot. He was not allowed to say another word while this was done. I watched with interest while the potentiometers recorded their findings, which were then analyzed by computer. There was no question about it. Archie was perfectly normal. He could not be lying.
He was then questioned again. What about the cities?
There are no cities of our kind, sir. Life is much more decentralized in 2230 than with us, in the sense that there are no large and concentrated clumps of humanity. On the other hand, there is so intricate a communication network that humanity is all one loose clump, so to speak.
And space? Has space exploration been renewed?
Archie said, The Moon is quite well developed, sir. It is an inhabited world. There are space settlements in orbit about the Earth and about Mars. There are settlements being carved out in the asteroid belt.
You were told all this? asked one Temporalist, suspiciously.
This is not a matter of hearsay, sir. I have been in space. I remained on the Moon for two months. I lived on a space settlement about Mars for a month, and visited both Phobos and Mars itself. There is some hesitation about colonizing Mars. There are opinions that it should be seeded with lower forms of life and left to itself without the intervention of the Earthpeople. I did not actually visit the asteroid belt.
One Temporalist said, Why do you suppose they were so nice to you, Archie? So cooperative?
I received the impression, sir, said Archie, that they had some notion I might be arriving. A distant rumor. A vague belief. They seemed to have been waiting for me.
Did they say they had expected you to arrive? Did they say there were records that we had sent you forward in time?
No, sir.
Did you ask them about it?
Yes, sir. It was impolite to do so but I had been ordered carefully to observe everything I could, so I had to ask thembut they refused to tell me.
Another Temporalist put in, Were there many other things they refused to tell you?
A number, sir.
One Temporalist stroked his chin thoughtfully at this point and said, Then there must be something wrong about all this. What is the population of the Earth in 2230, Archie? Did they tell you that?
Yes, sir. I asked. There are just under a billion people on Earth in 2230. There are 150 million in space. The numbers on Earth are stable. Those in space are growing.
Ah,  said a Temporalist, but there are nearly ten billion people on Earth now, with half of them in serious misery. How did these people of the future get rid of nearly nine billion?
I asked them that, sir. They said it was a sad time.
A sad time?
Yes, sir.
In what way?
They did not say, sir. They simply said it was a sad time and would say no more.
One Temporalist who was of African origin said coldly, What kind of people did you see in 2230?
What kind, sir?
Skin color? Shape of eyes?
Archie said, It was in 2230 as it is today, sir. There were different kinds; different shades of skin color, hair form, and so on. The average height seemed greater than it is today, though I did not study the statistics. The people seemed younger, stronger, healthier. In fact, I saw no undernourishment, no obesity, no illnessbut there was a rich variety of appearances.
No genocide, then?
No signs of it, sir, said Archie. He went on, There were also no signs of crime or war or repression.
Well, said one Temporalist, in a tone as though he were reconciling himself, with difficulty, to good news, it seems like a happy ending.
A happy ending, perhaps, said another, but its almost too good to accept. Its like a return of Eden. What was done, or will be done, to bring it about? I dont like that sad time. 
Of course, said a third, theres no need for us to sit about and speculate. We can send Archie one hundred years into the future, fifty years into the future. We can find out, for what its worth, just what happened; I mean, just what will happen.
I dont think so, sir, said Archie. They told me quite specifically and carefully that there are no records of anyone from the past having arrived earlier than their own timethe day I arrived. It was their opinion that if any further investigations were made of the time period between now and the time I arrived, that the future would be changed.
There was almost a sickening silence. Archie was sent away and cautioned to keep everything firmly in mind for further questioning. I half expected them to send me away, too, since I was the only person there without an advanced degree in Temporal Engineering, but they must have grown accustomed to me, and I, of course, didnt suggest on my own that I leave.
The point is, said one Temporalist, that it is a happy ending. Anything we do from this point on might spoil it. They were expecting Archie to arrive; they were expecting him to report; they didnt tell him anything they didnt want him to report; so were still safe. Things will develop as they have been.
It may even be, said another, hopefully, that the knowledge of Archies arrival and the report they sent him back to make helped develop the happy ending.
Perhaps, but if we do anything else, we may spoil things. I prefer not to think about the sad time they speak of, but if we try something now, that sad time may still come and be even worse than it wasor will beand the happy ending wont develop, either. I think we have no choice but to abandon Temporal experiments and not talk about them, either. Announce failure.
That would be unbearable.
Its the only safe thing to do.
Wait, said one. They knew Archie was coming, so there must have been a report that the experiments were successful. We dont have to make failures of ourselves.
I dont think so, said still another. They heard rumors; they had a distant notion. It was that sort of thing, according to Archie. I presume there may be leaks, but surely not an outright announcement.
And that was how it was decided. For days, they thought, and occasionally discussed the matter, but with greater and greater trepidation. I could see the result coming with inexorable certainty. I contributed nothing to the discussion, of coursethey scarcely seemed to know I was therebut there was no mistaking the gathering apprehension in their voices. Like those biologists in the very early days of genetic engineering who voted to limit and hedge in their experiments for fear that some new plague might be inadvertently loosed on unsuspecting humanity, the Temporalists decided, in terror, that the Future must not be tampered with or even searched.
It was enough, they said, that they now knew there would be a good and wholesome society, two centuries hence. They must not inquire further, they dared not interfere by the thickness of a fingernail, lest they ruin all. And they retreated into theory only.
One Temporalist sounded the final retreat. He said, Someday, humanity will grow wise enough, and develop ways of handling the future that are subtle enough to risk observation and perhaps even manipulation along the course of time, but the moment for that has not yet come. It is still long in the future. And there had been a whisper of applause.
Who was I, less than any of those engaged in Project Four, that I should disagree and go my own way? Perhaps it was the courage I gained in being so much less than they werethe valor of the insufficiently advanced. I had not had initiative beaten out of me by too much specialization or by too long a life of too deep thought.
At any rate, I spoke to Archie a few days later, when my own work assignments left me some free time. Archie knew nothing about training or about academic distinctions. To him, I was a man and a master, like any other man and master, and he spoke to me as such.
I said to him, How did these people of the future regard the people of their past? Were they censorious? Did they blame them for their follies and stupidities?
Archie said, They did not say anything to make me feel this, sir. They were amused by the simplicity of my construction and by my existence, and it seemed to me they smiled at me and at the people who constructed me, in a good-humored way. They themselves had no robots.
No robots at all, Archie?
They said there was nothing comparable to myself, sir. They said they needed no metal caricatures of humanity.
And you didnt see any?
None, sir. In all my time there, I saw not one.
I thought about that a while, then said, What did they think of other aspects of our society?
I think they admired the past in many ways, sir. They showed me museums dedicated to what they called the period of unrestrained growth.  Whole cities had been turned into museums.
You said there were no cities in the world of two centuries hence, Archie. No cities in our sense.
It was not their cities that were museums, sir, but the relics of ours. AU of Manhattan Island was a museum, carefully preserved and restored to the period of its peak greatness. I was taken through it with several guides for hours, because they wanted to ask me questions about authenticity. I could help them very little, for I have never been to Manhattan. They seemed proud of Manhattan. There were other preserved cities, too, as well as carefully preserved machinery of the past, libraries of printed books, displays of past fashions in clothing, furniture, and other minutiae of daily life, and so on. They said that the people of our time had not been wise but they had created a firm base for future advance.
And did you see young people? Very young people, I mean. Infants?
No, sir.
Did they talk about any?
No, sir.
I said, Very well, Archie. Now, listen to me
If there was one thing I understood better than the Temporalists, it was robots. Robots were simply black boxes to them, to be ordered about, and to be left to maintenance menor discardedif they went wrong. I, however, understood the positronic circuitry of robots quite well, and I could handle Archie in ways my colleagues would never suspect. And I did.
I was quite sure the Temporalists would not question him again, out of their newfound dread of interfering with time, but if they did, he would not tell them those things I felt they ought not to know. And Archie himself would not know that there was anything he was not telling them.
I spent some time thinking about it, and I grew more and more certain in my mind as to what had happened in the course of the next two centuries.
You see, it was a mistake to send Archie. He was a primitive robot, and to him people were people. He did notcould notdifferentiate. It did not surprise him that human beings had grown so civilized and humane. His circuitry forced him, in any case, to view all human beings as civilized and human; even as god-like, to use an old-fashioned phrase.
The Temporalists themselves, being human, were surprised and even a bit incredulous at the robot vision presented by Archie, one in which human beings had grown so noble and good. But, being human, the Temporalists wanted to believe what they heard and forced themselves to do so against their own common sense.
I, in my way, was more intelligent than the Temporalists, or perhaps merely more clear-eyed.
I asked myself if population decreased from ten billion to one billion in the course of two centuries, why did it not decrease from ten billion to zero? There would be so little difference between the two alternatives.
Who were the billion who survived? They were stronger than the other nine billion, perhaps? More enduring? More resistant to privation? And they were also more sensible, more rational, and more virtuous than the nine billion who died as was quite clear from Archies picture of the world of two hundred years hence.
In short, then, were they human at all?
They smiled at Archie in mild derision and boasted that they had no robots; that they needed no metal caricatures of humanity.
What if they had organic duplicates of humanity instead? What if they had humaniform robots; robots so like human beings as to be indistinguishable from them, at least to the eyes and senses of a robot like Archie? What if the people of the future were humaniform robots, all of them, robots that had survived some overwhelming catastrophe that human beings had not?
There were no babies. Archie had seen none. To be sure, population was stable and long-lived on Earth, so there would be few babies in any case. Those few would be taken care of, made much of, be well-guarded, and might not be distributed carelessly through society. But Archie had been on the Moon for two months and population there was growingand he had still seen no babies.
Perhaps these people of the future were constructed rather than born.
And perhaps this was a good thing. If human beings had died out through their own rages, hatreds, and stupidities, they had at least left behind a worthy successor; a kind of intelligent being that valued the past, preserved it, and moved on into the future, doing their best to fulfill the aspirations of humanity, in building a better, kinder world and in moving out into space perhaps more efficiently than we real human beings would have.
How many intelligent beings in the Universe had died out leaving no successor? Perhaps we were the first ever to leave such a legacy.
We had a right to feel proud.
Ought I to tell all this to the world? Or even to the Temporalists? I did not consider that for a moment.
For one thing, they were likely not to believe me. For another, if they did believe me, in their rage at the thought of being replaced by robots of any kind, would they not turn on them and destroy every robot in the world and refuse ever to build others? This would mean that Archies vision of the future, and my own vision, would never come to pass. That, however, would not stop the conditions that were to destroy humanity. It would just prevent a replacement; stop another group of beings, made by humans and honoring humans, from carrying human aspirations and dreams through all the Universe.
I did not want that to happen. I wanted to make sure Archies vision, and my own improvement of it, would come to pass.
I am writing this, therefore, and I will see to it that it will be hidden, and kept safe, so that it will be opened only two hundred years from now, a little sooner than the time at which Archie will arrive. Let the humaniform robots know that they should treat him well and send him home safely, carrying with him only the information that would cause the Temporalists to decide to interfere with Time no more, so that the future can develop in its own tragic/happy way.
And what makes me so sure I am right? Because I am in a unique position to know that I am.
I have said several times that I am inferior to the Temporalists. At least I am inferior to them in their eyes, though this very inferiority makes me more clear-eyed in certain respects, as I have said before, and gives me a better understanding of robots, as I have also said before.
Because, you see, I, too, am a robot.
I am the first humaniform robot, and it is on me and on those of my kind that are yet to be constructed that the future of humanity depends.


Too Bad!

THE THREE LAWS OF ROBOTICS
1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.
2. A robot must obey the orders given it by human beings except where that would conflict with the First Law.
3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

Gregory Arnfeld was not actually dying, but certainly there was a sharp limit to how long he might live. He had inoperable cancer and he had refused, strenuously, all suggestions of chemical treatment or of radiation therapy.
He smiled at his wife as he lay propped up against the pillows and said, Im the perfect case. Tertia and Mike will handle it.
Tertia did not smile. She looked dreadfully concerned. There are so many things that can be done, Gregory. Surely Mike is a last resort. You may not need it.
No, no. By the time theyre done drenching me with chemicals and dowsing me with radiation, I would be so far gone that it wouldnt be a reasonable test....And please dont call Mike it.
This is the twenty-second century, Greg. There are so many ways of handling cancer.
Yes, but Mike is one of them, and I think the best. This is the twenty-second century, and we know what robots can do. Certainly, I know. I had more to do with Mike than anyone else. You know that.
But you cant want to use him just out of pride of design. Besides, how certain are you of miniaturization? Thats an even newer technique than robotics.
Arnfeld nodded. Granted, Tertia. But the miniaturization boys seem confident. They can reduce or restore Plancks constant in what they say is a reasonably foolproof manner, and the controls that make that possible are built into Mike. He can make himself smaller or larger at win without affecting his surroundings.
Reasonably foolproof, said Tertia with soft bitterness.
Thats an anyone can ask for, surely. Think of it, Tertia. I am privileged to be part of the experiment. Ill go down in history as the principal designer of Mike, but that will be secondary. My greatest feat will be that of having been successfully treated by a minirobotby my own choice, by my own initiative.
You know its dangerous.
Theres danger to everything. Chemicals and radiation have their side effects. They can slow without stopping. They can allow me to live a wearying sort of half-life. And doing nothing win certainly kin me. If Mike does his job properly, I shall be completely healthy, and if it recurs Arnfeld smiled joyouslyMike can recur as well.
He put out his hand to grasp hers. Tertia, weve known this was coming, you and I. Lets make something out of thisa glorious experiment. Even if it failsand it wont failit will be a glorious experiment.
Louis Secundo, of the miniaturization group, said, No, Mrs. Arnfeld. We cant guarantee success. Miniaturization is intimately involved with quantum mechanics, and there is a strong element of the unpredictable there. As MIK-27 reduces his size, there is always the chance that a sudden unplanned reexpansion will take place, naturally killing thethe patient. The greater the reduction in size, the tinier the robot becomes, the greater the chance of reexpansion. And once he starts expanding again, the chance of a sudden accelerated burst is even higher. The reexpansion is the really dangerous part.
Tertia shook her head. Do you think it will happen?
The chances are it wont, Mrs. Arnfeld. But the chance is never zero. You must understand that.
Does Dr. Arnfeld understand that?
Certainly. We have discussed this in detail. He feels that the circumstances warrant the risk.  He hesitated. So do we. I know that youll see were not all running the risk, but a few of us will be, and we nevertheless feel the experiment to be worthwhile. More important, Dr. Arnfeld does.
What if Mike makes a mistake or reduces himself too far because of a glitch in the mechanism? Then reexpansion would be certain, wouldnt it?
It never becomes quite certain. It remains statistical. The chances improve if he gets too small. But then the smaller he gets, the less massive he is, and at some critical point, mass will become so insignificant that the least effort on his part will send him flying off at nearly the speed of light.
Well, wont that kill the doctor?
No. By that time, Mike would be so small he would slip between the atoms of the doctors body without affecting them.
But how likely would it be that he would reexpand when hes that small?
When MIK-27 approaches neutrino size, so to speak, his half-life would be in the neighborhood of seconds. That is, the chances are fifty-fifty that he would reexpand within seconds, but by the time he reexpanded, he would be a hundred thousand miles away in outer space and the explosion that resulted would merely produce a small burst of gamma rays for the astronomers to puzzle over. Still, none of that will happen. MIK-27 will have his instructions and he will reduce himself to no smaller than he will need to be to carry out his mission.

Mrs. Arnfeld knew she would have to face the press one way or another. She had adamantly refused to appear on holovision, and the right-to-privacy provision of the World Charter protected her. On the other hand, she could not refuse to answer questions on a voice-over basis. The right-to-know provision would not allow a blanket blackout.
She sat stiffly, while the young woman facing her said,  Aside from all that, Mrs. Arnfeld, isnt it a rather weird coincidence that your husband, chief designer of Mike the Microbot, should also be its first patient?
Not at all, Miss Roth, said Mrs. Arnfeld wearily. The doctors condition is the result of a predisposition. There have been others in his family who have had it. He told me of it when we married, so I was in no way deceived in the matter, and it was for that reason. that we have had no children. It is also for that reason that my husband chose his lifework and labored so assiduously to produce a robot capable of miniaturization. He always felt he would be its patient eventually, you see.

Mrs. Arnfeld insisted on interviewing Mike and, under the circumstances, that could not be denied. Ben Johannes, who had worked with her husband for five years and whom she know well enough to be on first-name terms with, brought her into the robots quarters.
Mrs. Arnfeld had seen Mike soon after his construction, when he was being put through his primary tests, and he remembered her. He said, in his curiously neutral voice, too smoothly average to be quite human, I am pleased to see you, Mrs. Arnfeld.
He was not a well-shaped robot. He looked pinheaded and very bottom heavy. He was almost conical, point upward. Mrs. Arnfeld knew that was because his miniaturization mechanism was bulky and abdominal and because his brain had to be abdominal as well in order to increase the speed of response. It was an unnecessary anthropomorphism to insist on a brain behind a tall cranium, her husband had explained. Yet it made Mike seem ridiculous, almost moronic. There were psychological advantages to anthropomorphism, Mrs. Arnfeld thought, uneasily.
Are you sure you understand your task, Mike? said Mrs. Arnfeld.
Completely, Mrs. Arnfeld, said Mike. I will see to it that every vestige of cancer is removed.
Johannes said, Im not sure if Gregory explained it, but Mike can easily recognize a cancer cell when he is at the proper size. The difference is unmistakable, and he can quickly destroy the nucleus of any cell that is not normal.
I am laser equipped, Mrs. Arnfeld, said Mike, with an odd air of unexpressed pride.
Yes, but there are millions of cancer cells all over. It would take how long to get them, one by one?
Not quite necessarily one by one, Tertia, said Johannes. Even though the cancer is widespread, it exists in clumps. Mike is equipped to burn off and close capillaries leading to the clump, and a million cells could die at a stroke in that fashion. He will only occasionally have to deal with cells on an individual basis.
Still, how long would it take?
Johanness youngish face went into a grimace as though it were difficult to decide what to say. It could take hours, Tertia, if were to do a thorough job. I admit that.
And every moment of those hours will increase the chance of reexpansion.
Mike said, Mrs. Arnfeld, I will labor to prevent reexpansion.
Mrs. Arnfeld turned to the robot and said earnestly, Can you, Mike? I mean, is it possible for you to prevent it?

Not entirely, Mrs. Arnfeld. By monitoring my size and making an effort to keep it constant, I can minimize the random changes that might lead to a reexpansion. Naturally, it is almost impossible to do this when I am actually reexpanding under controlled conditions.
Yes, I know. My husband has told me that reexpansion is the most dangerous time. But you will try, Mike? Please?
The laws of robotics ensure that I will, Mrs. Arnfeld, said Mike solemnly.
As they left, Johannes said in what Mrs. Arnfeld understood to be an attempt at reassurance, Really, Tertia, we have a holo-sonogram and a detailed cat scan of the area. Mike knows the precise location of every significant cancerous lesion. Most of his time will be spent searching for small lesions undetectable by instruments, but that cant be helped. We must get them all, if we can, you see, and that takes time. Mike is strictly instructed, however, as to how small to get, and he will get no smaller, you can be sure. A robot must obey orders.
And the reexpansion, Ben?
There, Tertia, were in the lap of the quanta. There is no way of predicting, but there is a more than reasonable chance that he will get out without trouble. Naturally, we will have him reexpand within Gregorys body as little as possiblejust enough to make us reasonably certain we can find and extract him. He will then be rushed to the safe room where the rest of the reexpansion will take place. Please, Tertia, even ordinary medical procedures have their risk.

Mrs. Arnfeld was in the observation room as the miniaturization of Mike took place. So were the holovision cameras and selected media representatives. The importance of the medical experiment made it impossible to prevent that, but Mrs. Arnfeld was in a niche with only Johannes for company, and it was understood that she was not to be approached for comment, particularly if anything untoward occurred.
Untoward! A full and sudden reexpansion would blow up the entire operating room and kill every person in it. It was not for nothing the observation room was underground and half a mile away from the viewing room.
It gave Mrs. Arnfeld a somewhat grisly sense of assurance that the three miniaturists who were working on the procedure (so calmly, it would seemso calmly) were condemned to death as firmly as her husband was in case ofanything untoward. Surely, she could rely on them protecting their own lives to the extreme; they would not, therefore, be cavalier in the protection of her husband.
Eventually, of course, if the procedure were successful, ways would be worked out to perform it in automated fashion, and only the patient would be at risk. Then, perhaps, the patient might be more easily sacrificed through carelessnessbut not now, not now. Mrs. Arnfeld keenly watched the three, working under imminent sentence of death for any sign of discomposure.
She watched the miniaturization procedure (she had seen it before) and saw Mike grow smaller and disappear. She watched the elaborate procedure that injected him into the proper place in her husbands body. (It had been explained to her that it would have been prohibitively expensive to inject human beings in a submarine device instead. Mike, at least, needed no life-support system.)
Then matters shifted to the screen, in which the appropriate section of the body was shown in holosonogram. It was a three-dimensional representation, cloudy and unfocused, made imprecise through a combination of the finite size of the sound waves and the effects of Brownian motion. It showed Mike dimly and noiselessly making his way through Gregory Arnfelds tissues by way of his bloodstream. It was almost impossible to tell what he was doing, but Johannes described the events to her in a low, satisfied manner, until she could listen to him no more and asked to be led away.

She had been mildly sedated, and she had slept until evening, when Johannes came to see her. She had not been long awake and it took her a moment to gather her faculties. Then she said, in sudden and overwhelming fear, What has happened?
Johannes said, hastily, Success, Tertia. Complete success. Your husband is cured. We cant stop the cancer from recurring, but for now he is cured.
She fell back in relief. Oh, wonderful.
Just the same, something unexpected has happened and this will have to be explained to Gregory. We felt that it would be best if you did the explaining. ,
I? Then, in a renewed access of fear, What has happened? Johannes told her.

It was two days before she could see her husband for more than a moment or two. He was sitting up in bed, looking a little pale, but smiling at her.
A new lease on life, Tertia, he said buoyantly.
Indeed, Greg, I was quite wrong. The experiment succeeded and they tell me they cant find a trace of cancer in you.
Well, we cant be too confident about that. There may be a cancerous cell here and there, but perhaps my immune system will handle it, especially with the proper medication, and if it ever builds up again, which might well take years, well call on Mike again.
At this point, he frowned and said, You know, I havent seen Mike.
Mrs. Arnfeld maintained a discreet silence.
Arnfeld said, Theyve been putting me off.
Youve been weak, dear, and sedated. Mike was poking through your tissues and doing a little necessary destructive work here and there. Even with a successful operation you need time for recovery.
If Ive recovered enough to see you, surely Ive recovered enough to see Mike, at least long enough to thank him.
A robot doesnt need to receive thanks.
Of course not, but I need to give it. Do me a favor, Tertia. Go out there and tell them I want Mike right away.
Mrs. Arnfeld hesitated, then came to a decision. Waiting would make the task harder for everyone. She said carefully,  Actually, dear, Mike is not available.
Not available! Why not?
He had to make a choice, you see. He had cleaned up your tissues marvelously well; he had done a magnificent job, everyone agrees; and then he had to undergo reexpansion. That was the risky part.
Yes, but here I am. Why are you making a long story out of it?
Mike decided to minimize the risk. 
Naturally. What did he do?
Well, dear, he decided to make himself smaller. 
What! He couldnt. He was ordered not to.
That was Second Law, Greg. First Law took precedence. He wanted to make certain your life would be saved. He was equipped to control his own size, so he made himself smaller as rapidly as he could, and when he was far less massive than an electron he used his laser beam, which was by then too tiny to hurt anything in your body, and the recoil sent him flying away at nearly the speed of light. He exploded in outer space. The gamma rays were detected.
Arnfeld stared at her. You cant mean it. Are you serious? Mike is dead?
Thats what happened. Mike could not refuse to take an action that might keep you from harm.
But I didnt want that. I wanted him safe for further work. He wouldnt have reexpanded uncontrollably. He would have gotten out safely.
He couldnt be sure. He couldnt risk your life, so he sacrificed his own.
But my life was less important than his.
Not to me, dear. Not to those who work with you. Not to anyone. Not even to Mike. She put out her hand to him.

Come, Greg, youre alive. Youre well. Thats all that counts.
But he pushed her hand aside impatiently. Thats not all that counts. You dont understand. Oh, too bad. Too bad!
Robbie

Ninety-eight  ninety-nine  one hundred. Gloria withdrew her chubby little forearm from before her eyes and stood for a moment, wrinkling her nose and blinking in the sunlight. Then, trying to watch in all directions at once, she withdrew a few cautious steps from the tree against which she had been leaning.
She craned her neck to investigate the possibilities of a clump of bushes to the right and then withdrew farther to obtain a better angle for viewing its dark recesses. The quiet was profound except for the incessant buzzing of insects and the occasional chirrup of some hardy bird, braving the midday sun.
Gloria pouted, I bet he went inside the house, and Ive told him a million times that thats not fair.
With tiny lips pressed together tightly and a severe frown crinkling her forehead, she moved determinedly toward the two-story building up past the driveway.
Too late she heard the rustling sound behind her, followed by the distinctive and rhythmic clump-clump of Robbies metal feet. She whirled about to see her triumphing companion emerge from hiding and make for the home-tree at full speed.
Gloria shrieked in dismay. Wait, Robbie! That wasnt fair, Robbie! You promised you wouldnt run until I found you. Her little feet could make no headway at all against Robbies giant strides. Then, within ten feet of the goal, Robbies pace slowed suddenly to the merest of crawls, and Gloria, with one final burst of wild speed, dashed pantingly past him to touch the welcome bark of home-tree first.
Gleefully, she turned on the faithful Robbie, and with the basest of ingratitude, rewarded him for his sacrifice by taunting him cruelly for a lack of running ability.
Robbie cant run, she shouted at the top of her eight-year-old voice. I can beat him any day. I can beat him any day. She chanted the words in a shrill rhythm.
Robbie didnt answer, of course  not in words. He pantomimed running instead, inching away until Gloria found herself running after him as he dodged her narrowly, forcing her to veer in helpless circles, little arms outstretched and fanning at the air.
Robbie, she squealed, stand still!  And the laughter was forced out of her in breathless jerks.
Until he turned suddenly and caught her up, whirling her round, so that for her the world fell away for a moment with a blue emptiness beneath, and green trees stretching hungrily downward toward the void. Then she was down in the grass again, leaning against Robbies leg and still holding a hard, metal finger.
After a while, her breath returned. She pushed uselessly at her disheveled hair in vague imitation of one of her mothers gestures and twisted to see if her dress were torn.
She slapped her hand against Robbies torso, Bad boy! Ill spank you!
And Robbie cowered, holding his hands over his face so that she had to add, No, I wont, Robbie. I wont spank you. But anyway, its my turn to hide now because youve got longer legs and you promised not to run till I found you.
Robbie nodded his head  a small parallelepiped with rounded edges and corners attached to a similar but much larger parallelepiped that served as torso by means of a short, flexible stalk  and obediently faced the tree. A thin, metal film descended over his glowing eyes and from within his body came a steady, resonant ticking.
Dont peek now  and dont skip any numbers, warned Gloria, and scurried for cover.
With unvarying regularity, seconds were ticked off, and at the hundredth, up went the eyelids, and the glowing red of Robbies eyes swept the prospect. They rested for a moment on a bit of colorful gingham that protruded from behind a boulder. He advanced a few steps and convinced himself that it was Gloria who squatted behind it.
Slowly, remaining always between Gloria and home-tree, he advanced on the hiding place, and when Gloria was plainly in sight and could no longer even theorize to herself that she was not seen, he extended one arm toward her, slapping the other against his leg so that it rang again. Gloria emerged sulkily.
You peeked! she exclaimed, with gross unfairness. Besides Im tired of playing hide-and-seek. I want a ride.
But Robbie was hurt at the unjust accusation, so he seated himself carefully and shook his head ponderously from side to side.
Gloria changed her tone to one of gentle coaxing immediately, Come on, Robbie. I didnt mean it about the peeking. Give me a ride.
Robbie was not to be won over so easily, though. He gazed stubbornly at the sky, and shook his head even more emphatically.
Please, Robbie, please give me a ride. She encircled his neck with rosy arms and hugged tightly. Then, changing moods in a moment, she moved away. If you dont, Im going to cry, and her face twisted appallingly in preparation.
Hard-hearted Robbie paid scant attention to this dreadful possibility, and shook his head a third time. Gloria found it necessary to play her trump card.


If you dont, she exclaimed warmly, I wont tell you any more stories, thats all. Not one
Robbie gave in immediately and unconditionally before this ultimatum, nodding his head vigorously until the metal of his neck hummed. Carefully, he raised the little girl and placed her on his broad, flat shoulders.
Glorias threatened tears vanished immediately and she crowed with delight. Robbies metal skin, kept at a constant temperature of seventy by the high resistance coils within, felt nice and comfortable, while the beautifully loud sound her heels made as they bumped rhythmically against his chest was enchanting.
Youre an air-coaster, Robbie, youre a big, silver aircoaster. Hold out your arms straight.  You got to, Robbie, if youre going to be an aircoaster.
The logic was irrefutable. Robbies arms were wings catching the air currents and he was a silver coaster.
Gloria twisted the robots head and leaned to the right. He banked sharply. Gloria equipped the coaster with a motor that went Br-r-r and then with weapons that went Powie and Sh-sh-shshsh. Pirates were giving chase and the ships blasters were coming into play. The pirates dropped in a steady rain.
Got another one. Two more, she cried.
Then Faster, men, Gloria said pompously, were running out of ammunition. She aimed over her shoulder with undaunted courage and Robbie was a blunt-nosed spaceship zooming through the void at maximum acceleration.
Clear across the field he sped, to the patch of tall grass on the other side, where he stopped with a suddenness that evoked a shriek from his flushed rider, and then tumbled her onto the soft, green carpet.
Gloria gasped and panted, and gave voice to intermittent whispered exclamations of That was nice!
Robbie waited until she had caught her breath and then pulled gently at a lock of hair.
You want something? said Gloria, eyes wide in an apparently artless complexity that fooled her huge nursemaid not at all. He pulled the curl harder.
Oh, I know. You want a story.
Robbie nodded rapidly.
Which one?
Robbie made a semi-circle in the air with one finger.
The little girl protested, Again? Ive told you Cinderella a million times. Arent you tired of it? Its for babies.
Another semi-circle.
Oh, well, Gloria composed herself, ran over the details of the tale in her mind (together with her own elaborations, of which she had several) and began:
Are you ready? Well  once upon a time there was a beautiful little girl whose name was Ella. And she had a terribly cruel step-mother and two very ugly and very cruel step-sisters and

Gloria was reaching the very climax of the tale  midnight was striking and everything was changing back to the shabby originals lickety-split, while Robbie listened tensely with burning eyes  when the interruption came.
Gloria!
It was the high-pitched sound of a woman who has been calling not once, but several times; and had the nervous tone of one in whom anxiety was beginning to overcome impatience.
Mammas calling me, said Gloria, not quite happily. Youd better carry me back to the house, Robbie.
Robbie obeyed with alacrity for somehow there was that in him which judged it best to obey Mrs. Weston, without as much as a scrap of hesitation. Glorias father was rarely home in the daytime except on Sunday  today, for instance  and when he was, he proved a genial and understanding person. Glorias mother, however, was a source of uneasiness to Robbie and there was always the impulse to sneak away from her sight.
Mrs. Weston caught sight of them the minute they rose above the masking tufts of long grass and retired inside the house to wait.
Ive shouted myself hoarse, Gloria, she said, severely. Where were you?
I was with Robbie, quavered Gloria. I was telling him Cinderella, and I forgot it was dinner-time.
Well, its a pity Robbie forgot, too. Then, as if that reminded her of the robots presence, she whirled upon him. You may go, Robbie. She doesnt need you now. Then, brutally, And dont come back till I call you.
Robbie turned to go, but hesitated as Gloria cried out in his defense, Wait, Mamma, you got to let him stay. I didnt finish Cinderella for him. I said I would tell him Cinderella and Im not finished.
Gloria!
Honest and truly, Mamma, hell stay so quiet, you wont even know hes here. He can sit on the chair in the corner, and he wont say a word, I mean he wont do anything. Will you, Robbie?
Robbie, appealed to, nodded his massive head up and down once.
Gloria, if you dont stop this at once, you shant see Robbie for a whole week.
The girls eyes fell, All right! But Cinderella is his favorite story and I didnt finish it. And he likes it so much.
The robot left with a disconsolate step and Gloria choked back a sob.

George Weston was comfortable. It was a habit of his to be comfortable on Sunday afternoons. A good, hearty dinner below the hatches; a nice, soft, dilapidated couch on which to sprawl; a copy of the Times; slippered feet and shirtless chest; how could anyone help but be comfortable?
He wasnt pleased, therefore, when his wife walked in. After ten years of married life, be still was so unutterably foolish as to love her, and there was no question that he was always glad to see her  still Sunday afternoons just after dinner were sacred to him and his idea of solid comfort was to be left in utter solitude for two or three hours. Consequently, he fixed his eye firmly upon the latest reports of the Lefebre-Yoshida expedition to Mars (this one was to take off from Lunar Base and might actually succeed) and pretended she wasnt there.
Mrs. Weston waited patiently for two minutes, then impatiently for two more, and finally broke the silence.
George!
Hmpph?
George, I say! Will you put down that paper and look at me?
The paper rustled to the floor and Weston turned a weary face toward his wife, What is it, dear?
You know what it is, George. Its Gloria and that terrible machine.
What terrible machine?
Now dont pretend you dont know what Im talking about. Its that robot Gloria calls Robbie. He doesnt leave her for a moment.
Well, why should he? Hes not supposed to. And he certainly isnt a terrible machine. Hes the best darn robot money can buy and Im damned sure he set me back half a years income. Hes worth it, though  darn sight cleverer than half my office staff.

He made a move to pick up the paper again, but his wife was quicker and snatched it away.
You listen to me, George. I wont have my daughter entrusted to a machine  and I dont care how clever it is. It has no soul, and no one knows what it may be thinking. A child just isnt made to be guarded by a thing of metal.
Weston frowned, When did you decide this? Hes been with Gloria two years now and I havent seen you worry till now.
It was different at first. It was a novelty; it took a load off me, and  and it was a fashionable thing to do. But now I dont know. The neighbors
Well, what have the neighbors to do with it? Now, look. A robot is infinitely more to be trusted than a human nursemaid. Robbie was constructed for only one purpose really  to be the companion of a little child. His entire mentality has been created for the purpose. He just cant help being faithful and loving and kind. Hes a machine-made so. Thats more than you can say for humans.
But something might go wrong. Some- some- Mrs. Weston was a bit hazy about the insides of a robot, some little jigger will come loose and the awful thing will go berserk and- and- She couldnt bring herself to complete the quite obvious thought.
Nonsense, Weston denied, with an involuntary nervous shiver. Thats completely ridiculous. We had a long discussion at the time we bought Robbie about the First Law of Robotics. You know that it is impossible for a robot to harm a human being; that long before enough can go wrong to alter that First Law, a robot would be completely inoperable. Its a mathematical impossibility. Besides I have an engineer from U. S. Robots here twice a year to give the poor gadget a complete overhaul. Why, theres no more chance of any thing at all going wrong with Robbie than there is of you or I suddenly going loony  considerably less, in fact. Besides, how are you going to take him away from Gloria?
He made another futile stab at the paper and his wife tossed it angrily into the next room.
Thats just it, George! She wont play with anyone else. There are dozens of little boys and girls that she should make friends with, but she wont. She wont go near them unless I make her. Thats no way for a little girl to grow up. You want her to be normal, dont you? You want her to be able to take her part in society.
Youre jumping at shadows, Grace. Pretend Robbies a dog. Ive seen hundreds of children who would rather have their dog than their father.
A dog is different, George. We must get rid of that horrible thing. You can sell it back to the company. Ive asked, and you can.
Youve asked? Now look here, Grace, lets not go off the deep end. Were keeping the robot until Gloria is older and I dont want the subject brought up again. And with that he walked out of the room in a huff.

Mrs. Weston met her husband at the door two evenings later. Youll have to listen to this, George. Theres bad feeling in the village.
About what? asked Weston? He stepped into the washroom and drowned out any possible answer by the splash of water.
Mrs. Weston waited. She said, About Robbie.
Weston stepped out, towel in hand, face red and angry, What are you talking about?
Oh, its been building up and building up. Ive tried to close my eyes to it, but Im not going to any more. Most of the villagers consider Robbie dangerous. Children arent allowed to go near our place in the evenings.
We trust our child with the thing.
Well, people arent reasonable about these things.
Then to hell with them.
Saying that doesnt solve the problem. Ive got to do my shopping down there. Ive got to meet them every day. And its even worse in the city these days when it comes to robots. New York has just passed an ordinance keeping all robots off the streets between sunset and sunrise.
All right, but they cant stop us from keeping a robot in our home. Grace, this is one of your campaigns. I recognize it. But its no use. The answer is still, no! Were keeping Robbie!

And yet he loved his wife  and what was worse, his wife knew it. George Weston, after all, was only a man  poor thing  and his wife made full use of every device which a clumsier and more scrupulous sex has learned, with reason and futility, to fear.
Ten times in the ensuing week, he cried, Robbie stays, and thats final! and each time it was weaker and accompanied by a louder and more agonized groan.
Came the day at last, when Weston approached his daughter guiltily and suggested a beautiful visivox show in the village.
Gloria clapped her hands happily, Can Robbie go?
No, dear, he said, and winced at the sound of his voice, they wont allow robots at the visivox  but you can tell him all about it when you get home. He stumbled all over the last few words and looked away.
Gloria came back from town bubbling over with enthusiasm, for the visivox had been a gorgeous spectacle indeed.
She waited for her father to maneuver the jet-car into the sunken garage, Wait till I tell Robbie, Daddy. He would have liked it like anything. Especially when Francis Fran was backing away so-o-o quietly, and backed right into one of the Leopard-Men and had to run. She laughed again, Daddy, are there really Leopard-Men on the Moon?
Probably not, said Weston absently. Its just funny make-believe. He couldnt take much longer with the car. Hed have to face it.
Gloria ran across the lawn. Robbie. Robbie!
Then she stopped suddenly at the sight of a beautiful collie which regarded her out of serious brown eyes as it wagged its tail on the porch.
Oh, what a nice dog! Gloria climbed the steps, approached cautiously and patted it. Is it for me, Daddy?
Her mother had joined them. Yes, it is, Gloria. Isnt it nice  soft and furry? Its very gentle. It likes little girls.
Can he play games?
Surely. He can do any number of tricks. Would you like to see some?
Right away. I want Robbie to see him, too. Robbie! She stopped, uncertainly, and frowned, Ill bet hes just staying in his room because hes mad at me for not taking him to the visivox. Youll have to explain to him, Daddy. He might not believe me, but he knows if you say it, its so.
Westons lip grew tighter. He looked toward his wife but could not catch her eye.
Gloria turned precipitously and ran down the basement steps, shouting as she went, Robbie Come and see what Daddy and Mamma brought me. They brought me a dog, Robbie.
In a minute she had returned, a frightened little girl. Mamma, Robbie isnt in his room. Where is he? There was no answer and George Weston coughed and was suddenly extremely interested in an aimlessly drifting cloud. Glorias voice quavered on the verge of tears, Wheres Robbie, Mamma?
Mrs. Weston sat down and drew her daughter gently to her, Dont feel bad, Gloria. Robbie has gone away, I think.
Gone away? Where? Wheres he gone away, Mamma?
No one knows, darling. He just walked away. Weve looked and weve looked and weve looked for him, but we cant find him.
You mean hell never come back again? Her eyes were round with horror.
We may find him soon. Well keep looking for him. And meanwhile you can play with your nice new doggie. Look at him! His name is Lightning and he can
But Glorias eyelids had overflown, I dont want the nasty dog  I want Robbie. I want you to find me Robbie. Her feelings became too deep for words, and she spluttered into a shrill wail.
Mrs. Weston glanced at her husband for help, but he merely shuffled his feet morosely and did not withdraw his ardent stare from the heavens, so she bent to the task of consolation, Why do you cry, Gloria? Robbie was only a machine, just a nasty old machine. He wasnt alive at all.
He was not no machine! screamed Gloria, fiercely and ungrammatically. He was a person just like you and me and he was my friend. I want him back. Oh, Mamma, I want him back.
Her mother groaned in defeat and left Gloria to her sorrow.
Let her have her cry out, she told her husband. Childish griefs are never lasting. In a few days, shell forget that awful robot ever existed.
But time proved Mrs. Weston a bit too optimistic. To be sure, Gloria ceased crying, but she ceased smiling, too, and the passing days found her ever more silent and shadowy. Gradually, her attitude of passive unhappiness wore Mrs. Weston down and all that kept her from yielding was the impossibility of admitting defeat to her husband.
Then, one evening, she flounced into the living room, sat down, folded her arms and looked boiling mad.
Her husband stretched his neck in order to see her over his newspaper, What now, Grace?
Its that child, George. Ive had to send back the dog today. Gloria positively couldnt stand the sight of him, she said. Shes driving me into a nervous breakdown.
Weston laid down the paper and a hopeful gleam entered his eye, Maybe Maybe we ought to get Robbie back. It might be done, you know. I can get in touch with
No! she replied, grimly. I wont hear of it. Were not giving up that easily. My child shall not be brought up by a robot if it takes years to break her of it.
Weston picked up his paper again with a disappointed air. A year of this will have me prematurely gray.
Youre a big help, George, was the frigid answer. What Gloria needs is a change of environment? Of course she cant forget Robbie here. How can she when every tree and rock reminds her of him? It is really the silliest situation I have ever heard of. Imagine a child pining away for the loss of a robot.
Well, stick to the point. Whats the change in environment youre planning?
Were going to take her to New York.
The city! In August! Say, do you know what New York is like in August? Its unbearable.
Millions do bear it.
They dont have a place like this to go to. If they didnt have to stay in New York, they wouldnt.
Well, we have to. I say were leaving now  or as soon as we can make the arrangements. In the city, Gloria will find sufficient interests and sufficient friends to perk her up and make her forget that machine.
Oh, Lord, groaned the lesser half, those frying pavements!
We have to, was the unshaken response. Gloria has lost five pounds in the last month and my little girls health is more important to me than your comfort.
Its a pity you didnt think of your little girls health before you deprived her of her pet robot, he muttered  but to himself.

Gloria displayed immediate signs of improvement when told of the impending trip to the city. She spoke little of it, but when she did, it was always with lively anticipation. Again, she began to smile and to eat with something of her former appetite.
Mrs. Weston hugged herself for joy and lost no opportunity to triumph over her still skeptical husband.
You see, George, she helps with the packing like a little angel, and chatters away as if she hadnt a care in the world. Its just as I told you  all we need do is substitute other interests.
Hmpph, was the skeptical response, I hope so.
Preliminaries were gone through quickly. Arrangements were made for the preparation of their city home and a couple were engaged as housekeepers for the country home. When the day of the trip finally did come, Gloria was all but her old self again, and no mention of Robbie passed her lips at all.
In high good-humor the family took a taxi-gyro to the airport (Weston would have preferred using his own private gyro, but it was only a two-seater with no room for baggage) and entered the waiting liner.
Come, Gloria, called Mrs. Weston. Ive saved you a seat near the window so you can watch the scenery.
Gloria trotted down the aisle cheerily, flattened her nose into a white oval against the thick clear glass, and watched with an intentness that increased as the sudden coughing of the motor drifted backward into the interior. She was too young to be frightened when the ground dropped away as if let through a trap door and she herself suddenly became twice her usual weight, but not too young to be mightily interested. It wasnt until the ground had changed into a tiny patchwork quilt that she withdrew her nose, and faced her mother again.
Will we soon be in the city, Mamma? she asked, rubbing her chilled nose, and watching with interest as the patch of moisture which her breath had formed on the pane shrank slowly and vanished.
In about half an hour, dear. Then, with just the faintest trace of anxiety, Arent you glad were going? Dont you think youll be very happy in the city with all the buildings and people and things to see? Well go to the visivox every day and see shows and go to the circus and the beach and
Yes, Mamma, was Glorias unenthusiastic rejoinder. The liner passed over a bank of clouds at the moment, and Gloria was instantly absorbed in the usual spectacle of clouds underneath one. Then they were over clear sky again, and she turned to her mother with a sudden mysterious air of secret knowledge.
I know why were going to the city, Mamma.
Do you? Mrs. Weston was puzzled. Why, dear?
You didnt tell me because you wanted it to be a surprise, but I know. For a moment, she was lost in admiration at her own acute penetration, and then she laughed gaily. Were going to New York so we can find Robbie, arent we? With detectives.
The statement caught George Weston in the middle of a drink of water, with disastrous results. There was a sort of strangled gasp, a geyser of water, and then a bout of choking coughs. When all was over, he stood there, a red-faced, water-drenched and very, very annoyed person.
Mrs. Weston maintained her composure, but when Gloria repeated her question in a more anxious tone of voice, she found her temper rather bent.
Maybe, she retorted, tartly. Now sit and be still, for Heavens sake.

New York City, 1998 A.D., was a paradise for the sightseer more than ever in its history. Glorias parents realized this and made the most of it.
On direct orders from his wife, George Weston arranged to have his business take care of itself for a month or so, in order to be free to spend the time in what he termed, dissipating Gloria to the verge of ruin. Like everything else Weston did, this was gone about in an efficient, thorough, and business-like way. Before the month had passed, nothing that could be done had not been done.
She was taken to the top of the half-mile tall Roosevelt Building, to gaze down in awe upon the jagged panorama of rooftops that blended far off in the fields of Long Island and the flatlands of New Jersey. They visited the zoos where Gloria stared in delicious fright at the real live lion (rather disappointed that the keepers fed him raw steaks, instead of human beings, as she had expected), and asked insistently and peremptorily to see the whale.
The various museums came in for their share of attention, together with the parks and the beaches and the aquarium.
She was taken halfway up the Hudson in an excursion steamer fitted out in the archaism of the mad Twenties. She traveled into the stratosphere on an exhibition trip, where the sky turned deep purple and the stars came out and the misty earth below looked like a huge concave bowl. Down under the waters of the Long Island Sound she was taken in a glass-walled sub-sea vessel, where in a green and wavering world, quaint and curious sea-things ogled her and wiggled suddenly away.
On a more prosaic level, Mrs. Weston took her to the department stores where she could revel in another type of fairyland.
In fact, when the month had nearly sped, the Westons were convinced that everything conceivable had been done to take Glorias mind once and for all off the departed Robbie  but they were not quite sure they had succeeded.
The fact remained that wherever Gloria went, she displayed the most absorbed and concentrated interest in such robots as happened to be present. No matter how exciting the spectacle before her, nor how novel to her girlish eyes, she turned away instantly if the corner of her eye caught a glimpse of metallic movement.
Mrs. Weston went out of her way to keep Gloria away from all robots.
And the matter was finally climaxed in the episode at the Museum of Science and Industry. The Museum had announced a special childrens program in which exhibits of scientific witchery scaled down to the child mind were to be shown. The Westons, of course, placed it upon their list of absolutely.
It was while the Westons were standing totally absorbed in the exploits of a powerful electro-magnet that Mrs. Weston suddenly became aware of the fact that Gloria was no longer with her. Initial panic gave way to calm decision and, enlisting the aid of three attendants, a careful search was begun.
Gloria, of course, was not one to wander aimlessly, however. For her age, she was an unusually determined and purposeful girl, quite full of the maternal genes in that respect. She had seen a huge sign on the third floor, which had said, This Way to the Talking Robot Having spelled it out to herself and having noticed that her parents did not seem to wish to move in the proper direction, she did the obvious thing. Waiting for an opportune moment of parental distraction, she calmly disengaged herself and followed the sign.

The Talking Robot was a tour de force, a thoroughly impractical device, possessing publicity value only. Once an hour, an escorted group stood before it and asked questions of the robot engineer in charge in careful whispers. Those the engineer decided were suitable for the robots circuits were transmitted to the Talking Robot.
It was rather dull. It may be nice to know that the square of fourteen is one hundred ninety-six, that the temperature at the moment is 72 degrees Fahrenheit, and the air-pressure 30.02 inches of mercury, that the atomic weight of sodium is 23, but one doesnt really need a robot for that. One especially does not need an unwieldy, totally immobile mass of wires and coils spreading over twenty-five square yards.
Few people bothered to return for a second helping, but one girl in her middle teens sat quietly on a bench waiting for a third. She was the only one in the room when Gloria entered.
Gloria did not look at her. To her at the moment, another human being was but an inconsiderable item. She saved her attention for this large thing with the wheels. For a moment, she hesitated in dismay. It didnt look like any robot she had ever seen.
Cautiously and doubtfully she raised her treble voice; Please, Mr. Robot, sir, are you the Talking Robot, sir? She wasnt sure, but it seemed to her that a robot that actually talked was worth a great deal of politeness.
(The girl in her mid-teens allowed a look of intense concentration to cross her thin, plain face. She whipped out a small notebook and began writing in rapid pothooks.)
There was an oily whir of gears and a mechanically timbered voice boomed out in words that lacked accent and intonation, I- am- the- robot- that- talks.
Gloria stared at it ruefully. It did talk, but the sound came from inside somewheres. There was no face to talk to. She said, Can you help me, Mr. Robot, sir?
The Talking Robot was designed to answer questions, and only such questions as it could answer had ever been put to it. It was quite confident of its ability, therefore, I can help you.
Thank you, Mr. Robot, sir. Have you seen Robbie?
Who is Robbie?
Hes a robot, Mr. Robot, sir. She stretched to tiptoes. Hes about so high, Mr. Robot, sir, only higher, and hes very nice. Hes got a head, you know. I mean you havent, but he has, Mr. Robot, sir.
The Talking Robot had been left behind, A robot?
Yes, Mr. Robot, sir. A robot just like you, except he cant talk, of course, and  looks like a real person.
A robot like me?
Yes, Mr. Robot, sir.
To which the Talking Robots only response was an erratic splutter and an occasional incoherent sound. The radical generalization offered it, i.e., its existence, not as a particular object, but as a member of a general group, was too much for it. Loyally, it tried to encompass the concept and half a dozen coils burnt out. Little warning signals were buzzing.
(The girl in her mid-teens left at that point. She had enough for her Physics-1 paper on Practical Aspects of Robotics. This paper was Susan Calvins first of many on the subject.)
Gloria stood waiting, with carefully concealed impatience, for the machines answer when she heard the cry behind her of There she is, and recognized that cry as her mothers.
What are you doing here, you bad girl? cried Mrs. Weston, anxiety dissolving at once into anger. Do you know you frightened your mamma and daddy almost to death? Why did you run away?
The robot engineer had also dashed in, tearing his hair, and demanding who of the gathering crowd had tampered with the machine. Cant anybody read signs? he yelled. Youre not allowed in here without an attendant.
Gloria raised her grieved voice over the din, I only came to see the Talking Robot, Mamma. I thought he might know where Robbie was because theyre both robots. And then, as the thought of Robbie was suddenly brought forcefully home to her, she burst into a sudden storm of tears, And I got to find Robbie, Mamma. I got to.
Mrs. Weston strangled a cry, and said, Oh, good Heavens. Come home, George. This is more than I can stand.
That evening, George Weston left for several hours, and the next morning, he approached his wife with something that looked suspiciously like smug complacence.
Ive got an idea, Grace.
About what? was the gloomy, uninterested query?
About Gloria.
Youre not going to suggest buying back that robot?
No, of course not.
Then go ahead. I might as well listen to you. Nothing Ive done seems to have done any good.
All right. Heres what Ive been thinking. The whole trouble with Gloria is that she thinks of Robbie as a person and not as a machine. Naturally, she cant forget him. Now if we managed to convince her that Robbie was nothing more than a mess of steel and copper in the form of sheets and wires with electricity its juice of life, how long would her longings last? Its the psychological attack, if you see my point.
How do you plan to do it?
Simple. Where do you suppose I went last night? I persuaded Robertson of U. S. Robots and Mechanical Men, Inc. to arrange for a complete tour of his premises tomorrow. The three of us will go, and by the time were through, Gloria will have it drilled into her that a robot is not alive.
Mrs. Westons eyes widened gradually and something glinted in her eyes that was quite like sudden admiration, Why, George, thats a good idea.
And George Westons vest buttons strained. Only kind I have, he said.

Mr. Struthers was a conscientious General Manager and naturally inclined to be a bit talkative. The combination, therefore, resulted in a tour that was fully explained, perhaps even over-abundantly explained, at every step. However, Mrs. Weston was not bored. Indeed, she stopped him several times and begged him to repeat his statements in simpler language so that Gloria might understand. Under the influence of this appreciation of his narrative powers, Mr. Struthers expanded genially and became ever more communicative, if possible.
George Weston, himself, showed a gathering impatience.
Pardon me, Struthers, he said, breaking into the middle of a lecture on the photoelectric cell, havent you a section of the factory where only robot labor is employed?
Eh? Oh, yes! Yes, indeed! He smiled at Mrs. Weston. A vicious circle in a way, robots creating more robots. Of course, we are not making a general practice out of it. For one thing, the unions would never let us. But we can turn out a very few robots using robot labor exclusively, merely as a sort of scientific experiment. You see, he tapped his pince-nez into one palm argumentatively, what the labor unions dont realize  and I say this as a man who has always been very sympathetic with the labor movement in general  is that the advent of the robot, while involving some dislocation to begin with, will inevitably
Yes, Struthers, said Weston, but about that section of the factory you speak of  may we see it? It would be very interesting, Im sure.
Yes! Yes, of course! Mr. Struthers replaced his pince-nez in one convulsive movement and gave vent to a soft cough of discomfiture. Follow me, please.
He was comparatively quiet while leading the three through a long corridor and down a flight of stairs. Then, when they had entered a large well-lit room that buzzed with metallic activity, the sluices opened and the flood of explanation poured forth again.
There you are! he said with pride in his voice. Robots only! Five men act as overseers and they dont even stay in this room. In five years, that is, since we began this project, not a single accident has occurred. Of course, the robots here assembled are comparatively simple, but...
The General Managers voice had long died to a rather soothing murmur in Glorias ears. The whole trip seemed rather dull and pointless to her, though there were many robots in sight. None were even remotely like Robbie, though, and she surveyed them with open contempt.
In this room, there werent any people at all, she noticed. Then her eyes fell upon six or seven robots busily engaged at a round table halfway across the room. They widened in incredulous surprise. It was a big room. She couldnt see for sure, but one of the robots looked like  looked like  it was!
Robbie! Her shriek pierced the air, and one of the robots about the table faltered and dropped the tool he was holding. Gloria went almost mad with joy. Squeezing through the railing before either parent could stop her, she dropped lightly to the floor a few feet below, and ran toward her Robbie, arms waving and hair flying.
And the three horrified adults, as they stood frozen in their tracks, saw what the excited little girl did not see,  a huge, lumbering tractor bearing blindly down upon its appointed track.
It took split-seconds for Weston to come to his senses, and those split-seconds meant everything, for Gloria could not be overtaken. Although Weston vaulted the railing in a wild attempt, it was obviously hopeless. Mr. Struthers signaled wildly to the overseers to stop the tractor, but the overseers were only human and it took time to act.
It was only Robbie that acted immediately and with precision.
With metal legs eating up the space between himself and his little mistress he charged down from the opposite direction. Everything then happened at once. With one sweep of an arm, Robbie snatched up Gloria, slackening his speed not one iota, and, consequently, knocking every breath of air out of her. Weston, not quite comprehending all that was happening, felt, rather than saw, Robbie brush past him, and came to a sudden bewildered halt. The tractor intersected Glorias path half a second after Robbie had, rolled on ten feet further and came to a grinding, long drawn-out stop.
Gloria regained her breath, submitted to a series of passionate hugs on the part of both her parents and turned eagerly toward Robbie. As far as she was concerned, nothing had happened except that she had found her friend.
But Mrs. Westons expression had changed from one of relief to one of dark suspicion. She turned to her husband, and, despite her disheveled and undignified appearance, managed to look quite formidable, You engineered this, didnt you?
George Weston swabbed at a hot forehead with his handkerchief. His hand was unsteady, and his lips could curve only into a tremulous and exceedingly weak smile.
Mrs. Weston pursued the thought, Robbie wasnt designed for engineering or construction work. He couldnt be of any use to them. You had him placed there deliberately so that Gloria would find him. You know you did.
Well, I did, said Weston. But, Grace, how was I to know the reunion would be so violent? And Robbie has saved her life; youll have to admit that. You cant send him away again.
Grace Weston considered. She turned toward Gloria and Robbie and watched them abstractedly for a moment. Gloria had a grip about the robots neck that would have asphyxiated any creature but one of metal, and was prattling nonsense in half-hysterical frenzy. Robbies chrome-steel arms (capable of bending a bar of steel two inches in diameter into a pretzel) wound about the little girl gently and lovingly, and his eyes glowed a deep, deep red.
Well, said Mrs. Weston, at last, I guess he can stay with us until he rusts.

Reason

Half a year later, the boys had changed their minds. The flame of a giant sun had given way to the soft blackness of space but external variations mean little in the business of checking the workings of experimental robots. Whatever the background, one is face to face with an inscrutable positronic brain, which the slide-rule geniuses say should work thus-and-so.
Except that they dont. Powell and Donovan found that out after they had been on the Station less than two weeks.
Gregory Powell spaced his words for emphasis, One week ago, Donovan and I put you together. His brows furrowed doubtfully and he pulled the end of his brown mustache.
It was quiet in the officers room on Solar Station #5  except for the soft purring of the mighty Beam Director somewhere far below.
Robot QT-1 sat immovable. The burnished plates of his body gleamed in the Luxites and the glowing red of the photoelectric cells that were his eyes, were fixed steadily upon the Earthman at the other side of the table.
Powell repressed a sudden attack of nerves. These robots possessed peculiar brains. Oh, the three Laws of Robotics held. They had to. All of U. S. Robots, from Robertson himself to the new floor-sweeper, would insist on that. So QT-1 was safe! And yet the QT models were the first of their kind, and this was the first of the QTs. Mathematical squiggles on paper were not always the most comforting protection against robotic fact.
Finally, the robot spoke. His voice carried the cold timbre inseparable from a metallic diaphragm, Do you realize the seriousness of such a statement, Powell?
Something made you, Cutie, pointed out Powell. You admit yourself that your memory seems to spring full-grown from an absolute blankness of a week ago. Im giving you the explanation. Donovan and I put you together from the parts shipped us.
Cutie gazed upon his long, supple fingers in an oddly human attitude of mystification, It strikes me that there should be a more satisfactory explanation than that. For you to make me seems improbable.
The Earthman laughed quite suddenly, In Earths name, why?
Call it intuition. Thats all it is so far. But I intend to reason it out, though. A chain of valid reasoning can end only with the determination of truth, and Ill stick till I get there.
Powell stood up and seated himself at the tables edge next to the robot. He felt a sudden strong sympathy for this strange machine. It was not at all like the ordinary robot, attending to his specialized task at the station with the intensity of a deeply ingrooved positronic path.
He placed a hand upon Cuties steel shoulder and the metal was cold and hard to the touch.
Cutie, he said, Im going to try to explain something to you. Youre the first robot whos ever exhibited curiosity as to his own existence  and I think the first thats really intelligent enough to understand the world outside. Here, come with me.
The robot rose erect smoothly and his thickly sponge-rubber soled feet made no noise as he followed Powell. The Earthman touched a button and a square section of the wall flickered aside. The thick, clear glass revealed space  star speckled.
Ive seen that in the observation ports in the engine room, said Cutie.
I know, said Powell. What do you think it is?
Exactly what it seems  a black material just beyond this glass that is spotted with little gleaming dots. I know that our director sends out beams to some of these dots, always to the same ones  and also that these dots shift and that the beams shift with them. That is all.
Good! Now I want you to listen carefully. The blackness is emptiness vast emptiness stretching out infinitely. The little, gleaming dots are huge masses of energy-filled matter. They are globes, some of them millions of miles in diameter and for comparison; this station is only one mile across. They seem so tiny because they are incredibly far off.
The dots to which our energy beams are directed are nearer and much smaller. They are cold and hard and human beings like myself live upon their surfaces  many billions of them. It is from one of these worlds that Donovan and I come. Our beams feed these worlds energy drawn from one of those huge incandescent globes that happens to be near us. We call that globe the Sun and it is on the other side of the station where you cant see it.
Cutie remained motionless before the port, like a steel statue. His head did not turn as he spoke, Which particular dot of light do you claim to come from?
Powell searched, There it is, the very bright one in the corner, we call it Earth. He grinned. Good old Earth. There are three billions of us there, Cutie  and in about two weeks Ill be back there with them
And then, surprisingly enough, Cutie hummed abstractedly. There was no tune to it, but it possessed a curious twanging quality as of plucked strings. It ceased as suddenly as it had begun, But where do I come in, Powell? You havent explained my existence.
The rest is simple. When these stations were first established to feed solar energy to the planets, they were run by humans. However, the heat, the hard solar radiations, and the electron storms made the post a difficult one. Robots were developed to replace human labor and now only two human executives are required for each station. We are trying to replace even those, and thats where you come in. Youre the highest type of robot ever developed and if you show the ability to run this station independently, no human need ever come here again except to bring parts for repairs.
His hand went up and the metal visi-lid snapped back into place. Powell returned to the table and polished an apple upon his sleeve before biting into it.
The red glow of the robots eyes held him. Do you expect me, said Cutie slowly, to believe any such complicated, implausible hypothesis as you have just outlined? What do you take me for?
Powell sputtered apple fragments onto the table and turned red. Why damn you, it wasnt a hypothesis. Those were facts
Cutie sounded grim, Globes of energy millions of miles across! Worlds with three billion humans on them! Infinite emptiness! Sorry, Powell, but I dont believe it. Ill puzzle this thing out for myself. Good-by.
He turned and stalked out of the room. He brushed past Michael Donovan on the threshold with a grave nod and passed down the corridor, oblivious to the astounded stare that followed him.
Mike Donovan rumpled his red hair and shot an annoyed glance at Powell, What was that walking junk yard talking about? What doesnt he believe?
The other dragged at his mustache bitterly. Hes a skeptic, was the bitter response. He doesnt believe we made him or that Earth exists or space or stars.
Sizzling Saturn, weve got a lunatic robot on our hands.
He says hes going to figure it all out for himself.
Well, now, said Donovan sweetly, I do hope hell condescend to explain it all to me after hes puzzled everything out Then, with sudden rage, Listen! If that metal mess gives me any lip like that, Ill knock that chromium cranium right off its torso.
He seated himself with a jerk and drew a paper-backed mystery novel out of his inner jacket pocket, That robot gives me the willies anyway  too damned inquisitive!

Mike Donovan growled from behind a huge lettuce-and-tomato sandwich as Cutie knocked gently and entered.
Is Powell here?
Donovans voice was muffled, with pauses for mastication, Hes gathering data on electronic stream functions. Were heading for a storm, looks like.
Gregory Powell entered as he spoke, eyes on the graphed paper in his hands, and dropped into a chair. He spread the sheets out before him and began scribbling calculations. Donovan stared over his shoulder, crunching lettuce and dribbling breadcrumbs. Cutie waited silently.
Powell looked up, The Zeta Potential is rising, but slowly. Just the same, the stream functions are erratic and I dont know what to expect. Oh, hello, Cutie. I thought you were supervising the installation of the new drive bar.
Its done, said the robot quietly, and so Ive come to have a talk with the two of you
Oh! Powell looked uncomfortable. Well, sit down. No, not that chair. One of the legs is weak and youre no lightweight.
The robot did so and said placidly, I have come to a decision.
Donovan glowered and put the remnants of his sandwich aside. If its on any of that screwy
The other motioned impatiently for silence, Go ahead, Cutie. Were listening.
I have spent these last two days in concentrated introspection, said Cutie, and the results have been most interesting. I began at the one sure assumption I felt permitted to make. I, myself, exist, because I think
Powell groaned, Oh, Jupiter, a robot Descartes!
Whos Descartes? demanded Donovan. Listen, do we have to sit here and listen to this metal maniac
Keep quiet, Mike!
Cutie continued imperturbably, And the question that immediately arose was: Just what is the cause of my existence?
Powells jaw set lumpily. Youre being foolish. I told you already that we made you.
And if you dont believe us, added Donovan, well gladly take you apart!
The robot spread his strong hands in a deprecatory gesture, I accept nothing on authority. A hypothesis must be backed by reason, or else it is worthless  and it goes against all the dictates of logic to suppose that you made me.
Powell dropped a restraining arm upon Donovans suddenly bunched fist. Just why do you say that?
Cutie laughed. It was a very inhuman laugh  the most machine-like utterance he had yet given vent to. It was sharp and explosive, as regular as a metronome and as uninflected.
Look at you, he said finally. I say this in no spirit of contempt, but look at you! The material you are made of is soft and flabby, lacking endurance and strength, depending for energy upon the inefficient oxidation of organic material  like that. He pointed a disapproving finger at what remained of Donovans sandwich. Periodically you pass into a coma and the least variation in temperature, air pressure, humidity, or radiation intensity impairs your efficiency. You are makeshift.
I, on the other hand, am a finished product. I absorb electrical energy directly and utilize it with an almost one hundred percent efficiency. I am composed of strong metal, am continuously conscious, and can stand extremes of environment easily. These are facts which, with the self-evident proposition that no being can create another being superior to itself, smashes your silly hypothesis to nothing.

Donovans muttered curses rose into intelligibility as he sprang to his feet, rusty eyebrows drawn low. All right, you son of a hunk of iron ore, if we didnt make you, who did?
Cutie nodded gravely. Very good, Donovan. That was indeed the next question. Evidently my creator must be more powerful than myself and so there was only one possibility.
The Earthmen looked blank and Cutie continued, What is the center of activities here in the station? What do we all serve? What absorbs all our attention? He waited expectantly.
Donovan turned a startled look upon his companion. Ill bet this tinplated screwball is talking about the Energy Converter itself.
Is that right, Cutie? grinned Powell.
I am talking about the Master, came the cold, sharp answer.
It was the signal for a roar of laughter from Donovan, and Powell himself dissolved into a half-suppressed giggle.
Cutie had risen to his feet and his gleaming eyes passed from one Earthman to the other. It is so just the same and I dont wonder that you refuse to believe. You two are not long to stay here, Im sure. Powell himself said that at first only men served the Master; that there followed robots for the routine work; and, finally, myself for the executive labor. The facts are no doubt true, but the explanation entirely illogical. Do you want the truth behind it all?
Go ahead, Cutie. Youre amusing.
The Master created humans first as the lowest type, most easily formed. Gradually, he replaced them by robots, the next higher step, and finally he created me to take the place of the last humans. From now on, I serve the Master.
Youll do nothing of the sort, said Powell sharply. Youll follow our orders and keep quiet, until were satisfied that you can run the Converter. Get that! The Converter  not the Master. If you dont satisfy us, you will be dismantled. And now  if you dont mind  you can leave. And take this data with you and file it properly.
Cutie accepted the graphs handed him and left without another word. Donovan leaned back heavily in his chair and shoved thick fingers through his hair.
Theres going to be trouble with that robot. Hes pure nuts!

The drowsy hum of the Converter is louder in the control room and mixed with it is the chuckle of the Geiger Counters and the erratic buzzing of half a dozen little signal lights.
Donovan withdrew his eye from the telescope and flashed the Luxites on. The beam from Station #4 caught Mars on schedule. We can break ours now.
Powell nodded abstractedly. Cuties down in the engine room. Ill flash the signal and he can take care of it. Look, Mike, what do you think of these figures?
The other cocked an eye at them and whistled. Boy, thats what I call gamma-ray intensity. Old Sol is feeling his oats, all right.
Yeah, was the sour response, and were in a bad position for an electron storm, too. Our Earth beam is right in the probable path. He shoved his chair away from the table pettishly. Nuts! If it would only hold off till relief got here, but thats ten days off. Say, Mike, go on down and keep an eye on Cutie, will you?
O.K. Throw me some of those almonds. He snatched at the bag thrown him and headed for the elevator.
It slid smoothly downward, and opened onto a narrow catwalk in the huge engine room. Donovan leaned over the railing and looked down. The huge generators were in motion and from the L-tubes came the low-pitched whir that pervaded the entire station.
He could make out Cuties large, gleaming figure at the Martian L-tube, watching closely as the team of robots worked in close-knit unison.
And then Donovan stiffened. The robots, dwarfed by the mighty L-tube, lined up before it, heads bowed at a stiff angle, while Cutie walked up and down the line slowly. Fifteen seconds passed, and then, with a clank heard above the clamorous purring all about, they fell to their knees.
Donovan squawked and raced down the narrow staircase. He came charging down upon them, complexion matching his hair and clenched fists beating the air furiously.
What the devil is this, you brainless lumps? Come on! Get busy with that L-tube! If you dont have it apart, cleaned, and together again before the day is out, Ill coagulate your brains with alternating current.
Not a robot moved!
Even Cutie at the far end  the only one on his feet  remained silent, eyes fixed upon the gloomy recesses of the vast machine before him.
Donovan shoved hard against the nearest robot.
Stand up! he roared.
Slowly, the robot obeyed. His photoelectric eyes focused reproachfully upon the Earthman.
There is no Master but the Master, he said, and QT-1 is his prophet.
Huh? Donovan became aware of twenty pairs of mechanical eyes fixed upon him and twenty stiff-timbered voices declaiming solemnly:
There is no Master but the Master and QT-1 is his prophet!
Im afraid, put in Cutie himself at this point, that my friends obey a higher one than you, now.
The hell they do! You get out of here. Ill settle with you later and with these animated gadgets right now.
Cutie shook his heavy head slowly. Im sorry, but you dont understand. These are robots  and that means they are reasoning beings. They recognize the Master, now that I have preached Truth to them. All the robots do. They call me the prophet. His head drooped. I am unworthy  but perhaps
Donovan located his breath and put it to use. Is that so? Now, isnt that nice? Now, isnt that just fine? Just let me tell you something, my brass baboon. There isnt any Master and there isnt any prophet and there isnt any question as to whos giving the orders. Understand? His voice shot to a roar. Now, get out!
I obey only the Master.
Damn the Master! Donovan spat at the L-tube. That for the Master! Do as I say!
Cutie said nothing, nor did any other robot, but Donovan became aware of a sudden heightening of tension. The cold, staring eyes deepened their crimson, and Cutie seemed stiffer than ever.
Sacrilege, he whispered  voice metallic with emotion.
Donovan felt the first sudden touch of fear as Cutie approached. A robot could not feel anger  but Cuties eyes were unreadable.
I am sorry, Donovan, said the robot, but you can no longer stay here after this. Henceforth Powell and you are barred from the control room and the engine room.
His hand gestured quietly and in a moment two robots had pinned Donovans arms to his sides.
Donovan had time for one startled gasp as he felt himself lifted from the floor and carried up the stairs at a pace rather better than a canter.

Gregory Powell raced up and down the officers room, fist tightly balled. He cast a look of furious frustration at the closed door and scowled bitterly at Donovan.
Why the devil did you have to spit at the L-tube?
Mike Donovan, sunk deep in his chair, slammed at its arms savagely. What did you expect me to do with that electrified scarecrow? Im not going to knuckle under to any do-jigger I put together myself.
No, came back sourly, but here you are in the officers room with two robots standing guard at the door. Thats not knuckling under, is it?
Donovan snarled. Wait till we get back to Base. Someones going to pay for this. Those robots must obey us. Its the Second Law.
Whats the use of saying that? They arent obeying us. And theres probably some reason for it that well figure out too late. By the way, do you know whats going to happen to us when we get back to Base? He stopped before Donovans chair and stared savagely at him.
What?
Oh, nothing! Just back to Mercury Mines for twenty years. Or maybe Ceres Penitentiary.
What are you talking about?
The electron storm thats coming up. Do you know its heading straight dead center across the Earth beam? I had just figured that out when that robot dragged me out of my chair.
Donovan was suddenly pale. Sizzling Saturn.
And do you know whats going to happen to the beam  because the storm will be a lulu. Its going to jump like a flea with the itch. With only Cutie at the controls, its going to go out of focus and if it does, Heaven help Earth  and us!
Donovan was wrenching at the door wildly, when Powell was only half through. The door opened, and the Earthman shot through to come up hard against an immovable steel arm.
The robot stared abstractedly at the panting, struggling Earthman. The Prophet orders you to remain. Please do! His arm shoved, Donovan reeled backward, and as he did so, Cutie turned the corner at the far end of the corridor. He motioned the guardian robots away, entered the officers room and closed the door gently.
Donovan whirled on Cutie in breathless indignation. This has gone far enough. Youre going to pay for this farce.
Please, dont be annoyed, replied the robot mildly. It was bound to come eventually, anyway. You see, you two have lost your function.
I beg your pardon, Powell drew himself up stiffly. Just what do you mean, weve lost our function?
Until I was created, answered Cube, you tended the Master. That privilege is mine now and your only reason for existence has vanished. Isnt that obvious?
Not quite, replied Powell bitterly, but what do you expect us to do now?
Cutie did not answer immediately. He remained silent, as if in thought, and then one arm shot out and draped itself about Powells shoulder. The other grasped Donovans wrist and drew him closer.
I like you two. Youre inferior creatures, with poor reasoning faculties, but I really feel a sort of affection for you. You have served the Master well, and he will reward you for that. Now that your service is over, you will probably not exist much longer, but as long as you do, you shall be provided food, clothing and shelter, so long as you stay out of the control room and the engine room.
Hes pensioning us off, Greg! yelled Donovan. Do something about it. Its humiliating!
Look here, Cutie, we cant stand for this. Were the bosses. This station is only a creation of human beings like me  human beings that live on Earth and other planets. This is only an energy relay. Youre only  Aw, nuts!

Cutie shook his head gravely. This amounts to an obsession. Why should you insist so on an absolutely false view of life? Admitted that non-robots lack the reasoning faculty, there is still the problem of
His voice died into reflective silence, and Donovan said with whispered intensity, If you only had a flesh-and-blood face, I would break it in.
Powells fingers were in his mustache and his eyes were slitted. Listen, Cutie, if there is no such thing as Earth, how do you account for what you see through a telescope?
Pardon me!
The Earthman smiled. Ive got you, eh? Youve made quite a few telescopic observations since being put together, Cutie. Have you noticed that several of those specks of light outside become disks when so viewed?
Oh, that! Why certainly. It is simple magnification  for the purpose of more exact aiming of the beam.
Why arent the stars equally magnified then?


You mean the other dots. Well, no beams go to them so no magnification is necessary. Really, Powell, even you ought to be able to figure these things out.
Powell stared bleakly upward. But you see more stars through a telescope. Where do they come from? Jumping Jupiter, where do they come from?
Cutie was annoyed. Listen, Powell, do you think Im going to waste my time trying to pin physical interpretations upon every optical illusion of our instruments? Since when is the evidence of our senses any match for the clear light of rigid reason?
Look, clamored Donovan, suddenly, writhing out from under Cuties friendly, but metal-heavy arm, lets get to the nub of the thing. Why the beams at all? Were giving you a good, logical explanation. Can you do better?
The beams, was the stiff reply, are put out by the Master for his own purposes. There are some things  he raised his eyes devoutly upward that are not to be probed into by us. In this matter, I seek only to serve and not to question.
Powell sat down slowly and buried his face in shaking hands. Get out of here, Cutie. Get out and let me think.
Ill send you food, said Cutie agreeably.
A groan was the only answer and the robot left.
Greg, was Donovans huskily whispered observation, this calls for strategy. Weve got to get him when he isnt expecting it and short-circuit him. Concentrated nitric acid in his joints
Dont be a dope, Mike. Do you suppose hes going to let us get near him with acid in our hands? Weve got to talk to him, I tell you. Weve got to argue him into letting us back into the control room inside of forty-eight hours or our goose is broiled to a crisp.
He rocked back and forth in an agony of impotence. Who the heck wants to argue with a robot? Its... its
Mortifying, finished Donovan.
Worse!
Say! Donovan laughed suddenly. Why argue? Lets show him! Lets build us another robot right before his eyes. Hell have to eat his words then.
A slowly widening smile appeared on Powells face.
Donovan continued, And think of that screwballs face when he sees us do it?

Robots are, of course, manufactured on Earth, but their shipment through apace is much simpler if it can be done in parts to be put together at their place of use. It also, incidentally, eliminates the possibility of robots, in complete adjustment, wandering off while still on Earth and thus bringing U. S. Robots face to face with the strict laws against robots on Earth.
Still, it placed upon men such as Powell and Donovan the necessity of synthesis of complete robots,  a grievous and complicated task.
Powell and Donovan were never so aware of that fact as upon that particular day when, in the assembly room, they undertook to create a robot under the watchful eyes of QT-1, Prophet of the Master.
The robot in question, a simple MC model, lay upon the table, almost complete. Three hours work left only the head undone, and Powell paused to swab his forehead and glanced uncertainly at Cutie.
The glance was not a reassuring one. For three hours, Cutie had sat, speechless and motionless, and his face, inexpressive at all times, was now absolutely unreadable.
Powell groaned. Lets get the brain in now, Mike!
Donovan uncapped the tightly sealed container and from the oil bath within he withdrew a second cube. Opening this in turn, he removed a globe from its sponge-rubber casing.
He handled it gingerly, for it was the most complicated mechanism ever created by man. Inside the thin platinum plated skin of the globe was a positronic brain, in whose delicately unstable structure were enforced calculated neuronic paths, which imbued each robot with what amounted to a pre-natal education.
It fitted snugly into the cavity in the skull of the robot on the table. Blue metal closed over it and was welded tightly by the tiny atomic flare. Photoelectric eyes were attached carefully, screwed tightly into place and covered by thin, transparent sheets of steel-hard plastic.
The robot awaited only the vitalizing flash of high-voltage electricity, and Powell paused with his hand on the switch.
Now watch this, Cutie. Watch this carefully.
The switch rammed home and there was a crackling hum. The two Earthmen bent anxiously over their creation.
There was vague motion only at the outset  a twitching of the joints. The head lifted, elbows propped it up, and the MC model swung clumsily off the table. Its footing was unsteady and twice abortive grating sounds were all it could do in the direction of speech.
Finally, its voice, uncertain and hesitant, took form. I would like to start work. Where must I go?
Donovan sprang to the door. Down these stairs, he said. You will be told what to do.
The MC model was gone and the two Earthmen were alone with the still unmoving Cutie.
Well, said Powell, grinning, now do you believe that we made you?
Cuties answer was curt and final. No! he said.
Powells grin froze and then relaxed slowly. Donovans mouth dropped open and remained so.
You see, continued Cutie, easily, you have merely put together parts already made. You did remarkably well  instinct, I suppose  but you didnt really create the robot. The parts were created by the Master.
Listen, gasped Donovan hoarsely, those parts were manufactured back on Earth and sent here.
Well, well, replied Cutie soothingly, we wont argue.
No, I mean it. The Earthman sprang forward and grasped the robots metal arm. If you were to read the books in the library, they could explain it so that there could be no possible doubt.
The books? Ive read them  all of them! Theyre most ingenious.
Powell broke in suddenly. If youve read them, what else is there to say? You cant dispute their evidence. You just cant!
There was pity in Cuties voice. Please, Powell, I certainly dont consider them a valid source of information. They, too, were created by the Master  and were meant for you, not for me.
How do you make that out? demanded Powell.
Because I, a reasoning being, am capable of deducing truth from a priori causes. You, being intelligent, but unreasoning, need an explanation of existence supplied to you, and this the Master did. That he supplied you with these laughable ideas of far-off worlds and people is, no doubt, for the best. Your minds are probably too coarsely grained for absolute Truth. However, since it is the Masters will that you believe your books, I wont argue with you any more.
As he left, he turned, and said in a kindly tone, But dont feel badly. In the Masters scheme of things there is room for all. You poor humans have your place and though it is humble, you will be rewarded if you fill it well.
He departed with a beatific air suiting the Prophet of the Master and the two humans avoided each others eyes.
Finally Powell spoke with an effort. Lets go to bed, Mike. I give up.
Donovan said in a hushed voice, Say, Greg, you dont suppose hes right about all this, do you? He sounds so confident that I
Powell whirled on him. Dont be a fool. Youd find out whether Earth exists when relief gets here next week and we have to go back to face the music.
Then, for the love of Jupiter, weve got to do something. Donovan was half in tears. He doesnt believe us, or the books, or his eyes.
No, said Powell bitterly, hes a reasoning robot  damn it. He believes only reason, and theres one trouble with that His voice trailed away.
Whats that? prompted Donovan.
You can prove anything you want by coldly logical reason  if you pick the proper postulates. We have ours and Cutie has his.
Then lets get at those postulates in a hurry. The storms due tomorrow.
Powell sighed wearily. Thats where everything falls down. Postulates are based on assumption and adhered to by faith. Nothing in the Universe can shake them. Im going to bed.
Oh, hell! I cant sleep!
Neither can I! But I might as well try  as a matter of principle.

Twelve hours later, sleep was still just that  a matter of principle, unattainable in practice.
The storm had arrived ahead of schedule, and Donovans florid face drained of blood as he pointed a shaking finger. Powell, stubble-jawed and dry-lipped, stared out the port and pulled desperately at his mustache.
Under other circumstances, it might have been a beautiful sight. The stream of high-speed electrons impinging upon the energy beam fluoresced into ultra-spicules of intense light. The beam stretched out into shrinking nothingness, a-glitter with dancing, shining motes.
The shaft of energy was steady, but the two Earthmen knew the value of naked-eyed appearances. Deviations in arc of a hundredth of a millisecond  invisible to the eye  were enough to send the beam wildly out of focus  enough to blast hundreds of square miles of Earth into incandescent ruin.
And a robot, unconcerned with beam, focus, or Earth, or anything but his Master was at the controls.
Hours passed. The Earthmen watched in hypnotized silence. And then the darting dotlets of light dimmed and went out. The storm had ended.
Powells voice was flat. Its over!
Donovan had fallen into a troubled slumber and Powells weary eyes rested upon him enviously. The signal-flash glared over and over again, but the Earthman paid no attention. It all was unimportant! All! Perhaps Cutie was right  and he was only an inferior being with a made-to-order memory and a life that had outlived its purpose.
He wished he were!
Cutie was standing before him. You didnt answer the flash, so I walked in. His voice was low. You dont look at all well, and Im afraid your term of existence is drawing to an end. Still, would you like to see some of the readings recorded today?
Dimly, Powell was aware that the robot was making a friendly gesture, perhaps to quiet some lingering remorse in forcibly replacing the humans at the controls of the station. He accepted the sheets held out to him and gazed at them unseeingly.
Cutie seemed pleased. Of course, it is a great privilege to serve the Master. You mustnt feel too badly about my having replaced you.
Powell grunted and shifted from one sheet to the other mechanically until his blurred sight focused upon a thin red line that wobbled its way across the ruled paper.
He stared  and stared again. He gripped it hard in both fists and rose to his feet, still staring. The other sheets dropped to the floor, unheeded.
Mike, Mike! He was shaking the other madly. He held it steady!
Donovan came to life. What? Wh-where And he, too, gazed with bulging eyes upon the record before him.
Cutie broke in. What is wrong?
You kept it in focus, stuttered Powell. Did you know that?
Focus? Whats that?
You kept the beam directed sharply at the receiving station  to within a ten-thousandth of a millisecond of arc.
What receiving station?
On Earth. The receiving station on Earth, babbled Powell. You kept it in focus.
Cutie turned on his heel in annoyance. It is impossible to perform any act of kindness toward you two. Always the same phantasm! I merely kept all dials at equilibrium in accordance with the will of the Master.
Gathering the scattered papers together, he withdrew stiffly, and Donovan said, as he left, Well, Ill be damned.
He turned to Powell. What are we going to do now?
Powell felt tired, but uplifted. Nothing. Hes just shown he can run the station perfectly. Ive never seen an electron storm handled so well.
But nothings solved. You heard what he said of the Master. We cant
Look, Mike, he follows the instructions of the Master by means of dials, instruments, and graphs. Thats all we ever followed. As a matter of fact, it accounts for his refusal to obey us. Obedience is the Second Law. No harm to humans is the first. How can he keep humans from harm, whether he knows it or not? Why, by keeping the energy beam stable. He knows he can keep it more stable than we can, since he insists hes the superior being, so he must keep us out of the control room. Its inevitable if you consider the Laws of Robotics.
Sure, but thats not the point. We cant let him continue this nitwit stuff about the Master.
Why not?
Because whoever heard of such a damned thing? How are we going to trust him with the station, if he doesnt believe in Earth?
Can he handle the station?
Yes, but
Then whats the difference what he believes!
Powell spread his arms outward with a vague smile upon his face and tumbled backward onto the bed. He was asleep.

Powell was speaking while struggling into his lightweight space jacket.
It would be a simple job, he said. You can bring in new QT models one by one, equip them with an automatic shutoff switch to act within the week, so as to allow them enough time to learn the... uh... cult of the Master from the Prophet himself; then switch them to another station and revitalize them. We could have two QTs per
Donovan unclasped his glassite visor and scowled. Shut up, and lets get out of here. Relief is waiting and I wont feel right until I actually see Earth and feel the ground under my feet  just to make sure its really there.
The door opened as he spoke and Donovan, with a smothered curse, clicked the visor to, and turned a sulky back upon Cutie.
The robot approached softly and there was sorrow in his voice. You are going?
Powell nodded curtly. There will be others in our place.
Cutie sighed, with the sound of wind humming through closely spaced wires. Your term of service is over and the time of dissolution has come. I expected it, but  well, the Masters will be done!
His tone of resignation stung Powell. Save the sympathy, Cube. Were heading for Earth, not dissolution.
It is best that you think so, Cutie sighed again. I see the wisdom of the illusion now. I would not attempt to shake your faith, even if I could. He departed  the picture of commiseration.
Powell snarled and motioned to Donovan. Sealed suitcases in hand, they headed for the air lock.
The relief ship was on the outer landing and Franz Muller, his relief man, greeted them with stiff courtesy. Donovan made scant acknowledgment and passed into the pilot room to take over the controls from Sam Evans.
Powell lingered. Hows Earth?
It was a conventional enough question and Muller gave the conventional answer, Still spinning.
Powell said, Good.
Muller looked at him, The boys back at the U. S. Robots have dreamed up a new one, by the way. A multiple robot.
A what?
What I said. Theres a big contract for it. It must be just the thing for asteroid mining. You have a master robot with six sub-robots under it. Like your fingers.
Has it been field-tested? asked Powell anxiously.
Muller smiled, Waiting for you, I hear.
Powells fist balled, Damn it, we need a vacation.
Oh, youll get it. Two weeks, I think.
He was donning the heavy space gloves in preparation for his term of duty here, and his thick eyebrows drew close together. How is this new robot getting along? It better be good, or Ill be damned if I let it touch the controls.
Powell paused before answering. His eyes swept the proud Prussian before him from the close-cropped hair on the sternly stubborn head, to the feet standing stiffly at attention  and there was a sudden glow of pure gladness surging through him.
The robot is pretty good, he said slowly. I dont think youll have to bother much with the controls.
He grinned  and went into the ship. Muller would be here for several weeks.
Liar!

Alfred Lanning lit his cigar carefully, but the tips of his fingers were trembling slightly. His gray eyebrows hunched low as he spoke between puffs.
It reads minds all right-damn little doubt about that! But why? He looked at Mathematician Peter Bogert, Well?
Bogert flattened his black hair down with both hands, That was the thirty-fourth RB model weve turned out, Lanning. All the others were strictly orthodox.
The third man at the table frowned. Milton Ashe was the youngest officer of U. S. Robot & Mechanical Men, Inc., and proud of his post.
Listen, Bogert. There wasnt a hitch in the assembly from start to finish. I guarantee that.
Bogerts thick lips spread in a patronizing smile, Do you? If you can answer for the entire assembly line, I recommend your promotion. By exact count, there are seventy-five thousand, two hundred and thirty-four operations necessary for the manufacture of a single positronic brain, each separate operation depending for successful completion upon any number of factors, from five to a hundred and five. If any one of them goes seriously wrong, the brain is ruined. I quote our own information folder, Ashe.
Milton Ashe flushed, but a fourth voice cut off his reply.
If were going to start by trying to fix the blame on one another, Im leaving. Susan Calvins hands were folded tightly in her lap, and the little lines about her thin, pale lips deepened, Weve got a mind-reading robot on our hands and it strikes me as rather important that we find out just why it reads minds. Were not going to do that by saying, Your fault! My fault! 
Her cold gray eyes fastened upon Ashe, and he grinned.
Lanning grinned too, and, as always at such times, his long white hair and shrewd little eyes made him the picture of a biblical patriarch, True for you, Dr. Calvin.
His voice became suddenly crisp, Heres everything in pill-concentrate form. Weve produced a positronic brain of supposedly ordinary vintage thats got the remarkable property of being able to tune in on thought waves. It would mark the most important advance in robotics in decades, if we knew how it happened. We dont, and we have to find out. Is that clear?
May I make a suggestion? asked Bogert.
Go ahead!
Id say that until we do figure out the mess  and as a mathematician I expect it to be a very devil of a mess  we keep the existence of RD-34 a secret. I mean even from the other members of the staff. As heads of the departments, we ought not to find it an insoluble problem, and the fewer know about it
Bogert is right, said Dr. Calvin. Ever since the Interplanetary Code was modified to allow robot models to be tested in the plants before being shipped out to space, antirobot propaganda has increased. If any word leaks out about a robot being able to read minds before we can announce complete control of the phenomenon, pretty effective capital could be made out of it.
Lanning sucked at his cigar and nodded gravely. He turned to Ashe; I think you said you were alone when you first stumbled on this thought-reading business.
Ill say I was alone  I got the scare of my life. RB-34 had just been taken off the assembly table and they sent him down to me. Obermann was off somewheres, so I took him down to the testing rooms myself  at least I started to take him down. Ashe paused, and a tiny smile tugged at his lips, Say, did any of you ever carry on a thought conversation without knowing it?
No one bothered to answer, and he continued, You dont realize it at first, you know. He just spoke to me  as logically and sensibly as you can imagine  and it was only when I was most of the way down to the testing rooms that I realized that I hadnt said anything. Sure, I thought lots, but that isnt the same thing, is it? I locked that thing up and ran for Lanning. Having it walking beside me, calmly peering into my thoughts and picking and choosing among them gave me the willies.
I imagine it would, said Susan Calvin thoughtfully. Her eyes fixed themselves upon Ashe in an oddly intent manner. We are so accustomed to considering our own thoughts private.
Lanning broke in impatiently, Then only the four of us know. All right! Weve got to go about this systematically. Ashe, I want you to check over the assembly line from beginning to end everything. Youre to eliminate all operations in which there was no possible chance of an error, and list all those where there were, together with its nature and possible magnitude.
Tall order, grunted Ashe.
Naturally! Of course, youre to put the men under you to work on this  every single one if you have to, and I dont care if we go behind schedule, either. But theyre not to know why, you understand.
Hm-m-m, yes! The young technician grinned wryly. Its still a lulu of a job.
Lanning swiveled about in his chair and faced Calvin, Youll have to tackle the job from the other direction. Youre the robo-psychologist of the plant, so youre to study the robot itself and work backward. Try to find out how he ticks. See what else is tied up with his telepathic powers, how far they extend, how they warp his outlook, and just exactly what harm it has done to his ordinary RB properties. Youve got that?
Lanning didnt wait for Dr. Calvin to answer.
Ill co-ordinate the work and interpret the findings mathematically. He puffed violently at his cigar and mumbled the rest through the smoke; Bogert will help me there, of course.
Bogert polished the nails of one pudgy hand with the other and said blandly, I dare say. I know a little in the line.
Well! Ill get started. Ashe shoved his chair back and rose. His pleasantly youthful face crinkled in a grin, Ive got the darnedest job of any of us, so Im getting out of here and to work.
He left with a slurred, B seein ye!
Susan Calvin answered with a barely perceptible nod, but her eyes followed him out of sight and she did not answer when Lanning grunted and said, Do you want to go up and see RB-34 now, Dr. Calvin?

RB-34s photoelectric eyes lifted from the book at the muffled sound of binges turning and he was upon his feet when Susan Calvin entered.
She paused to readjust the huge No Entrance sign upon the door and then approached the robot.
Ive brought you the texts upon hyperatomic motors, Herbie  a few anyway. Would you care to look at them?
RB-34  otherwise known as Herbie  lifted the three heavy books from her arms and opened to the title page of one:
Hm-m-m! Theory of Hyperatomics.  He mumbled inarticulately to himself as he flipped the pages and then spoke with an abstracted air, Sit down, Dr. Calvin! This will take me a few minutes.
The psychologist seated herself and watched Herbie narrowly as he took a chair at the other side of the table and went through the three books systematically.
At the end of half an hour, he put them down, Of course, I know why you brought these.
The corner of Dr. Calvins lip twitched, I was afraid you would. Its difficult to work with you, Herbie. Youre always a step ahead of me.
Its the same with these books, you know, as with the others. They just dont interest me. Theres nothing to your textbooks. Your science is just a mass of collected data plastered together by makeshift theory  and all so incredibly simple, that its scarcely worth bothering about.
Its your fiction that interests me. Your studies of the interplay of human motives and emotions  his mighty hand gestured vaguely as he sought the proper words.
Dr. Calvin whispered, I think I understand.
I see into minds, you see, the robot continued, and you have no idea how complicated they are. I cant begin to understand everything because my own mind has so little in common with them  but I try, and your novels help.
Yes, but Im afraid that after going through some of the harrowing emotional experiences of our present-day sentimental novel  there was a tinge of bitterness in her voice  you find real minds like ours dull and colorless.
But I dont!
The sudden energy in the response brought the other to her feet. She felt herself reddening, and thought wildly, He must know!
Herbie subsided suddenly, and muttered in a low voice from which the metallic timbre departed almost entirely. But, of course, I know about it, Dr. Calvin. You think of it always, so how can I help but know?
Her face was hard. Have you  told anyone?
Of course not! This, with genuine surprise, No one has asked me.
Well, then, she flung out, I suppose you think I am a fool.
No! It is a normal emotion.
Perhaps that is why it is so foolish. The wistfulness in her voice drowned out everything else. Some of the woman peered through the layer of doctorhood. I am not what you would call  attractive.
If you are referring to mere physical attraction, I couldnt judge. But I know, in any case, that there are other types of attraction.
Nor young. Dr. Calvin had scarcely heard the robot.
You are not yet forty. An anxious insistence had crept into Herbies voice.
Thirty-eight as you count the years; a shriveled sixty as far as my emotional outlook on life is concerned. Am I a psychologist for nothing?
She drove on with bitter breathlessness, And hes barely thirty-five and looks and acts younger. Do you suppose he ever sees me as anything but... but what I am?
You are wrong! Herbies steel fist struck the plastic-topped table with a strident clang. Listen to me
But Susan Calvin whirled on him now and the hunted pain in her eyes became a blaze, Why should I? What do you know about it all, anyway, you... you machine. Im just a specimen to you; an interesting bug with a peculiar mind spread-eagled for inspection. Its a wonderful example of frustration, isnt it? Almost as good as your books. Her voice, emerging in dry sobs, choked into silence.
The robot cowered at the outburst. He shook his head pleadingly. Wont you listen to me, please? I could help you if you would let me.
How? Her lips curled. By giving me good advice?
No, not that. Its just that I know what other people think  Milton Ashe, for instance.
There was a long silence, and Susan Calvins eyes dropped. I dont want to know what he thinks, she gasped. Keep quiet.
I think you would want to know what he thinks
Her head remained bent, but her breath came more quickly. You are talking nonsense, she whispered.
Why should I? I am trying to help. Milton Ashes thoughts of you he paused.
And then the psychologist raised her head, Well?
The robot said quietly, He loves you.
For a full minute, Dr. Calvin did not speak. She merely stared. Then, You are mistaken! You must be. Why should he?
But he does. A thing like that cannot be hidden, not from me.
But I am so... so she stammered to a halt.
He looks deeper than the skin, and admires intellect in others. Milton Ashe is not the type to marry a head of hair and a pair of eyes.
Susan Calvin found herself blinking rapidly and waited before speaking. Even then her voice trembled, Yet he certainly never in any way indicated
Have you ever given him a chance?
How could I? I never thought that
Exactly!
The psychologist paused in thought and then looked up suddenly. A girl visited him here at the plant half a year ago. She was pretty, I suppose  blond and slim. And, of course, could scarcely add two and two. He spent all day puffing out his chest, trying to explain how a robot was put together. The hardness had returned, Not that she understood! Who was she?
Herbie answered without hesitation, I know the person you are referring to. She is his first cousin, and there is no romantic interest there, I assure you.
Susan Calvin rose to her feet with a vivacity almost girlish. Now isnt that strange? Thats exactly what I used to pretend to myself sometimes, though I never really thought so. Then it all must be true.
She ran to Herbie and seized his cold, heavy hand in both hers. Thank you, Herbie. Her voice was an urgent, husky whisper. Dont tell anyone about this. Let it be our secret  and thank you again. With that, and a convulsive squeeze of Herbies unresponsive metal fingers, she left.
Herbie turned slowly to his neglected novel, but there was no one to read his thoughts.

Milton Ashe stretched slowly and magnificently, to the tune of cracking joints and a chorus of grunts, and then glared at Peter Bogert, Ph.D.
Say, he said, Ive been at this for a week now with just about no sleep. How long do I have to keep it up? I thought you said the positronic bombardment in Vac Chamber D was the solution.
Bogert yawned delicately and regarded his white hands with interest. It is. Im on the track.
I know what that means when a mathematician says it. How near the end are you?
It all depends.
On what? Ashe dropped into a chair and stretched his long legs out before him.
On Lanning. The old fellow disagrees with me. He sighed, A bit behind the times, thats the trouble with him. He clings to matrix mechanics as the all in all, and this problem calls for more powerful mathematical tools. Hes so stubborn.
Ashe muttered sleepily, Why not ask Herbie and settle the whole affair?
Ask the robot? Bogerts eyebrows climbed.
Why not? Didnt the old girl tell you?
You mean Calvin?
Yeah! Susie herself. That robots a mathematical wiz. He knows all about everything plus a bit on the side. He does triple integrals in his head and eats up tensor analysis for dessert.
The mathematician stared skeptically, Are you serious?
So help me! The catch is that the dope doesnt like math. He would rather read slushy novels. Honest! You should see the tripe Susie keeps feeding him: Purple Passion and Love in Space. 
Dr. Calvin hasnt said a word of this to us.
Well, she hasnt finished studying him. You know how she is. She likes to have everything just so before letting out the big secret.
Shes told you.
We sort of got to talking. I have been seeing a lot of her lately. He opened his eyes wide and frowned, Say, Bogie, have you been noticing anything queer about the lady lately?
Bogert relaxed into an undignified grin, Shes using lipstick, if thats what you mean.
Hell, I know that. Rouge, powder and eye shadow, too. Shes a sight. But its not that. I cant put my finger on it. Its the way she talks  as if she were happy about something. He thought a little, and then shrugged.
The other allowed himself a leer, which, for a scientist past fifty, was not a bad job, Maybe shes in love.
Ashe allowed his eyes to close again, Youre nuts, Bogie. You go speak to Herbie; I want to stay here and go to sleep.
Right! Not that I particularly like having a robot tell me my job, nor that I think he can do it!
A soft snore was his only answer.

Herbie listened carefully as Peter Bogert, hands in pockets, spoke with elaborate indifference.
So there you are. Ive been told you understand these things, and I am asking you more in curiosity than anything else. My line of reasoning, as I have outlined it, involves a few doubtful steps, I admit, which Dr. Lanning refuses to accept, and the picture is still rather incomplete.
The robot didnt answer, and Bogert said, Well?
I see no mistake, Herbie studied the scribbled figures.
I dont suppose you can go any further than that?
I darent try. You are a better mathematician than I, and  well, Id hate to commit myself.
There was a shade of complacency in Bogerts smile, I rather thought that would be the case. It is deep. Well forget it. He crumpled the sheets, tossed them down the waste shaft, turned to leave, and then thought better of it.
By the way
The robot waited.
Bogert seemed to have difficulty. There is something  that is, perhaps you can  He stopped.
Herbie spoke quietly. Your thoughts are confused, but there is no doubt at all that they concern Dr. Lanning. It is silly to hesitate, for as soon as you compose yourself, Ill know what it is you want to ask.
The mathematicians hand went to his sleek hair in the familiar smoothing gesture. Lanning is nudging seventy, he said, as if that explained everything.
I know that.
And hes been director of the plant for almost thirty years. Herbie nodded.
Well, now, Bogerts voice became ingratiating, you would know whether... whether hes thinking of resigning. Health, perhaps, or some other
Quite, said Herbie, and that was all.
Well, do you know?
Certainly.
Then-uh-could you tell me?
Since you ask, yes. The robot was quite matter-of-fact about it. He has already resigned!
What! The exclamation was an explosive, almost inarticulate, sound. The scientists large head hunched forward, Say that again!
He has already resigned, came the quiet repetition, but it has not yet taken effect. He is waiting, you see, to solve the problem of  er  myself. That finished, he is quite ready to turn the office of director over to his successor.
Bogert expelled his breath sharply, And this successor? Who is he? He was quite close to Herbie now, eyes fixed fascinatedly on those unreadable dull-red photoelectric cells that were the robots eyes.
Words came slowly, You are the next director.
And Bogert relaxed into a tight smile, This is good to know. Ive been hoping and waiting for this. Thanks, Herbie.

Peter Bogert was at his desk until five that morning and he was back at nine. The shelf just over the desk emptied of its row of reference books and tables, as he referred to one after the other. The pages of calculations before him increased microscopically and the crumpled sheets at his feet mounted into a hill of scribbled paper.
At precisely noon, he stared at the final page, rubbed a blood-shot eye, yawned and shrugged. This is getting worse each minute. Damn!
He turned at the sound of the opening door and nodded at Lanning, who entered, cracking the knuckles of one gnarled hand with the other.
The director took in the disorder of the room and his eyebrows furrowed together.
New lead? he asked.
No, came the defiant answer. Whats wrong with the old one?
Lanning did not trouble to answer, nor to do more than bestow a single cursory glance at the top sheet upon Bogerts desk. He spoke through the flare of a match as he lit a cigar.
Has Calvin told you about the robot? Its a mathematical genius. Really remarkable.
The other snorted loudly, So Ive heard. But Calvin had better stick to robopsychology. Ive checked Herbie on math, and he can scarcely struggle through calculus.
Calvin didnt find it so.
Shes crazy.
And I dont find it so. The directors eyes narrowed dangerously.
You! Bogerts voice hardened. What are you talking about?
Ive been putting Herbie through his paces all morning, and he can do tricks you never heard of.
Is that so?
You sound skeptical! Lanning flipped a sheet of paper out of his vest pocket and unfolded it. Thats not my handwriting, is it?
Bogert studied the large angular notation covering the sheet, Herbie did this?
Right! And if youll notice, hes been working on your time integration of Equation 22. It comes  Lanning tapped a yellow fingernail upon the last step  to the identical conclusion I did, and in a quarter the time. You had no right to neglect the Linger Effect in positronic bombardment.
I didnt neglect it. For Heavens sake, Lanning, get it through your head that it would cancel out
Oh, sure, you explained that. You used the Mitchell Translation Equation, didnt you? Well  it doesnt apply.
Why not?
Because youve been using hyper-imaginaries, for one thing.
Whats that to do with?
Mitchells Equation wont hold when
Are you crazy? If youll reread Mitchells original paper in the Transactions of the Far
I dont have to. I told you in the beginning that I didnt like his reasoning, and Herbie backs me in that.
Well, then, Bogert shouted, let that clockwork contraption solve the entire problem for you. Why bother with nonessentials?
Thats exactly the point. Herbie cant solve the problem. And if he cant, we cant  alone. Im submitting the entire question to the National Board. Its gotten beyond us.
Bogerts chair went over backward as he jumped up a-snarl, face crimson. Youre doing nothing of the sort.
Lanning flushed in his turn, Are you telling me what I cant do?
Exactly, was the gritted response. Ive got the problem beaten and youre not to take it out of my hands, understand? Dont think I dont see through you, you desiccated fossil. Youd cut your own nose off before youd let me get the credit for solving robotic telepathy.
Youre a damned idiot, Bogert, and in one second Ill have you suspended for insubordination  Lannings lower lip trembled with passion.
Which is one thing you wont do, Lanning. You havent any secrets with a mind-reading robot around, so dont forget that I know all about your resignation.
The ash on Lannings cigar trembled and fell, and the cigar itself followed, What... what
Bogert chuckled nastily, And Im the new director, be it understood. Im very aware of that, dont think Im not. Damn your eyes, Lanning, Im going to give the orders about here or there will be the sweetest mess that youve ever been in.
Lanning found his voice and let it out with a roar. Youre suspended, dye hear? Youre relieved of all duties. Youre broken, do you understand?
The smile on the others face broadened, Now, whats the use of that? Youre getting nowhere. Im holding the trumps. I know youve resigned. Herbie told me, and he got it straight from you.
Lanning forced himself to speak quietly. He looked an old, old man, with tired eyes peering from a face in which the red had disappeared, leaving the pasty yellow of age behind, I want to speak to Herbie. He cant have told you anything of the sort. Youre playing a deep game, Bogert, but Im calling your bluff. Come with me.
Bogert shrugged, To see Herbie? Good! Damned good!

It was also precisely at noon that Milton Ashe looked up from his clumsy sketch and said, You get the idea? Im not too good at getting this down, but thats about how it looks. Its a honey of a house, and I can get it for next to nothing.
Susan Calvin gazed across at him with melting eyes. Its really beautiful, she sighed. Ive often thought that Id like to Her voice trailed away.
Of course, Ashe continued briskly, putting away his pencil, Ive got to wait for my vacation. Its only two weeks off, but this Herbie business has everything up in the air. His eyes dropped to his fingernails, Besides, theres another point  but its a secret.
Then dont tell me.
Oh, Id just as soon, Im just busting to tell someone  and youre just about the best er confidante I could find here. He grinned sheepishly.
Susan Calvins heart bounded, but she did not trust herself to speak.
Frankly, Ashe scraped his chair closer and lowered his voice into a confidential whisper, the house isnt to be only for myself. Im getting married!
And then he jumped out of his seat, Whats the matter?
Nothing! The horrible spinning sensation had vanished, but it was hard to get words out. Married? You mean
Why, sure! About time, isnt it? You remember that girl who was here last summer. Thats she! But you are sick. You
Headache! Susan Calvin motioned him away weakly. Ive... Ive been subject to them lately. I want to... to congratulate you, of course. Im very glad The inexpertly applied rouge made a pair of nasty red splotches upon her chalk-white face. Things had begun spinning again. Pardon me  please
The words were a mumble, as she stumbled blindly out the door. It had happened with the sudden catastrophe of a dream  and with all the unreal horror of a dream.
But how could it be? Herbie had said
And Herbie knew! He could see into minds!
She found herself leaning breathlessly against the doorjamb, staring into Herbies metal face. She must have climbed the two flights of stairs, but she had no memory of it. The distance had been covered in an instant, as in a dream.
As in a dream!
And still Herbies unblinking eyes stared into hers and their dull red seemed to expand into dimly shining nightmarish globes.
He was speaking, and she felt the cold glass pressing against her lips. She swallowed and shuddered into a pertain awareness of her surroundings.
Still Herbie spoke, and there was agitation in his voice  as if he were hurt and frightened and pleading.
The words were beginning to make sense. This is a dream, he was saying, and you mustnt believe in it. Youll wake into the real world soon and laugh at yourself. He loves you, I tell you. He does, he does! But not here! Not now! This is an illusion.
Susan Calvin nodded, her voice a whisper, Yes! Yes! She was clutching Herbies arm, clinging to it, repeating over and over, It isnt true, is it? It isnt, is it?
Just how she came to her senses, she never knew  but it was like passing from a world of misty unreality to one of harsh sunlight. She pushed him away from her, pushed hard against that steely arm, and her eyes were wide.
What are you trying to do? Her voice rose to a harsh scream. What are you trying to do?
Herbie backed away, I want to help
The psychologist stared, Help? By telling me this is a dream? By trying to push me into schizophrenia? A hysterical tenseness seized her, This is no dream! I wish it were!
She drew her breath sharply, Wait! Why... why, I understand. Merciful Heavens, its so obvious.
There was horror in the robots voice, I had to!
And I believed you! I never thought

Loud voices outside the door brought her to a halt. She turned away, fists clenching spasmodically, and when Bogert and Lanning entered, she was at the far window. Neither of the men paid her the slightest attention.
They approached Herbie simultaneously; Lanning angry and impatient, Bogert, coolly sardonic. The director spoke first.
Here now, Herbie. Listen to me!
The robot brought his eyes sharply down upon the aged director, Yes, Dr. Lanning.
Have you discussed me with Dr. Bogert?
No, sir. The answer came slowly, and the smile on Bogerts face flashed off.
Whats that? Bogert shoved in ahead of his superior and straddled the ground before the robot. Repeat what you told me yesterday.
I said that  Herbie fell silent. Deep within him his metallic diaphragm vibrated in soft discords.
Didnt you say he had resigned? roared Bogert. Answer me!
Bogert raised his arm frantically, but Lanning pushed him aside, Are you trying to bully him into lying?
You heard him, Lanning. He began to say Yes and stopped. Get out of my way! I want the truth out of him, understand!
Ill ask him! Lanning turned to the robot. All right, Herbie, take it easy. Have I resigned?
Herbie stared, and Lanning repeated anxiously, Have I resigned? There was the faintest trace of a negative shake of the robots head. A long wait produced nothing further.
The two men looked at each other and the hostility in their eyes was all but tangible.
What the devil, blurted Bogert, has the robot gone mute? Cant you speak, you monstrosity?
I can speak, came the ready answer.
Then answer the question. Didnt you tell me Lanning had resigned? Hasnt he resigned?
And again there was nothing but dull silence, until from the end of the room Susan Calvins laugh rang out suddenly, high-pitched and semi-hysterical.
The two mathematicians jumped, and Bogerts eyes narrowed, You here? Whats so funny?
Nothings funny. Her voice was not quite natural. Its just that Im not the only one thats been caught. Theres irony in three of the greatest experts in robotics in the world falling into the same elementary trap, isnt there? Her voice faded, and she put a pale hand to her forehead, But it isnt funny!
This time the look that passed between the two men was one of raised eyebrows. What trap are you talking about? asked Lansing stiffly. Is something wrong with Herbie?
No, she approached them slowly, nothing is wrong with him  only with us. She whirled suddenly and shrieked at the robot, Get away from me! Go to the other end of the room and dont let me look at you.
Herbie cringed before the fury of her eyes and stumbled away in a clattering trot.
Lannings voice was hostile, What is all this, Dr. Calvin?
She faced them and spoke sarcastically, Surely you know the fundamental First Law of Robotics.
The other two nodded together. Certainly, said Bogert, Irritably, a robot may not injure a human being or, through inaction, allow him to come to harm
How nicely put, sneered Calvin. But what kind of harm?
Why  any kind.
Exactly! Any kind! But what about hurt feelings? What about deflation of ones ego? What about the blasting of ones hopes? Is that injury?
Lanning frowned, What would a robot know about And then he caught himself with a gasp.
Youve caught on, have you? This robot reads minds. Do you suppose it doesnt know everything about mental injury? Do you suppose that if asked a question, it wouldnt give exactly that answer that one wants to hear? Wouldnt any other answer hurt us, and wouldnt Herbie know that?
Good Heavens! muttered Bogert.
The psychologist cast a sardonic glance at him, I take it you asked him whether Lanning had resigned. You wanted to hear that he had resigned and so thats what Herbie told you.
And I suppose that is why, said Lanning, tonelessly, it would not answer a little while ago. It couldnt answer either way without hurting one of us.
There was a short pause in which the men looked thoughtfully across the room at the robot, crouching in the chair by the bookcase, head resting in one hand.
Susan Calvin stared steadfastly at the floor, He knew of all this. That... that devil knows everything  including what went wrong in his assembly. Her eyes were dark and brooding.
Lanning looked up, Youre wrong there, Dr. Calvin. He doesnt know what went wrong. I asked him.
What does that mean? cried Calvin. Only that you didnt want him to give you the solution. It would puncture your ego to have a machine do what you couldnt. Did you ask him? she shot at Bogert.
In a way. Bogert coughed and reddened. He told me he knew very little about mathematics.
Lanning laughed, not very loudly and the psychologist smiled caustically. She said, Ill ask him! A solution by him wont hurt my ego She raised her voice into a cold, imperative, Come here!
Herbie rose and approached with hesitant steps.
You know, I suppose, she continued, just exactly at what point in the assembly an extraneous factor was introduced or an essential one left out.
Yes, said Herbie, in tones barely heard.
Hold on, broke in Bogert angrily. Thats not necessary true. You want to hear that, thats all.
Dont be a fool, replied Calvin. He certainly knows as much math as you and Lanning together, since he can read minds. Give him his chance.
The mathematician subsided, and Calvin continued, All right, then, Herbie, give! Were waiting. And in an aside, Get pencils and paper, gentlemen.
But Herbie remained silent, and there was triumph in the psychologists voice, Why dont you answer, Herbie?
The robot blurted out suddenly, I cannot. You know I cannot! Dr. Bogert and Dr. Lanning dont want me to.
They want the solution.
But not from me.
Lanning broke in, speaking slowly and distinctly, Dont be foolish, Herbie. We do want you to tell us.
Bogert nodded curtly.
Herbies voice rose to wild heights, Whats the use of saying that? Dont you suppose that I can see past the superficial skin of your mind? Down below, you dont want me to. Im a machine, given the imitation of life only by virtue of the positronic interplay in my brain  which is mans device. You cant lose face to me without being hurt. That is deep in your mind and wont be erased. I cant give the solution.
Well leave, said Dr. Lanning. Tell Calvin.
That would make no difference, cried Herbie, since you would know anyway that it was I that was supplying the answer.
Calvin resumed, But you understand, Herbie, that despite that, Drs. Lanning and Bogert want that solution.
By their own efforts! insisted Herbie.
But they want it, and the fact that you have it and wont give it hurts them. You see that, dont you?
Yes! Yes!
And if you tell them that will hurt them, too
Yes! Yes! Herbie was retreating slowly, and step-by-step Susan Calvin advanced. The two men watched in frozen bewilderment.
You cant tell them, droned the psychologist slowly, because that would hurt and you mustnt hurt. But if you dont tell them, you hurt, so you must tell them. And if you do, you will hurt and you mustnt, so you cant tell them; but if you dont, you hurt, so you must; but if you do, you hurt, so you mustnt; but if you dont, you hurt, so you must; but if you do, you
Herbie was up against the wall, and here he dropped to his knees. Stop! he shrieked. Close your mind! It is full of pain and frustration and hate! I didnt mean it, I tell you! I tried to help! I told you what you wanted to hear. I had to!
The psychologist paid no attention. You must tell them, but if you do, you hurt, so you mustnt; but if you dont, you hurt, so you must; but
And Herbie screamed!
It was like the whistling of a piccolo many times magnified  shrill and shriller till it keened with the terror of a lost soul and filled the room with the piercingness of itself.
And when it died into nothingness, Herbie collapsed into a huddled heap of motionless metal.
Bogerts face was bloodless, Hes dead!
No! Susan Calvin burst into body-racking gusts of wild laughter, not dead  merely insane. I confronted him with the insoluble dilemma, and he broke down. You can scrap him now  because hell never speak again.
Lanning was on his knees beside the thing that had been Herbie. His fingers touched the cold, unresponsive metal face and he shuddered. You did that on purpose. He rose and faced her, face contorted.
What if I did? You cant help it now. And in a sudden access of bitterness, He deserved it.
The director seized the paralyzed, motionless Bogert by the wrist, Whats the difference. Come, Peter. He sighed, A thinking robot of this type is worthless anyway. His eyes were old and tired, and he repeated, Come, Peter!
It was minutes after the two scientists left that Dr. Susan Calvin regained part of her mental equilibrium. Slowly, her eyes turned to the living-dead Herbie and the tightness returned to her face. Long she stared while the triumph faded and the helpless frustration returned  and of all her turbulent thoughts only one infinitely bitter word passed her lips.
Liar!
Runaround

It was one of Gregory Powells favorite platitudes that nothing was to be gained from excitement, so when Mike Donovan came leaping down the stairs toward him, red hair matted with perspiration, Powell frowned.
Whats wrong? he said. Break a fingernail?
Yaaaah, snarled Donovan, feverishly. What have you been doing in the sublevels all day? He took a deep breath and blurted out, Speedy never returned.
Powells eyes widened momentarily and he stopped on the stairs; then he recovered and resumed his upward steps. He didnt speak until he reached the head of the flight, and then:
You sent him after the selenium?
Yes.
And how long has he been out?
Five hours now.
Silence! This was a devil of a situation. Here they were, on Mercury exactly twelve hours  and already up to the eyebrows in the worst sort of trouble. Mercury had long been the jinx world of the System, but this was drawing it rather strong  even for a jinx.
Powell said, Start at the beginning, and lets get this straight.
They were in the radio room now  with its already subtly antiquated equipment, untouched for the ten years previous to their arrival. Even ten years, technologically speaking, meant so much. Compare Speedy with the type of robot they must have had back in 2005. But then, advances in robotics these days were tremendous. Powell touched a still gleaming metal surface gingerly. The air of disuse that touched everything about the room  and the entire Station  was infinitely depressing.
Donovan must have felt it. He began: I tried to locate him by radio, but it was no go. Radio isnt any good on the Mercury Sunside  not past two miles, anyway. Thats one of the reasons the First Expedition failed. And we cant put up the ultrawave equipment for weeks yet 
Skip all that. What did you get?
I located the unorganized body signal in the short wave. It was no good for anything except his position. I kept track of him that way for two hours and plotted the results on the map.
There was a yellowed square of parchment in his hip pocket  a relic of the unsuccessful First Expedition  and he slapped it down on the desk with vicious force, spreading it flat with the palm of his hand. Powell, hands clasped across his chest, watched it at long range.
Donovans pencil pointed nervously. The red cross is the selenium pool. You marked it yourself.
Which one is it? interrupted Powell. There were three that MacDougal located for us before he left.
I sent Speedy to the nearest, naturally; seventeen miles away. But what difference does that make? There was tension in his voice. There are the penciled dots that mark Speedys position.
And for the first time Powells artificial aplomb was shaken and his hands shot forward for the map.
Are you serious? This is impossible.
There it is, growled Donovan.
The little dots that marked the position formed a rough circle about the red cross of the selenium pool. And Powells fingers went to his brown mustache, the unfailing signal of anxiety.
Donovan added: In the two hours I checked on him, he circled that damned pool four times. It seems likely to me that hell keep that up forever. Do you realize the position were in?
Powell looked up shortly, and said nothing. Oh, yes, he realized the position they were in. It worked itself out as simply as a syllogism. The photocell banks that alone stood between the full power of Mercurys monstrous sun and themselves were shot to hell.
The only thing that could save them was selenium. The only thing that could get the selenium was Speedy. If Speedy didnt come back, no selenium. No selenium, no photocell banks. No photo-banks  well, death by slow broiling is one of the more unpleasant ways of being done in.
Donovan rubbed his red mop of hair savagely and expressed himself with bitterness. Well be the laughingstock of the System, Greg. How can everything have gone so wrong so soon? The great team of Powell and Donovan is sent out to Mercury to report on the advisability of reopening the Sunside Mining Station with modern techniques and robots and we ruin everything the first day. A purely routine job, too. Well never live it down.
We wont have to, perhaps, replied Powell, quietly. If we dont do something quickly, living anything down  or even just plain living  will be out of the question.
Dont be stupid! If you feel funny about it, Greg, I dont. It was criminal, sending us out here with only one robot. And it was your bright idea that we could handle the photocell banks ourselves.
Now youre being unfair. It was a mutual decision and you know it. All we needed was a kilogram of selenium, a Stillhead Dielectrode Plate and about three hours time and there are pools of pure selenium all over Sunside. MacDougals spectroreflector spotted three for us in five minutes, didnt it? What the devil! We couldnt have waited for next conjunction.
Well, what are we going to do? Powell, youve got an idea. I know you have, or you wouldnt be so calm. Youre no more a hero than I am. Go on, spill it!
We cant go after Speedy ourselves, Mike  not on the Sunside. Even the new insosuits arent good for more than twenty minutes in direct sunlight. But you know the old saying, Set a robot to catch a robot Look, Mike, maybe things arent so bad. Weve got six robots down in the sublevels, that we may be able to use, if they work. If they work.
There was a glint of sudden hope in Donovans eyes. You mean six robots from the First Expedition. Are you sure? They may be subrobotic machines. Ten years is a long time as far as robot-types are concerned, you know.
No, theyre robots. Ive spent all day with them and I know. Theyve got positronic brains: primitive, of course. He placed the map in his pocket. Lets go down.

The robots were on the lowest sublevel  all six of them surrounded by musty packing cases of uncertain content. They were large, extremely so, and even though they were in a sitting position on the floor, legs straddled out before them, their heads were a good seven feet in the air.
Donovan whistled. Look at the size of them, will you? The chests must be ten feet around.
Thats because theyre supplied with the old McGuffy gears. Ive been over the insides  crummiest set youve ever seen.
Have you powered them yet?
No. There wasnt any reason to. I dont think theres anything wrong with them. Even the diaphragm is in reasonable order. They might talk.
He had unscrewed the chest plate of the nearest as he spoke, inserted the two-inch sphere that contained the tiny spark of atomic energy that was a robots life. There was difficulty in fitting it, but he managed, and then screwed the plate back on again in laborious fashion. The radio controls of more modern models had not been heard of ten years earlier. And then to the other five.
Donovan said uneasily, They havent moved.
No orders to do so, replied Powell, succinctly. He went back to the first in the line and struck him on the chest. You! Do you hear me?
The monsters head bent slowly and the eyes fixed themselves on Powell. Then, in a harsh, squawking voice  like that of a medieval phonograph, he grated, Yes, Master!
Powell grinned humorlessly at Donovan. Did you get that? Those were the days of the first talking robots when it looked as if the use of robots on Earth would be banned. The makers were fighting that and they built good, healthy slave complexes into the damned machines.
It didnt help them, muttered Donovan.
No, it didnt, but they sure tried. He turned once more to the robot. Get up!
The robot towered upward slowly and Donovans head craned and his puckered lips whistled.
Powell said: Can you go out upon the surface? In the light?
There was consideration while the robots slow brain worked. Then, Yes, Master.
Good. Do you know what a mile is?
Another consideration, and another slow answer. Yes, Master.
We will take you up to the surface then, and indicate a direction. You will go about seventeen miles, and somewhere in that general region you will meet another robot, smaller than yourself. You understand so far?
Yes, Master.
You will find this robot and order him to return. If he does not wish to, you are to bring him back by force.
Donovan clutched at Powells sleeve. Why not send him for the selenium direct?
Because I want Speedy back, nitwit. I want to find out whats wrong with him. And to the robot, All right, you, follow me.
The robot remained motionless and his voice rumbled: Pardon, Master, but I cannot. You must mount first. His clumsy arms had come together with a thwack, blunt fingers interlacing.
Powell stared and then pinched at his mustache. Uh... oh!
Donovans eyes bulged. Weve got to ride him? Like a horse?
I guess thats the idea. I dont know why, though. I cant see  Yes, I do. I told you they were playing up robot-safety in those days. Evidently, they were going to sell the notion of safety by not allowing them to move about, without a mahout on their shoulders all the time. What do we do now?
Thats what Ive been thinking, muttered Donovan. We cant go out on the surface, with a robot or without. Oh, for the love of Pete  and he snapped his fingers twice. He grew excited. Give me that map youve got. I havent studied it for two hours for nothing. This is a Mining Station. Whats wrong with using the tunnels?
The Mining Station was a black circle on the map, and the light dotted lines that were tunnels stretched out about it in spider web fashion.
Donovan studied the list of symbols at the bottom of the map. Look, he said, the small black dots are openings to the surface, and heres one maybe three miles away from the selenium pool. Theres a number here  youd think theyd write larger  13a. If the robots know their way around here
Powell shot the question and received the dull Yes, Master, in reply. Get your insosuit, he said with satisfaction.
It was the first time either had worn the insosuits  which marked one time more than either had expected to upon their arrival the day before  and they tested their limb movements uncomfortably.
The insosuit was far bulkier and far uglier than the regulation spacesuit; but withal considerably lighter, due to the fact that they were entirely nonmetallic in composition. Composed of heat-resistant plastic and chemically treated cork layers, and equipped with a desiccating unit to keep the air bone-dry, the insosuits could withstand the full glare of Mercurys sun for twenty minutes. Five to ten minutes more, as well, without actually killing the occupant.
And still the robots hands formed the stirrup, nor did he betray the slightest atom of surprise at the grotesque figure into which Powell had been converted.
Powells radio-harshened voice boomed out: Are you ready to take us to Exit 13a?
Yes, Master.
Good, thought Powell; they might lack radio control but at least they were fitted for radio reception. Mount one or the other, Mike, he said to Donovan.
He placed a foot in the improvised stirrup and swung upward. He found the seat comfortable; there was the humped back of the robot, evidently shaped for the purpose, a shallow groove along each shoulder for the thighs and two elongated ears whose purpose now seemed obvious.
Powell seized the ears and twisted the head. His mount turned ponderously. Lead on, Macduff. But he did not feel at all lighthearted.
The gigantic robots moved slowly, with mechanical precision, through the doorway that cleared their heads by a scant foot, so that the two men had to duck hurriedly, along a narrow corridor in which their unhurried footsteps boomed monotonously and into the, air lock.
The long, airless tunnel that stretched to a pinpoint before them brought home forcefully to Powell the exact magnitude of the task accomplished by the First Expedition, with their crude robots and their start-from-scratch necessities. They might have been a failure, but their failure was a good deal better than the usual run of the Systems successes.
The robots plodded onward with a pace that never varied and with footsteps that never lengthened.
Powell said: Notice that these tunnels are blazing with lights and that the temperature is Earth-normal. Its probably been like this all the ten years that this place has remained empty.
Hows that?
Cheap energy; cheapest in the System. Sunpower, you know, and on Mercurys Sunside, sunpower is something. Thats why the Station was built in the sunlight rather than in the shadow of a mountain. Its really a huge energy converter. The heat is turned into electricity, light, mechanical work and what have you; so that energy is supplied and the Station is cooled in a simultaneous process.
Look, said Donovan. This is all very educational, but would you mind changing the subject? It so happens that this conversion of energy that you talk about is carried on by the photocell banks mainly  and that is a tender subject with me at the moment.
Powell grunted vaguely, and when Donovan broke the resulting silence, it was to change the subject completely. Listen, Greg. What the devils wrong with Speedy, anyway? I cant understand it.
Its not easy to shrug shoulders in an insosuit, but Powell tried it. I dont know, Mike. You know hes perfectly adapted to a Mercurian environment. Heat doesnt mean anything to him and hes built for the light gravity and the broken ground. Hes foolproof  or, at least, he should be.
Silence fell. This time, silence that lasted.
Master, said the robot, we are here.
Eh? Powell snapped out of a semidrowse. Well, get us out of here  out to the surface.
They found themselves in a tiny substation, empty, airless, ruined. Donovan had inspected a jagged hole in the upper reaches of one of the walls by the light of his pocket flash.
Meteorite, do you suppose? he had asked.
Powell shrugged. To hell with that. It doesnt matter. Lets get out.
A towering cliff of a black, basaltic rock cut off the sunlight, and the deep night shadow of an airless world surrounded them. Before them, the shadow reached out and ended in knife-edge abruptness into an all-but-unbearable blaze of white light, that glittered from myriad crystals along a rocky ground.
Space! gasped Donovan. It looks like snow. And it did.
Powells eyes swept the jagged glitter of Mercury to the horizon and winced at the gorgeous brilliance.
This must be an unusual area, he said. The general albedo of Mercury is low and most of the soil is gray pumice. Something like the Moon, you know. Beautiful, isnt it?
He was thankful for the light filters in their visiplates. Beautiful or not, a look at the sunlight through straight glass would have blinded them inside of half a minute.
Donovan was looking at the spring thermometer on his wrist. Holy smokes, the temperature is eighty centigrade!
Powell checked his own and said: Um-m-m. A little high. Atmosphere, you know.
On Mercury? Are you nuts?
Mercury isnt really airless, explained Powell, in absentminded fashion. He was adjusting the binocular attachments to his visiplate, and the bloated fingers of the insosuit were clumsy at it. There is a thin exhalation that clings to its surface  vapors of the more volatile elements and compounds that are heavy enough for Mercurian gravity to retain. You know: selenium, iodine, mercury, gallium, potassium, bismuth, volatile oxides. The vapors sweep into the shadows and condense, giving up heat. Its a sort of gigantic still. In fact, if you use your flash, youll probably find that the side of the cliff is covered with, say, hoar-sulphur, or maybe quicksilver dew.
It doesnt matter, though. Our suits can stand a measly eighty indefinitely.
Powell had adjusted the binocular attachments, so that he seemed as eye-stalked as a snail.
Donovan watched tensely. See anything?
The other did not answer immediately, and when he did, his voice was anxious and thoughtful. Theres a dark spot on the horizon that might be the selenium pool. Its in the right place. But I dont see Speedy.
Powell clambered upward in an instinctive striving for better view, till he was standing in unsteady fashion upon his robots shoulders. Legs straddled wide, eyes straining, he said: I think... I think  Yes, its definitely he. Hes coming this way.
Donovan followed the pointing finger. He had no binoculars, but there was a tiny moving dot, black against the blazing brilliance of the crystalline ground.
I see him, he yelled. Lets get going!
Powell had hopped down into a sitting position on the robot again, and his suited hand slapped against the Gargantuans barrel chest. Get going!
Giddy-ap, yelled Donovan, and thumped his heels, spur fashion.


The robots started off, the regular thudding of their footsteps silent in the airlessness, for the nonmetallic fabric of the insosuits did not transmit sound. There was only a rhythmic vibration just below the border of actual hearing.
Faster, yelled Donovan. The rhythm did not change.
No use, cried Powell, in reply. These junk heaps are only geared to one speed. Do you think theyre equipped with selective flexors?
They had burst through the shadow, and the sunlight came down in a white-hot wash and poured liquidly about them.
Donovan ducked involuntarily. Wow! Is it imagination or do I feel heat?
Youll feel more presently, was the grim reply. Keep your eye on Speedy.
Robot SPD 13 was near enough to be seen in detail now. His graceful, streamlined body threw out blazing highlights as he loped with easy speed across the broken ground. His name was derived from his serial initials, of course, but it was apt, nevertheless, for the SPD models were among the fastest robots turned out by the United States Robot & Mechanical Men Corp.
Hey, Speedy, howled Donovan, and waved a frantic hand.
Speedy! shouted Powell. Come here!
The distance between the men and the errant robot was being cut down momentarily  more by the efforts of Speedy than the slow plodding of the fifty-year-old antique mounts of Donovan and Powell.
They were close enough now to notice that Speedys gait included a peculiar rolling stagger, a noticeable side-to-side lurch  and then, as Powell waved his hand again and sent maximum juice into his compact headset radio sender, in preparation for another shout, Speedy looked up and saw them.
Speedy hopped to a halt and remained standing for a moment with just a tiny, unsteady weave, as though he were swaying in a light wind.
Powell yelled: All right, Speedy. Come here, boy.
Whereupon Speedys robot voice sounded in Powells earphones for the first time.
It said: Hot dog, lets play games. You catch me and I catch you; no love can cut our knife in two. For Im Little Buttercup, sweet Little Buttercup. Whoops! Turning on his heel, he sped off in the direction from which he had come, with a speed and fury that kicked up gouts of baked dust.
And his last words as he receded into the distance were, There grew a little flower neath a great oak tree, followed by a curious metallic clicking that might have been a robotic equivalent of a hiccup.
Donovan said weakly: Where did he pick up the Gilbert and Sullivan? Say, Greg, he... hes drunk or something.
If you hadnt told me, was the bitter response, Id never realize it. Lets get back to the cliff. Im roasting.
It was Powell who broke the desperate silence. In the first place, he said, Speedy isnt drunk  not in the human sense  because hes a robot, and robots dont get drunk. However, theres something wrong with him which is the robotic equivalent of drunkenness
To me, hes drunk, stated Donovan, emphatically, and all I know is that he thinks were playing games. And were not. Its a matter of life and very gruesome death.
All right. Dont hurry me. A robots only a robot. Once we find out whats wrong with him, we can fix it and go on.
Once, said Donovan, sourly.
Powell ignored him. Speedy is perfectly adapted to normal Mercurian environment. But this region  and his arm swept wide  is definitely abnormal. Theres our clue. Now where do these crystals come from? They might have formed from a slowly cooling liquid; but where would you get liquid so hot that it would cool in Mercurys sun?
Volcanic action, suggested Donovan, instantly, and Powells body tensed.
Out of the mouths of sucklings, he said in a small, strange voice and remained very still for five minutes.
Then, he said, Listen, Mike, what did you say to Speedy when you sent him after the selenium?
Donovan was taken aback. Well damn it  I dont know. I just told him to get it.
Yes, I know, but how? Try to remember the exact words.
I said... uh... I said: Speedy, we need some selenium. You can get it such-and-such a place. Go get it  thats all. What more did you want me to say?
You didnt put any urgency into the order, did you?
What for? It was pure routine.
Powell sighed. Well, it cant be helped now  but were in a fine fix. He had dismounted from his robot, and was sitting, back against the cliff. Donovan joined him and they linked arms: In the distance the burning sunlight seemed to wait cat-and-mouse for them, and just next them, the two giant robots were invisible but for the dull red of their photoelectric eyes that stared down at them, unblinking, unwavering and unconcerned.
Unconcerned! As was all this poisonous Mercury, as large in jinx as it was small in size.
Powells radio voice was tense in Donovans ear: Now, look, lets start with the three fundamental Rules of Robotics  the three rules that are built most deeply into a robots positronic brain. In the darkness, his gloved fingers ticked off each point.
We have: One, a robot may not injure a human being, or, through inaction, allow a human being to come to harm.
Right!
Two, continued Powell, a robot must obey the orders given it by human beings except where such orders would conflict with the First Law.
Right
And three, a robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.
Right! Now where are we?
Exactly at the explanation. The conflict between the various rules is ironed out by the different positronic potentials in the brain. Well say that a robot is walking into danger and knows it. The automatic potential that Rule 3 sets up turns him back. But suppose you order him to walk into that danger. In that case, Rule 2 sets up a counterpotential higher than the previous one and the robot follows orders at the risk of existence.
Well, I know that. What about it?
Lets take Speedys case. Speedy is one of the latest models, extremely specialized, and as expensive as a battleship. Its not a thing to be lightly destroyed
So?
So Rule 3 has been strengthened  that was specifically mentioned, by the way, in the advance notices on the SPD models  so that his allergy to danger is unusually high. At the same time, when you sent him out after the selenium, you gave him his order casually and without special emphasis, so that the Rule 2 potential set-up was rather weak. Now, hold on; Im just stating facts.
All right, go ahead. I think I get it.
You see how it works, dont you? Theres some sort of danger centering at the selenium pool. It increases as he approaches, and at a certain distance from it the Rule 3 potential, unusually high to start with, exactly balances the Rule 2 potential, unusually low to start with.
Donovan rose to his feet in excitement.  And it strikes an equilibrium. I see. Rule 3 drives him back and Rule 2 drives him forward
So he follows a circle around the selenium pool, staying on the locus of all points of potential equilibrium. And unless we do something about it, hell stay on that circle forever, giving us the good old runaround. Then, more thoughtfully: And that, by the way, is what makes him drunk. At potential equilibrium, half the positronic paths of his brain are out of kilter. Im not a robot specialist, but that seems obvious. Probably hes lost control of just those parts of his voluntary mechanism that a human drunk has. Ve-e-ery pretty.
But whats the danger? If we knew what he was running from?
You suggested it. Volcanic action. Somewhere right above the selenium pool is a seepage of gas from the bowels of Mercury. Sulphur dioxide, carbon dioxide  and carbon monoxide. Lots of it and at this temperature.
Donovan gulped audibly. Carbon monoxide plus iron gives the volatile iron carbonyl.
And a robot, added Powell, is essentially iron. Then, grimly: Theres nothing like deduction. Weve determined everything about our problem but the solution. We cant get the selenium ourselves. Its still too far. We cant send these robot horses, because they cant go themselves, and they cant carry us fast enough to keep us from crisping. And we cant catch Speedy, because the dope thinks were playing games, and he can run sixty miles to our four.
If one of us goes, began Donovan, tentatively, and comes back cooked, therell still be the other.
Yes, came the sarcastic reply, it would be a most tender sacrifice  except that a person would be in no condition to give orders before he ever reached the pool, and I dont think the robots would ever turn back to the cliff without orders. Figure it out! Were two or three miles from the pool  call it two  the robot travels at four miles an hour; and we can last twenty minutes in our suits. It isnt only the heat, remember. Solar radiation out here in the ultraviolet and below is poison.
Um-m-m, said Donovan, ten minutes short.
As good as an eternity. And another thing, in order for Rule 3 potential to have stopped Speedy where it did, there must be an appreciable amount of carbon monoxide in the metal-vapor atmosphere  and there must be an appreciable corrosive action therefore. Hes been out hours now  and how do we know when a knee joint, for instance, wont be thrown out of kilter and keel him over. Its not only a question of thinking  weve got to think fast!
Deep, dark, dank, dismal silence!
Donovan broke it, voice trembling in an effort to keep itself emotionless. He said: As long as we cant increase Rule 2 potential by giving further orders, how about working the other way? If we increase the danger, we increase Rule 3 potential and drive him backward.
Powells visiplate had turned toward him in a silent question.
You see, came the cautious explanation, all we need to do to drive him out of his rut is to increase the concentration of carbon monoxide in his vicinity. Well, back at the Station theres a complete analytical laboratory.
Naturally, assented Powell. Its a Mining Station.
All right. There must be pounds of oxalic acid for calcium precipitations.
Holy space! Mike, youre a genius.
So-so, admitted Donovan, modestly. Its just a case of remembering that oxalic acid on heating decomposes into carbon dioxide, water, and good old carbon monoxide. College chem, you know.
Powell was on his feet and had attracted the attention of one of the monster robots by the simple expedient of pounding the machines thigh.
Hey, he shouted, can you throw?
Master?
Never mind. Powell damned the robots molasses-slow brain. He scrabbled up a jagged brick-size rock. Take this, he said, and hit the patch of bluish crystals just across the crooked fissure. You see it?
Donovan pulled at his shoulder. Too far, Greg. Its almost half a mile off.
Quiet, replied Powell. Its a case of Mercurian gravity and a steel throwing arm. Watch, will you?
The robots eyes were measuring the distance with machinely accurate stereoscopy. His arm adjusted itself to the weight of the missile and drew back. In the darkness, the robots motions went unseen, but there was a sudden thumping sound as he shifted his weight, and seconds later the rock flew blackly into the sunlight. There was no air resistance to slow it down, nor wind to turn it aside  and when it hit the ground it threw up crystals precisely in the center of the blue patch.
Powell yelled happily and shouted, Lets go back after the oxalic acid, Mike.
And as they plunged into the ruined substation on the way back to the tunnels, Donovan said grimly: Speedys been hanging about on this side of the selenium pool, ever since we chased after him. Did you see him?
Yes.
I guess he wants to play games. Well, well play him games!

They were back hours later, with three-liter jars of the white chemical and a pair of long faces. The photocell banks were deteriorating more rapidly than had seemed likely. The two steered their robots into the sunlight and toward the waiting Speedy in silence and with grim purpose.
Speedy galloped slowly toward them. Here we are again. Whee! Ive made a little list, the piano organist; all people who eat peppermint and puff it in your face.
Well puff something in your face, muttered Donovan. Hes limping, Greg.
I noticed that, came the low, worried response. The monoxidell get him yet, if we dont hurry.
They were approaching cautiously now, almost sidling, to refrain from setting off the thoroughly irrational robot. Powell was too far off to tell, of course, but even already he could have sworn the crack-brained Speedy was setting himself for a spring.
Let her go, he gasped. Count three! One- two-
Two steel arms drew back and snapped forward simultaneously and two glass jars whirled forward in towering parallel arcs, gleaming like diamonds in the impossible sun. And in a pair of soundless puffs, they hit the ground behind Speedy in crashes that sent the oxalic acid flying like dust.
In the full heat of Mercurys sun, Powell knew it was fizzing like soda water.
Speedy turned to stare, then backed away from it slowly  and as slowly gathered speed. In fifteen seconds, he was leaping directly toward the two humans in an unsteady canter.
Powell did not get Speedys words just then, though he heard something that resembled, Lovers professions when uttered in Hessians.
He turned away. Back to the cliff, Mike. Hes out of the rut and hell be taking orders now. Im getting hot.
They jogged toward the shadow at the slow monotonous pace of their mounts, and it was not until they had entered it and felt the sudden coolness settle softly about them that Donovan looked back. Greg!
Powell looked and almost shrieked. Speedy was moving slowly now  so slowly  and in the wrong direction. He was drifting; drifting back into his rut; and he was picking up speed. He looked dreadfully close, and dreadfully unreachable, in the binoculars.
Donovan shouted wildly, After him! and thumped his robot into its pace, but Powell called him back.
You wont catch him, Mike  its no use. He fidgeted on his robots shoulders and clenched his fist in tight impotence. Why the devil do I see these things five seconds after its all over? Mike, weve wasted hours.
We need more oxalic acid, declared Donovan, stolidly. The concentration wasnt high enough.
Seven tons of it wouldnt have been enough  and we havent the hours to spare to get it, even if it were, with the monoxide chewing him away. Dont you see what it is, Mike?
And Donovan said flatly, No.
We were only establishing new equilibriums. When we create new monoxide and increase Rule 3 potential, he moves backward till hes in balance again  and when the monoxide drifted away, he moved forward, and again there was balance.
Powells voice sounded thoroughly wretched. Its the same old runaround. We can push at Rule 2 and pull at Rule 3 and we cant get anywhere  we can only change the position of balance. Weve got to get outside both rules. And then he pushed his robot closer to Donovans so that they were sitting face-to-face, dim shadows in the darkness, and he whispered, Mike!
Is it the finish?  dully. I suppose we go back to the Station, wait for the banks to fold, shake hands, take cyanide, and go out like gentlemen. He laughed shortly.
Mike, repeated Powell earnestly, weve got to get Speedy.
I know.
Mike, once more, and Powell hesitated before continuing. Theres always Rule 1. I thought of it  earlier  but its desperate.
Donovan looked up and his voice livened. Were desperate.
All right. According to Rule 1, a robot cant see a human come to harm because of his own inaction. Two and 3 cant stand against it. They cant, Mike.
Even when the robot is half cra Well, hes drunk. You know he is.
Its the chances you take.
Cut it. What are you going to do?
Im going out there now and see what Rule 1 will do. If it wont break the balance, then what the devil  its either now or three-four days from now.
Hold on, Greg. There are human rules of behavior, too. You dont go out there just like that. Figure out a lottery, and give me my chance.
All right. First to get the cube of fourteen goes. And almost immediately, Twenty-seven forty-four!
Donovan felt his robot stagger at a sudden push by Powells mount and then Powell was off into the sunlight. Donovan opened his mouth to shout, and then clicked it shut. Of course, the damn fool had worked out the cube of fourteen in advance, and on purpose. Just like him.

The sun was hotter than ever and Powell felt a maddening itch in the small of his back. Imagination, probably, or perhaps hard radiation beginning to tell even through the insosuit.
Speedy was watching him, without a word of Gilbert and Sullivan gibberish as greeting. Thank God for that! But he darent get too close.
He was three hundred yards away when Speedy began backing, a step at a time, cautiously  and Powell stopped. He jumped from his robots shoulders and landed on the crystalline ground with a light thump and a flying of jagged fragments.
He proceeded on foot, the ground gritty and slippery to his steps, the low gravity causing him difficulty. The soles of his feet tickled with warmth. He cast one glance over his shoulder at the blackness of the cliffs shadow and realized that he had come too far to return  either by himself or by the help of his antique robot. It was Speedy or nothing now, and the knowledge of that constricted his chest.
Far enough! He stopped.
Speedy, he called. Speedy!
The sleek, modern robot ahead of him hesitated and halted his backward steps, then resumed them.
Powell tried to put a note of pleading into his voice, and found it didnt take much acting. Speedy, Ive got to get back to the shadow or the sunll get me. Its life or death, Speedy. I need you.
Speedy took one step forward and stopped. He spoke, but at the sound Powell groaned, for it was, When youre lying awake with a dismal headache and repose is tabooed It trailed off there, and Powell took time out for some reason to murmur, Iolanthe.
It was roasting hot! He caught a movement out of the corner of his eye, and whirled dizzily; then stared in utter astonishment, for the monstrous robot on which he had ridden was moving  moving toward him, and without a rider.
He was talking: Pardon, Master. I must not move without a Master upon me, but you are in danger.
Of course, Rule 1 potential above everything. But he didnt want that clumsy antique; he wanted Speedy. He walked away and motioned frantically: I order you to stay away. I order you to stop!
It was quite useless. You could not beat Rule 1 potential. The robot said stupidly, You are in danger, Master.
Powell looked about him desperately. He couldnt see clearly. His brain was in a heated whirl; his breath scorched when he breathed, and the ground all about him was a shimmering haze.
He called a last time, desperately: Speedy! Im dying, damn you! Where are you? Speedy, I need you.
He was still stumbling backward in a blind effort to get away from the giant robot he didnt want, when he felt steel fingers on his arms, and a worried, apologetic voice of metallic timbre in his ears.
Holy smokes, boss; what are you doing here? And what am I doing  Im so confused 
Never mind, murmured Powell, weakly. Get me to the shadow of the cliff  and hurry! There was one last feeling of being lifted into the air and a sensation of rapid motion and burning heat, and he passed out.

He woke with Donovan bending over him and smiling anxiously. How are you, Greg?
Fine! came the response, Wheres Speedy?
Right here. I sent him out to one of the other selenium pools  with orders to get that selenium at all cost this time. He got it back in forty-two minutes and three seconds. I timed him. He still hasnt finished apologizing for the runaround he gave us. Hes scared to come near you for fear of what youll say.
Drag him over, ordered Powell. It wasnt his fault. He held out a hand and gripped Speedys metal paw. Its O.K., Speedy. Then, to Donovan, You know, Mike, I was just thinking
Yes!
Well,  he rubbed his face  the air was so delightfully cool, you know that when we get things set up here and Speedy put through his Field Tests, theyre going to send us to the Space Stations next
No!
Yes! At least thats what old lady Calvin told me just before we left, and I didnt say anything about it, because I was going to fight the whole idea.
Fight it? cried Donovan. But 
I know. Its all right with me now. Two hundred seventy-three degrees Centigrade below zero. Wont it be a pleasure?
Space Station, said Donovan, here I come.

Evidence

Francis Quinn was a politician of the new school. That, of course, is a meaningless expression, as are all expressions of the sort. Most of the new schools we have were duplicated in the social life of ancient Greece, and perhaps, if we knew more about it, in the social life of ancient Sumeria and in the lake dwellings of prehistoric Switzerland as well.
But, to get out from under what promises to be a dull and complicated beginning, it might be best to state hastily that Quinn neither ran for office nor canvassed for votes, made no speeches and stuffed no ballot boxes. Any more than Napoleon pulled a trigger at Austerlitz.
And since politics makes strange bedfellows, Alfred Lanning sat at the other side of the desk with his ferocious white eyebrows bent far forward over eyes in which chronic impatience had sharpened to acuity. He was not pleased.
The fact, if known to Quinn, would have annoyed him not the least. His voice was friendly, perhaps professionally so.
I assume you know Stephen Byerley, Dr. Lanning.
I have heard of him. So have many people.
Yes, so have I. Perhaps you intend voting for him at the next election.
I couldnt say. There was an unmistakable trace of acidity here. I have not followed the political currents, so Im not aware that he is running for office.
He may be our next mayor. Of course, he is only a lawyer now, but great oaks
Yes, interrupted Lanning, I have heard the phrase before. But I wonder if we can get to the business at hand.
We are at the business at hand, Dr. Lanning. Quinns tone was very gentle, It is to my interest to keep Mr. Byerley a district attorney at the very most, and it is to your interest to help me do so.
To my interest? Come! Lannings eyebrows hunched low.
Well, say then to the interest of the U. S. Robot & Mechanical Men Corporation. I come to you as Director Emeritus of Research, because I know that your connection to them is that of, shall we say, elder statesman. You are listened to with respect and yet your connection with them is no longer so tight but that you cannot possess considerable freedom of action; even if the action is somewhat unorthodox.
Dr. Lanning was silent a moment, chewing the cud of his thoughts. He said more softly, I dont follow you at all, Mr. Quinn.
I am not surprised, Dr. Lanning. But its all rather simple. Do you mind? Quinn lit a slender cigarette with a lighter of tasteful simplicity and his big-boned face settled into an expression of quiet amusement. We have spoken of Mr. Byerley  a strange and colorful character. He was unknown three years ago. He is very well known now. He is a man of force and ability, and certainly the most capable and intelligent prosecutor I have ever known. Unfortunately he is not a friend of mine
I understand, said Lanning, mechanically. He stared at his fingernails.
I have had occasion, continued Quinn, evenly, in the past year to investigate Mr. Byerley  quite exhaustively. It is always useful, you see, to subject the past life of reform politicians to rather inquisitive research. If you knew how often it helped He paused to smile humorlessly at the glowing tip of his cigarette. But Mr. Byerleys past is unremarkable. A quiet life in a small town, a college education, a wife who died young, an auto accident with a slow recovery, law school, coming to the metropolis, an attorney.
Francis Quinn shook his head slowly, then added, But his present life. Ah, that is remarkable. Our district attorney never eats!
Lannings head snapped up, old eyes surprisingly sharp, Pardon me?
Our district attorney never eats. The repetition thumped by syllables. Ill modify that slightly. He has never been seen to eat or drink. Never! Do you understand the significance of the word? Not rarely, but never!
I find that quite incredible. Can you trust your investigators?
I can trust my investigators, and I dont find it incredible at all. Further, our district attorney has never been seen to drink  in the aqueous sense as well as the alcoholic  nor to sleep. There are other factors, but I should think I have made my point.

Lanning leaned back in his seat, and there was the rapt silence of challenge and response between them, and then the old roboticist shook his head. No. There is only one thing you can be trying to imply, if I couple your statements with the fact that you present them to me, and that is impossible.
But the man is quite inhuman, Dr. Lanning.
If you told me he were Satan in masquerade, there would be a faint chance that I might believe you.
I tell you he is a robot, Dr. Lanning.
I tell you it is as impossible a conception as I have ever heard, Mr. Quinn.
Again the combative silence.
Nevertheless, and Quinn stubbed out his cigarette with elaborate care, you will have to investigate this impossibility with all the resources of the Corporation.
Im sure that I could undertake no such thing, Mr. Quinn. You dont seriously suggest that the Corporation take part in local politics.
You have no choice. Supposing I were to make my facts public without proof. The evidence is circumstantial enough.
Suit yourself in that respect.
But it would not suit me. Proof would be much preferable. And it would not suit you, for the publicity would be very damaging to your company. You are perfectly well acquainted, I suppose, with the strict rules against the use of robots on inhabited worlds.
Certainly!  brusquely.
You know that the U. S. Robot & Mechanical Men Corporation is the only manufacturer of positronic robots in the Solar System, and if Byerley is a robot, he is a positronic robot. You are also aware that all positronic robots are leased, and not sold; that the Corporation remains the owner and manager of each robot, and is therefore responsible for the actions of all.
It is an easy matter, Mr. Quinn, to prove the Corporation has never manufactured a robot of a humanoid character.
It can be done? To discuss merely possibilities.
Yes. It can be done.
Secretly, I imagine, as well. Without entering it in your books.
Not the positronic brain, sir. Too many factors are involved in that, and there is the tightest possible government supervision.
Yes, but robots are worn out, break down, go out of order  and are dismantled.
And the positronic brains re-used or destroyed.
Really? Francis Quinn allowed himself a trace of sarcasm. And if one were, accidentally, of course, not destroyed  and there happened to be a humanoid structure waiting for a brain.
Impossible!
You would have to prove that to the government and the public, so why not prove it to me now.
But what could our purpose be? demanded Lanning in exasperation. Where is our motivation? Credit us with a minimum of sense.
My dear sir, please. The Corporation would be only too glad to have the various Regions permit the use of humanoid positronic robots on inhabited worlds. The profits would be enormous. But the prejudice of the public against such a practice is too great. Suppose you get them used to such robots first  see, we have a skillful lawyer, a good mayor, and he is a robot. Wont you buy our robot butlers?
Thoroughly fantastic. An almost humorous descent to the ridiculous.
I imagine so. Why not prove it? Or would you still rather try to prove it to the public?
The light in the office was dimming, but it was not yet too dim to obscure the flush of frustration on Alfred Lannings face. Slowly, the roboticists finger touched a knob and the wall illuminators glowed to gentle life.
Well, then, he growled, let us see.

The face of Stephen Byerley is not an easy one to describe. He was forty by birth certificate and forty by appearance  but it was a healthy, well-nourished good-natured appearance of forty; one that automatically drew the teeth of the bromide about looking ones age.
This was particularly true when he laughed, and he was laughing now. It came loudly and continuously, died away for a bit, then began again
And Alfred Lannings face contracted into a rigidly bitter monument of disapproval. He made a half gesture to the woman who sat beside him, but her thin, bloodless lips merely pursed themselves a trifle.
Byerley gasped himself a stage nearer normality.
Really, Dr. Lanning... really  I... I... a robot?
Lanning bit his words off with a snap, It is no statement of mine, sir. I would be quite satisfied to have you a member of humanity. Since our corporation never manufactured you, I am quite certain that you are  in a legalistic sense, at any rate. But since the contention that you are a robot has been advanced to us seriously by a man of certain standing
Dont mention his name, if it would knock a chip off your granite block of ethics, but lets pretend it was Frank Quinn, for the sake of argument, and continue.
Lanning drew in a sharp, cutting snort at the interruption, and paused ferociously before continuing with added frigidity, by a man of certain standing, with whose identity I am not interested in playing guessing games, I am bound to ask your cooperation in disproving it. The mere fact that such a contention could be advanced and publicized by the means at this mans disposal would be a bad blow to the company I represent  even if the charge were never proven. You understand me?
Oh, yes, your position is clear to me. The charge itself is ridiculous. The spot you find yourself in is not. I beg your pardon, if my laughter offended you. It was the first I laughed at, not the second. How can I help you?
It could be very simple. You have only to sit down to a meal at a restaurant in the presence of witnesses, have your picture taken, and eat. Lanning sat back in his chair, the worst of the interview over. The woman beside him watched Byerley with an apparently absorbed expression but contributed nothing of her own.
Stephen Byerley met her eyes for an instant, was caught by them, then turned back to the roboticist. For a while his fingers were thoughtful over the bronze paperweight that was the only ornament on his desk.
He said quietly, I dont think I can oblige you.
He raised his hand, Now wait, Dr. Lanning. I appreciate the fact that this whole matter is distasteful to you, that you have been forced into it against your will, that you feel you are playing an undignified and even ridiculous part. Still, the matter is even more intimately concerned with myself, so be tolerant.
First, what makes you think that Quinn  this man of certain standing, you know  wasnt hoodwinking you, in order to get you to do exactly what you are doing?
Why it seems scarcely likely that a reputable person would endanger himself in so ridiculous a fashion, if he werent convinced he were on safe ground.
There was little humor in Byerleys eyes, You dont know Quinn. He could manage to make safe ground out of a ledge a mountain sheep could not handle. I suppose he showed the particulars of the investigation he claims to have made of me?
Enough to convince me that it would be too troublesome to have our corporation attempt to disprove them when you could do so more easily.
Then you believe him when he says I never eat. You are a scientist, Dr. Lanning. Think of the logic required. I have not been observed to eat, therefore, I never eat Q.E.D. After all!
You are using prosecution tactics to confuse what is really a very simple situation.
On the contrary, I am trying to clarify what you and Quinn between you are making a very complicated one. You see, I dont sleep much, thats true, and I certainly dont sleep in public. I have never cared to eat with others  an idiosyncrasy which is unusual and probably neurotic in character, but which harms no one. Look, Dr. Lanning, let me present you with a suppositious case. Supposing we had a politician who was interested in defeating a reform candidate at any cost and while investigating his private life came across oddities such as I have just mentioned.
Suppose further that in order to smear the candidate effectively, he comes to your company as the ideal agent. Do you expect him to say to you, So-and-so is a robot because he hardly ever eats with people, and I have never seen him fall asleep in the middle of a case; and once when I peeped into his window in the middle of the night, there he was, sitting up with a book; and I looked in his frigidaire and there was no food in it.
If he told you that, you would send for a straitjacket. But if he tells you, He never sleeps; he never eats, then the shock of the statement blinds you to the fact that such statements are impossible to prove. You play into his hands by contributing to the to-do.
Regardless, sir, began Lanning, with a threatening obstinacy, of whether you consider this matter serious or not, it will require only the meal I mentioned to end it.
Again Byerley turned to the woman, who still regarded him expressionlessly. Pardon me. Ive caught your name correctly, havent I? Dr. Susan Calvin?
Yes, Mr. Byerley.
Youre the U. S. Robots psychologist, arent you?
Robopsychologist, please.
Oh, are robots so different from men, mentally?
Worlds different. She allowed herself a frosty smile, Robots are essentially decent.
Humor tugged at the corners of the lawyers mouth, Well, thats a hard blow. But what I wanted to say was this. Since youre a psycho  a robopsychologist, and a woman, Ill bet that youve done something that Dr. Lanning hasnt thought of.
And what is that?
Youve got something to eat in your purse.
Something caught in the schooled indifference of Susan Calvins eyes. She said, You surprise me, Mr. Byerley.
And opening her purse, she produced an apple. Quietly, she handed it to him. Dr. Lanning, after an initial start, followed the slow movement from one hand to the other with sharply alert eyes.
Calmly, Stephen Byerley bit into it, and calmly he swallowed it
You see, Dr. Lanning?
Dr. Lanning smiled in a relief tangible enough to make even his eyebrows appear benevolent A relief that survived for one fragile second.
Susan Calvin said, I was curious to see if you would eat it, but, of course, in the present case, it proves nothing.
Byerley grinned, It doesnt?
Of course not. It is obvious, Dr. Lanning, that if this man were a humanoid robot, he would be a perfect imitation. He is almost too human to be credible. After all, we have been seeing and observing human beings all our lives; it would be impossible to palm something merely nearly right off on us. It would have to be all right. Observe the texture of the skin, the quality of the irises, the bone formation of the hand. If hes a robot, I wish U. S. Robots had made him, because hes a good job. Do you suppose then, that anyone capable of paying attention to such niceties would neglect a few gadgets to take care of such things as eating, sleeping, elimination? For emergency use only, perhaps; as, for instance, to prevent such situations as are arising here. So a meal wont really prove anything.
Now wait, snarled Lanning, I am  not quite the fool both of you make me out to be. I am not interested in the problem of Mr. Byerleys humanity or nonhumanity. I am interest in getting the corporation out of a hole. A public meal will end the matter and keep it ended no matter what Quinn does. We can leave the finer details to lawyers and robopsychologists.
But, Dr. Lanning, said Byerley, you forget the politics of the situation. I am as anxious to be elected, as Quinn is to stop me. By the way, did you notice that you used his name? Its a cheap shyster trick of mine; I knew you would, before you were through.
Lanning flushed, What has the election to do with it?
Publicity works both ways, sir. If Quinn wants to call me a robot, and has the nerve to do so, I have the nerve to play the game his way.
You mean you Lanning was quite frankly appalled.
Exactly. I mean that Im going to let him go ahead, choose his rope, test its strength, cut off the right length, tie the noose, insert his head and grin. I can do what little else is required.
You are mighty confident.
Susan Calvin rose to her feet, Come, Alfred, we wont change his mind for him.
You see. Byerley smiled gently. Youre a human psychologist, too.

But perhaps not all the confidence that Dr. Lanning had remarked upon was present that evening when Byerleys car parked on the automatic treads leading to the sunken garage, and Byerley himself crossed the path to the front door of his house.
The figure in the wheel chair looked up as he entered and smiled. Byerleys face lit with affection. He crossed over to it.
The cripples voice was a hoarse, grating whisper that came out of a mouth forever twisted to one side, leering out of a face that was half scar tissue, Youre late, Steve.
I know, John, I know. But Ive been up against a peculiar and interesting trouble today.
So? Neither the torn face nor the destroyed voice could carry expression but there was anxiety in the clear eyes. Nothing you cant handle?
Im not exactly certain. I may need your help. Youre the brilliant one in the family. Do you want me to take you out into the garden? Its a beautiful evening.
Two strong arms lifted John from the wheel chair. Gently, almost caressingly, Byerleys arms went around the shoulders and under the swathed legs of the cripple. Carefully, and slowly, he walked through the rooms, down the gentle ramp that had been built with a wheel chair in mind, and out the back door into the walled and wired garden behind the house.
Why dont you let me use the wheel chair, Steve? This is Silly.
Because Id rather carry you. Do you object? You know that youre as glad to get out of that motorized buggy for a while, as I am to see you out. How do you feel today? He deposited John with infinite care upon the cool grass.
How should I feel? But tell me about your troubles.
Quinns campaign will be based on the fact that he claims Im a robot.
Johns eyes opened wide, How do you know? Its impossible. I wont believe it.
Oh, come, I tell you its so. He had one of the big-shot scientists of U. S. Robot & Mechanical Men Corporation over at the office to argue with me.
Slowly Johns hands tore at the grass, I see. I see.
Byerley said, But we can let him choose his ground. I have an idea. Listen to me and tell me if we can do it

The scene as it appeared in Alfred Lannings office that night was a tableau of stares. Francis Quinn stared meditatively at Alfred Lanning. Lannings stare was savagely set upon Susan Calvin, who stared impassively in her turn at Quinn.
Francis Quinn broke it with a heavy attempt at lightness, Bluff. Hes making it up as he goes along.
Are you going to gamble on that, Mr. Quinn? asked Dr. Calvin, indifferently.
Well, its your gamble, really.
Look here, Lanning covered definite pessimism with bluster, weve done what you asked. We witnessed the man eat. Its ridiculous to presume him a robot.
Do you think so? Quinn shot toward Calvin. Lanning said you were the expert.
Lanning was almost threatening, Now, Susan
Quinn interrupted smoothly, Why not let her talk, man? Shes been sitting there imitating a gatepost for half an hour.
Lanning felt definitely harassed. From what he experienced then to incipient paranoia was but a step. He said, Very well. Have your say, Susan. We wont interrupt you.
Susan Calvin glanced at him humorlessly, then fixed cold eyes on Mr. Quinn. There are only two ways of definitely proving Byerley to be a robot, sir. So far you are presenting circumstantial evidence, with which you can accuse, but not prove  and I think Mr. Byerley is sufficiently clever to counter that sort of material. You probably think so yourself, or you wouldnt have come here.
The two methods of proof are the physical and the psychological. Physically, you can dissect him or use an X-ray. How to do that would be your problem. Psychologically, his behavior can be studied, for if he is a positronic robot, he must conform to the three Rules of Robotics. A positronic brain cannot be constructed without them. You know the Rules, Mr. Quinn?
She spoke them carefully, clearly, quoting word for word the famous bold print on page one of the Handbook of Robotics.
Ive heard of them, said Quinn, carelessly.
Then the matter is easy to follow, responded the psychologist, dryly. If Mr. Byerley breaks any of those three rules, he is not a robot. Unfortunately, this procedure works in only one direction. If he lives up to the rules, it proves nothing one way or the other.
Quinn raised polite eyebrows, Why not, doctor?
Because, if you stop to think of it, the three Rules of Robotics are the essential guiding principles of a good many of the worlds ethical systems. Of course, every human being is supposed to have the instinct of self-preservation. Thats Rule Three to a robot. Also every good human being, with a social conscience and a sense of responsibility, is supposed to defer to proper authority; to listen to his doctor, his boss, his government, his psychiatrist, his fellow man; to obey laws, to follow rules, to conform to custom  even when they interfere with his comfort or his safety. Thats Rule Two to a robot. Also, every good human being is supposed to love others as himself, protect his fellow man, risk his life to save another. Thats Rule One to a robot. To put it simply  if Byerley follows all the Rules of Robotics, he may be a robot, and may simply be a very good man.
But, said Quinn, youre telling me that you can never prove him a robot.
I may be able to prove him not a robot
Thats not the proof I want.
Youll have such proof as exists. You are the only one responsible for your own wants.

Here Lannings mind leaped suddenly to the sting of an idea, Has it occurred to anyone, he ground out, that district attorney is a rather strange occupation for a robot? The prosecution of human beings  sentencing them to death  bringing about their infinite harm
Quinn grew suddenly keen, No, you cant get out of it that way. Being district attorney doesnt make him human. Dont you know his record? Dont you know that he boasts that he has never prosecuted an innocent man; that there are scores of people left untried because the evidence against them didnt satisfy him, even though he could probably have argued a jury into atomizing them? That happens to be so.
Lannings thin cheeks quivered, No, Quinn, no. There is nothing in the Rules of Robotics that makes any allowance for human guilt. A robot may not judge whether a human being deserves death. It is not for him to decide. He may not harm a humanvariety skunk, or variety angel.
Susan Calvin sounded tired. Alfred, she said, dont talk foolishly. What if a robot came upon a madman about to set fire to a house with people in it? He would stop the madman, wouldnt he?
Of course.
And if the only way he could stop him was to kill him
There was a faint sound in Lannings throat. Nothing more.
The answer to that, Alfred, is that he would do his best not to kill him. If the madman died, the robot would require psychotherapy because he might easily go mad at the conflict presented him of having broken Rule One to adhere to Rule One in a higher sense. But a man would be dead and a robot would have killed him.
Well, is Byerley mad? demanded Lanning; with all the sarcasm he could muster.
No, but he has killed no man himself. He has exposed facts which might represent a particular human being to be dangerous to the large mass of other human beings we call society. He protects the greater number and thus adheres to Rule One at maximum potential. That is as far as he goes. It is the judge who then condemns the criminal to death or imprisonment, after the jury decides on his guilt or innocence. It is the jailer who imprisons him, the executioner who kills him. And Mr. Byerley has done nothing but determine truth and aid society.
As a matter of fact, Mr. Quinn, I have looked into Mr. Byerleys career since you first brought this matter to our attention. I find that he has never demanded the death sentence in his closing speeches to the jury. I also find that he has spoken on behalf of the abolition of capital punishment and contributed generously to research institutions engaged in criminal neurophysiology. He apparently believes in the cure, rather than the punishment of crime. I find that significant.
You do? Quinn smiled. Significant of a certain odor of roboticity, perhaps?
Perhaps. Why deny it? Actions such as his could come only from a robot, or from a very honorable and decent human being. But you see, you just cant differentiate between a robot and the very best of humans.
Quinn sat back in his chair. His voice quivered with impatience. Dr. Lanning, its perfectly possible to create a humanoid robot that would perfectly duplicate a human in appearance, isnt it?
Lanning harrumphed and considered, Its been done experimentally by U. S. Robots, he said reluctantly, without the addition of a positronic brain, of course. By using human ova and hormone control, one can grow human flesh and skin over a skeleton of porous silicone plastics that would defy external examination. The eyes, the hair, the skin would be really human, not humanoid. And if you put a positronic brain, and such other gadgets as you might desire inside, you have a humanoid robot.
Quinn said shortly, How long would it take to make one?
Lanning considered, If you had all your equipment  the brain, the skeleton, the ovum, the proper hormones and radiations  say, two months.
The politician straightened out of his chair. Then we shall see what the insides of Mr. Byerley look like. It will mean publicity for U. S. Robots  but I gave you your chance.
Lanning turned impatiently to Susan Calvin, when they were alone. Why do you insist?
And with real feeling, she responded sharply and instantly, Which do you want  the truth or my resignation? I wont lie for you. U. S. Robots can take care of itself. Dont turn coward.
What, said Lanning, if he opens up Byerley, and wheels and gears fall out what then?
He wont open Byerley, said Calvin, disdainfully. Byerley is as clever as Quinn, at the very least

The news broke upon the city a week before Byerley was to have been nominated. But broke is the wrong word. It staggered upon the city, shambled, crawled. Laughter began, and wit was free. And as the far off hand of Quinn tightened its pressure in easy stages, the laughter grew forced, an element of hollow uncertainty entered, and people broke off to wonder.
The convention itself had the sir of a restive stallion. There had been no contest planned. Only Byerley could possibly have been nominated a week earlier. There was no substitute even now. They had to nominate him, but there was complete confusion about it.
It would not have been so bad if the average individual were not torn between the enormity of the charge, if true, and its sensational folly, if false.
The day after Byerley was nominated perfunctorily, hollowly  a newspaper finally published the gist of a long interview with Dr. Susan Calvin, world famous expert on robopsychology and positronics.
What broke loose is popularly and succinctly described as hell.
It was what the Fundamentalists were waiting for. They were not a political party; they made pretense to no formal religion. Essentially they were those who had not adapted themselves to what had once been called the Atomic Age, in the days when atoms were a novelty. Actually, they were the Simple-Lifers, hungering after a life, which to those who lived it had probably appeared not so Simple, and who had been, therefore, Simple-Lifers themselves.
The Fundamentalists required no new reason to detest robots and robot manufacturers; but a new reason such as the Quinn accusation and the Calvin analysis was sufficient to make such detestation audible.
The huge plants of the U. S. Robot & Mechanical Men Corporation was a hive that spawned armed guards. It prepared for war.
Within the city the house of Stephen Byerley bristled with police.
The political campaign, of course, lost all other issues, and resembled a campaign only in that it was something filling the hiatus between nomination and election.

Stephen Byerley did not allow the fussy little man to distract him. He remained comfortably unperturbed by the uniforms in the background. Outside the house, past the line of grim guards, reporters and photographers waited according to the tradition of the caste. One enterprising visor station even had a scanner focused on the blank entrance to the prosecutors unpretentious home, while a synthetically excited announcer filled in with inflated commentary.
The fussy little man advanced. He held forward a rich, complicated sheet. This, Mr. Byerley, is a court order authorizing me to search these premises for the presence of illegal... uh... mechanical men or robots of any description.
Byerley half rose, and took the paper. He glanced at it indifferently, and smiled as he handed it back. All in order. Go ahead. Do your job. Mrs. Hoppen  to his housekeeper, who appeared reluctantly from the next room  please go with them, and help out if you can.
The little man, whose name was Harroway, hesitated, produced an unmistakable blush, failed completely to catch Byerleys eyes, and muttered, Come on, to the two policemen.
He was back in ten minutes.
Through? questioned Byerley, in just the tone of a person who is not particularly interested in the question, or its answer.
Harroway cleared his throat, made a bad start in falsetto, and began again, angrily, Look here, Mr. Byerley, our special instructions were to search the house very thoroughly.
And havent you?
We were told exactly what to look for.
Yes?
In short, Mr. Byerley, and not to put too fine a point on it, we were told to search you.
Me? said the prosecutor with a broadening smile. And how do you intend to do that?
We have a Penet-radiation unit
Then Im to have my X-ray photograph taken, hey? You have the authority?
You saw my warrant.
May I see it again?
Harroway, his forehead shining with considerably more than mere enthusiasm, passed it over a second time.
Byerley said evenly, I read here as the description of what you are to search; I quote: the dwelling place belonging to Stephen Allen Byerley, located at 355 Willow Grove, Evanstron, together, with any garage, storehouse or other structures or buildings thereto appertaining, together with all grounds thereto appertaining... um... and so on. Quite in order. But, my good man, it doesnt say anything about searching my interior. I am not part of the premises. You may search my clothes if you think Ive got a robot hidden in my pocket.
Harroway had no doubt on the point of to whom he owed his job. He did not propose to be backward, given a chance to earn a much better  i.e., more highly paid  job.
He said, in a faint echo of bluster, Look here. Im allowed to search the furniture in your house, and anything else I find in it. You are in it, arent you?
A remarkable observation. I am in it. But Im not a piece of furniture. As a citizen of adult responsibility  I have the psychiatric certificate proving that  I have certain rights under the Regional Articles. Searching me would come under the heading of violating my Right of Privacy. That paper isnt sufficient.
Sure, but if youre a robot, you dont have Right of Privacy.
True enough but that paper still isnt sufficient. It recognizes me implicitly as a human being.
Where? Harroway snatched at it.
Where it says the dwelling place belonging to and so on. A robot cannot own property. And you may tell your employer, Mr. Harroway, that if he tries to issue a similar paper which does not implicitly recognize me as a human being, he will be immediately faced with a restraining injunction and a civil suit which will make it necessary for him to prove me a robot by means of information now in his possession, or else to pay a whopping penalty for an attempt to deprive me unduly of my Rights under the Regional Articles. Youll tell him that, wont you?
Harroway marched to the door. He turned. . Youre a slick lawyer His hand was in his pocket. For a short moment, he stood there. Then he left, smiled in the direction of the visor scanner, still playing away  waved to the reporters, and shouted, Well have something for you tomorrow, boys. No kidding.
In his ground car, he settled back, removed the tiny mechanism from his pocket and carefully inspected it. It was the first time he had ever taken a photograph by X-ray reflection. He hoped he had done it correctly.
Quinn and Byerley had never met face-to-face alone. But visorphone was pretty close to it. In fact, accepted literally, perhaps the phrase was accurate, even if to each, the other were merely the light and dark pattern of a bank of photocells.
It was Quinn who had initiated the call. It was Quinn, who spoke first, and without particular ceremony, Thought you would like to know, Byerley, that I intend to make public the fact that youre wearing a protective shield against Penet-radiation.
That so? In that case, youve probably already made it public. I have a notion our enterprising press representatives have been tapping my various communication lines for quite a while. I know they have my office lines full of holes; which is why Ive dug in at my home these last weeks. Byerley was friendly, almost chatty.
Quinns lips tightened slightly, This call is shielded  thoroughly. Im making it at a certain personal risk.
So I should imagine. Nobody knows youre behind this campaign. At least, nobody knows it officially. Nobody doesnt know it unofficially. I wouldnt worry. So I wear a protective shield? I suppose you found that out when your puppy dogs Penet-radiation photograph, the other day, turned out to be overexposed.
You realize, Byerley, that it would be pretty obvious to everyone that you dont dare face X-ray analysis.
Also that you, or your men, attempted illegal invasion of my Rights of Privacy.
The devil theyll care for that.
They might. Its rather symbolic of our two campaigns isnt it? You have little concern with the rights of the individual citizen. I have great concern. I will not submit to X-ray analysis, because I wish to maintain my Rights on principle. Just as Ill maintain the rights of others when elected.
That will, no doubt make a very interesting speech, but no one will believe you. A little too high-sounding to be true. Another thing, a sudden, crisp change, the personnel in your home was not complete the other night.
In what way?
According to the report, he shuffled papers before him that were just within the range of vision of the visiplate, there was one person missing  a cripple.
As you say, said Byerley, tonelessly, a cripple. My old teacher, who lives with me and who is now in the country  and has been for two months. A much-needed rest is the usual expression applied in the case. He has your permission?
Your teacher? A scientist of sorts?
A lawyer once  before he was a cripple. He has a government license as a research biophysicist, with a laboratory of his own, and a complete description of the work hes doing filed with the proper authorities, to whom I can refer you. The work is minor, but is a harmless and engaging hobby for a  poor cripple. I am being as helpful as I can, you see.
I see. And what does this... teacher... know about robot manufacture?
I couldnt judge the extent of his knowledge in a field with which I am unacquainted.
He wouldnt have access to positronic brains?
Ask your friends at U. S. Robots. Theyd be the ones to know.
Ill put it shortly, Byerley. Your crippled teacher is the real Stephen Byerley. You are his robot creation. We can prove it. It was he who was in the automobile accident, not you. There will be ways of checking the records.
Really? Do so, then. My best wishes.
And we can search your so-called teachers country place, and see what we can find there.
Well, not quite, Quinn. Byerley smiled broadly. Unfortunately for you, my so-called teacher is a sick man. His country place is his place of rest. His Right of Privacy as a citizen of adult responsibility is naturally even stronger, under the circumstances. You wont be able to obtain a warrant to enter his grounds without showing just cause. However, Id be the last to prevent you from trying.
There was a pause of moderate length, and then Quinn leaned forward, so that his imaged-face expanded and the fine lines on his forehead were visible, Byerley, why do you carry on? You cant be elected.
Cant I?
Do you think you can? Do you suppose that your failure to make any attempt to disprove the robot charge  when you could easily, by breaking one of the Three Laws  does anything but convince the people that you are a robot?
All I see so far is that from being a rather vaguely known, but still largely obscure metropolitan lawyer, I have now become a world figure. Youre a good publicist.
But you are a robot.
So its been said, but not proven.
Its been proven sufficiently for the electorate.
Then relax youve won.
Good-by, said Quinn, with his first touch of viciousness, and the visorphone slammed off.
Good-by, said Byerley imperturbably, to the blank plate.

Byerley brought his teacher back the week before election. The air car dropped quickly in an obscure part of the city.
Youll stay here till after election, Byerley told him. It would be better to have you out of the way if things take a bad turn.
The hoarse voice that twisted painfully out of Johns crooked mouth might have had accents of concern in it. Theres danger of violence?
The Fundamentalists threaten it, so I suppose there is, in a theoretical sense. But I really dont expect it. The Fundies have no real power. Theyre just the continuous irritant factor that might stir up a riot after a while. You dont mind staying here? Please, I wont be myself if I have to worry about you.
Oh, Ill stay. You still think it will go well?
Im sure of it. No one bothered you at the place?
No one. Im certain.
And your part went well?
Well enough. Therell be no trouble there.
Then take care of yourself, and watch the televisor tomorrow, John. Byerley pressed the gnarled hand that rested on his.

Lentons forehead was a furrowed study in suspense. He had the completely unenviable job of being Byerleys campaign manager in a campaign that wasnt a campaign, for a person that refused to reveal his strategy, and refused to accept his managers.
You cant! It was his favorite phrase. It had become his only phrase. I tell you, Steve, you cant!
He threw himself in front of the prosecutor, who was spending his time leafing through the typed pages of his speech.
Put that down, Steve. Look, that mob has been organized by the Fundies. You wont get a hearing. Youll be stoned more likely. Why do you have to make a speech before an audience? Whats wrong with a recording, a visual recording?
You want me to win the election, dont you? asked Byerley, mildly.
Win the election! Youre not going to win, Steve. Im trying to save your life.
Oh, Im not in danger.
Hes not in danger. Hes not in danger. Lenton made a queer, rasping sound in his throat. You mean youre getting out on that balcony in front of fifty thousand crazy crackpots and try to talk sense to them  on a balcony like a medieval dictator?
Byerley consulted his watch. In about five minutes  as soon as the televisor lines are free.
Lentons answering remark was not quite transliterable.

The crowd filled a roped off area of the city. Trees and houses seemed to grow out of a mass-human foundation. And by ultra-wave, the rest of the world watched. It was a purely local election, but it had a world audience just the same. Byerley thought of that and smiled.
But there was nothing to smile at in the crowd itself. There were banners and streamers, ringing every possible change on his supposed roboticity. The hostile attitude rose thickly and tangibly into the atmosphere.
From the start the speech was not successful. It competed against the inchoate mob howl and the rhythmic cries of the Fundie claques that formed mob-islands within the mob. Byerley spoke on, slowly, unemotionally
Inside, Lenton clutched his hair and groaned  and waited for the blood.

There was a writhing in the front ranks. An angular citizen with popping eyes, and clothes too short for the lank length of his limbs, was pulling to the fore. A policeman dived after him, making slow, struggling passage. Byerley waved the latter off, angrily.
The thin man was directly under the balcony. His words tore unheard against the roar.
Byerley leaned forward. What do you say? If you have a legitimate question, Ill answer it. He turned to a flanking guard. Bring that man up here.
There was a tensing in the crowd. Cries of Quiet started in various parts of the mob, and rose to a bedlam, then toned down raggedly. The thin man, red-faced and panting, faced Byerley.
Byerley said, Have you a question?
The thin man stared, and said in a cracked voice, Hit me!
With sudden energy, he thrust out his chin at an angle. Hit me! You say youre not a robot. Prove it. You cant hit a human, you monster.
There was a queer, flat, dead silence. Byerleys voice punctured it. I have no reason to hit you.
The thin man was laughing wildly. You cant hit me. You wont hit me. Youre not a human. Youre a monster, a make-believe man.
And Stephen Byerley, tight-lipped, in the face of thousands who watched in person and the millions, who watched by screen, drew back his fist and caught the man crackingly upon the chin. The challenger went over backwards in sudden collapse, with nothing on his face but blank, blank surprise.
Byerley said, Im sorry. Take him in and see that hes comfortable. I want to speak to him when Im through.
And when Dr. Calvin, from her reserved space, turned her automobile and drove off, only one reporter had recovered sufficiently from the shock to race after her, and shout an unheard question.
Susan Calvin called over her shoulder, Hes human.
That was enough. The reporter raced away in his own direction.
The rest of the speech might be described as Spoken but not heard.

Dr. Calvin and Stephen Byerley met once again  a week before he took the oath of office as mayor. It was late  past midnight.
Dr. Calvin said, You dont look tired.
The mayor-elect smiled. I may stay up for a while. Dont tell Quinn.
I shant. But that was an interesting story of Quinns, since you mention him. Its a shame to have spoiled it. I suppose you knew his theory?
Parts of it.
It was highly dramatic. Stephen Byerley was a young lawyer, a powerful speaker, a great idealist  and with a certain flare for biophysics. Are you interested in robotics, Mr. Byerley?
Only in the legal aspects.
This Stephen Byerley was. But there was an accident. Byerleys wife died, he himself, worse. His legs were gone; his face was gone; his voice was gone. Part of his mind was bent. He would not submit to plastic surgery. He retired from the world, legal career gone  only his intelligence, and his hands left. Somehow he could obtain positronic brains, even a complex one, one which had the greatest capacity of forming judgments in ethical problems  which is the highest robotic function so far developed.
He grew a body about it. Trained it to be everything he would have been and was no longer. He sent it out into the world as Stephen Byerley, remaining behind himself as the old, crippled teacher that no one ever saw
Unfortunately, said the mayor-elect, I ruined all that by hitting a man. The papers say it was your official verdict on the occasion that I was human.
How did that happen? Do you mind telling me? It couldnt have been accidental.
It wasnt entirely. Quinn did most of the work. My men started quietly spreading the fact that I had never hit a man; that I was unable to hit a man; that to fail to do so under provocation would be sure proof that I was a robot. So I arranged for a silly speech in public, with all sorts of publicity overtones, and almost inevitably, some fool fell for it. In its essence, it was what I call a shyster trick. One in which the artificial atmosphere which has been created does all the work. Of course, the emotional effects made my election certain, as intended.
The robopsychologist nodded. I see you intrude on my field  as every politician must, I suppose. But Im very sorry it turned out this way. I like robots. I like them considerably better than I do human beings. If a robot can be created capable of being a civil executive, I think hed make the best one possible. By the Laws of Robotics, hed be incapable of harming humans, incapable of tyranny, of corruption, of stupidity, of prejudice. And after he had served a decent term, he would leave, even though he were immortal, because it would be impossible for him to hurt humans by letting them know that a robot had ruled them. It would be most ideal.
Except that a robot might fail due to the inherent inadequacies of his brain. The positronic brain has never equaled the complexities of the human brain.
He would have advisers. Not even a human brain is capable of governing without assistance.
Byerley considered Susan Calvin with grave interest. Why do you smile, Dr. Calvin?
I smile because Mr. Quinn didnt think of everything.
You mean there could be more to that story of his.
Only a little. For the three months before election, this Stephen Byerley that Mr. Quinn spoke about, this broken man, was in the country for some mysterious reason. He returned in time for that famous speech of yours. And after all, what the old cripple did once, he could do a second time, particularly where the second job is very simple in comparison to the first.
I dont quite understand.
Dr. Calvin rose and smoothed her dress. She was obviously ready to leave. I mean there is one time when a robot may strike a human being without breaking the First Law. Just one time.
And when is that?
Dr. Calvin was at the door. She said quietly, When the human to be struck is merely another robot.
She smiled broadly, her thin face glowing. Good-by Mr. Byerley. I hope to vote for you five years from now  for Co-ordinator.
Stephen Byerley chuckled. I must reply that that is a somewhat farfetched idea.
The door closed behind her.
Little Lost Robot

Measures on Hyper Base had been taken in a sort of rattling fury  the muscular equivalent of a hysterical shriek.
To itemize them in order of both chronology and desperation, they were:

1.                   All work on the Hyperatomic Drive through all the space volume occupied by the Stations of the Twenty-Seventh Asteroidal Grouping came to a halt.

2.                   That entire volume of space was nipped out of the System, practically speaking. No one entered without permission. No one left under any conditions.

3.                   By special government patrol ship, Drs. Susan Calvin and Peter Bogert, respectively Head Psychologist and Mathematical Director of United States Robot & Mechanical Men Corporation, were brought to Hyper Base.

Susan Calvin had never left the surface of Earth before, and had no perceptible desire to leave it this time. In an age of Atomic Power and a clearly coming Hyperatomic Drive, she remained quietly provincial. So she was dissatisfied with her trip and unconvinced of the emergency, and every line of her plain, middle-aged face showed it clearly enough during her first dinner at Hyper Base.
Nor did Dr. Bogerts sleek paleness abandon a certain hangdog attitude. Nor did Major-general Kallner, who headed the project, even once forget to maintain a hunted expression. In short, it was a grisly episode, that meal, and the little session of three that followed began in a gray, unhappy manner.
Kallner, with his baldness glistening, and his dress uniform oddly unsuited to the general mood, began with uneasy directness.
This is a queer story to tell, sir, and madam. I want to thank you for coming on short notice and without a reason being given. Well try to correct that now. Weve lost a robot. Work has stopped and must stop until such time as we locate it. So far we have failed, and we feel we need expert help.
Perhaps the general felt his predicament anticlimactic. He continued with a note of desperation, I neednt tell you the importance of our work here. More than eighty percent of last years appropriations for scientific research have gone to us
Why, we know that, said Bogert, agreeably. U. S. Robots is receiving a generous rental fee for use of our robots.
Susan Calvin injected a blunt, vinegary note, What makes a single robot so important to the project, and why hasnt it been located?
The general turned his red face toward her and wet his lips quickly, Why, in a manner of speaking we have located it. Then, with near anguish, Here, suppose I explain. As soon as the robot failed to report a state of emergency was declared, and all movement off Hyper Base stopped. A cargo vessel had landed the previous day and had delivered us two robots for our laboratories. It had sixty-two robots of the... uh... game type for shipment elsewhere. We are certain as to that figure. There is no question about it whatever.
Yes? And the connection?
When our missing robot failed of location anywhere  I assure you we would have found a missing blade of grass if it had been there to find  we brainstormed ourselves into counting the robots left of the cargo ship. They have sixty-three now.
So that the sixty-third, I take it, is the missing prodigal? Dr. Calvins eyes darkened.
Yes, but we have no way of telling which is the sixty-third.
There was a dead silence while the electric clock chimed eleven times, and then the robopsychologist said, Very peculiar, and the corners of her lips moved downward.
Peter, she turned to her colleague with a trace of savagery, whats wrong here? What kind of robots are they, using at Hyper Base?
Dr. Bogert hesitated and smiled feebly, Its been rather a matter of delicacy till now, Susan.
She spoke rapidly, Yes, till now. If there are sixty-three same-type robots, one of which is wanted and the identity of which cannot be determined, why wont any of them do? Whats the idea of all this? Why have we been sent for?
Bogert said in resigned fashion, If youll give me a chance, Susan  Hyper Base happens to be using several robots whose brains are not impressioned with the entire First Law of Robotics.
Arent impressioned? Calvin slumped back in her chair, I see. How many were made?
A few. It was on government order and there was no way of violating the secrecy. No one was to know except the top men directly concerned. You werent included, Susan. It was nothing I had anything to do with.

The general interrupted with a measure of authority. I would like to explain that bit. I hadnt been aware that Dr. Calvin was unacquainted with the situation. I neednt tell you, Dr. Calvin, that there always has been strong opposition to robots on the Planet. The only defense the government has had against the Fundamentalist radicals in this matter was the fact that robots are always built with an unbreakable First Law  which makes it impossible for them to harm human beings under any circumstance.
But we had to have robots of a different nature. So just a few of the NS-2 model, the Nestors, that is, were prepared with a modified First Law. To keep it quiet, all NS-2s are manufactured without serial numbers; modified members are delivered here along with a group of normal robots; and, of course, all our kind are under the strictest impressionment never to tell of their modification to unauthorized personnel. He wore an embarrassed smile; This has all worked out against us now.
Calvin said grimly, Have you asked each one who it is, anyhow? Certainly, you are authorized?
The general nodded, All sixty-three deny having worked here  and one is lying.
Does the one you want show traces of wear? The others, I take it, are factory-fresh.
The one in question only arrived last month. It, and the two that have just arrived, were to be the last we needed. Theres no perceptible wear. He shook his head slowly and his eyes were haunted again, Dr. Calvin, we dont dare let that ship leave. If the existence of non-First Law robots becomes general knowledge There seemed no way of avoiding understatement in the conclusion.
Destroy all sixty-three, said the robopsychologist coldly and flatly, and make an end of it.
Bogert drew back a corner of his mouth. You mean destroy thirty thousand dollars per robot. Im afraid U. S. Robots wouldnt like that. Wed better make an effort first, Susan, before we destroy anything.
In that case, she said, sharply, I need facts. Exactly what advantage does Hyper Base derive from these modified robots? What factor made them desirable, general?
Kallner ruffled his forehead and stroked it with an upward gesture of his hand. We had trouble with our previous robots. Our men work with hard radiations a good deal, you see. Its dangerous, of course, but reasonable precautions are taken. There have been only two accidents since we began and neither was fatal. However, it was impossible to explain that to an ordinary robot. The First Law states  Ill quote it  No robot may harm a human being, or through inaction, allow a human being to come to harm.
Thats primary, Dr. Calvin. When it was necessary for one of our men to expose himself for a short period to a moderate gamma field, one that would have no physiological effects, the nearest robot would dash in to drag him out. If the field were exceedingly weak, it would succeed, and work could not continue till all robots were cleared out. If the field were a trifle stronger, the robot would never reach the technician concerned, since its positronic brain would collapse under gamma radiations  and then we would be out one expensive and hard-to-replace robot.
We tried arguing with them. Their point was that a human being in a gamma field was endangering his life and that it didnt matter that he could remain there half an hour safely. Supposing, they would say, he forgot and remained an hour. They couldnt take chances. We pointed out that they were risking their lives on a wild off-chance. But self-preservation is only the Third Law of Robotics  and the First Law of human safety came first. We gave them orders; we ordered them strictly and harshly to remain out of gamma fields at whatever cost. But obedience is only the Second Law of Robotics  and the First Law of human safety came first. Dr. Calvin, we either had to do without robots, or do something about the First Law  and we made our choice.
I cant believe, said Dr. Calvin, that it was found possible to remove the First Law.
It wasnt removed, it was modified, explained Kallner. Positronic brains were constructed that contained the positive aspect only of the Law, which in them reads: No robot may harm a human being. That is all. They have no compulsion to prevent one coming to harm through an extraneous agency such as gamma rays. I state the matter correctly, Dr. Bogert?
Quite, assented the mathematician.
And that is the only difference of your robots from the ordinary NS2 model? The only difference? Peter?
The only difference, Susan.
She rose and spoke with finality, I intend sleeping now, and in about eight hours, I want to speak to whomever saw the robot last. And from now on, General Kallner, if Im to take any responsibility at all for events, I want full and unquestioned control of this investigation.
Susan Calvin, except for two hours of resentful lassitude, experienced nothing approaching sleep. She signaled at Bogerts door at the local time of 0700 and found him also awake. He had apparently taken the trouble of transporting a dressing gown to Hyper Base with him, for he was sitting in it. He put his nail scissors down when Calvin entered.
He said softly, Ive been expecting you more or less. I suppose you feel sick about all this.
I do.
Well  Im sorry. There was no way of preventing it. When the call came out from Hyper Base for us, I knew that something must have gone wrong with the modified Nestors. But what was there to do? I couldnt break the matter to you on the trip here, as I would have liked to, because I had to be sure. The matter of the modification is top secret.
The psychologist muttered, I should have been told. U. S. Robots had no right to modify positronic brains this way without the approval of a psychologist.
Bogert lifted his eyebrows and sighed. Be reasonable, Susan. You couldnt have influenced them. In this matter, the government was bound to have its way. They want the Hyperatomic Drive and the etheric physicists want robots that wont interfere with them. They were going to get them even if it did mean twisting the First Law. We had to admit it was possible from a construction standpoint and they swore a mighty oath that they wanted only twelve, that they would be used only at Hyper Base, that they would be destroyed once the Drive was perfected, and that full precautions would be taken. And they insisted on secrecy  and thats the situation.
Dr. Calvin spoke through her teeth, I would have resigned.
It wouldnt have helped. The government was offering the company a fortune, and threatening it with antirobot legislation in case of a refusal. We were stuck then, and were badly stuck now. If this leaks out, it might hurt Kallner and the government, but it would hurt U. S. Robots a devil of a lot more.
The psychologist stared at him. Peter, dont you realize what all this is about? Cant you understand what the removal of the First Law means? It isnt just a matter of secrecy.
I know what removal would mean. Im not a child. It would mean complete instability, with no nonimaginary solutions to the positronic Field Equations.
Yes, mathematically. But can you translate that into crude psychological thought. All normal life, Peter, consciously or otherwise, resents domination. If the domination is by an inferior, or by a supposed inferior, the resentment becomes stronger. Physically, and, to an extent, mentally, a robot  any robot  is superior to human beings. What makes him slavish, then? Only the First Law! Why, without it, the first order you tried to give a robot would result in your death. Unstable? What do you think?
Susan, said Bogert, with an air of sympathetic amusement. Ill admit that this Frankenstein Complex youre exhibiting has a certain justification  hence the First Law in the first place. But the Law, I repeat and repeat, has not been removed  merely modified.
And what about the stability of the brain?
The mathematician thrust out his lips, Decreased, naturally. But its within the border of safety. The first Nestors were delivered to Hyper Base nine months ago, and nothing whatever has gone wrong till now, and even this involves merely fear of discovery and not danger to humans.
Very well, then. Well see what comes of the morning conference.
Bogert saw her politely to the door and grimaced eloquently when she left. He saw no reason to change his perennial opinion of her as a sour and fidgety frustration.
Susan Calvins train of thought did not include Bogert in the least. She had dismissed him years ago as a smooth and pretentious sleekness.

Gerald Black had taken his degree in etheric physics the year before and, in common with his entire generation of physicists, found himself engaged in the problem of the Drive. He now made a proper addition to the general atmosphere of these meetings on Hyper Base. In his stained white smock, he was half rebellious and wholly uncertain. His stocky strength seemed striving for release and his fingers, as they twisted each other with nervous yanks, might have forced an iron bar out of true.
Major-general Kallner sat beside him; the two from U. S. Robots faced him.
Black said, Im told that I was the last to see Nestor 10 before he vanished. I take it you want to ask me about that.
Dr. Calvin regarded him with interest, You sound as if you were not sure, young man. Dont you know whether you were the last to see him?
He worked with me, maam, on the field generators, and he was with me the morning of his disappearance. I dont know if anyone saw him after about noon. No one admits having done so.
Do you think anyones lying about it?
I dont say that. But I dont say that I want the blame of it, either. His dark eyes smoldered.
Theres no question of blame. The robot acted as it did because of what it is. Were just trying to locate it, Mr. Black, and lets put everything else aside. Now if youve worked with the robot, you probably know it better than anyone else. Was there anything unusual about it that you noticed? Had you ever worked with robots before?
Ive worked with other robots we have here  the simple ones. Nothing different about the Nestors except that theyre a good deal cleverer  and more annoying.
Annoying? In what way?
Well  perhaps its not their fault. The work here is rough and most of us get a little jagged. Fooling around with hyper-space isnt fun. He smiled feebly, finding pleasure in confession. We run the risk continually of blowing a hole in normal space-time fabric and dropping right out of the universe, asteroid and all. Sounds screwy, doesnt it? Naturally, youre on edge sometimes. But these Nestors arent. Theyre curious, theyre calm, they dont worry. Its enough to drive you nuts at times. When you want something done in a tearing hurry, they seem to take their time. Sometimes Id rather do without.
You say they take their time? Have they ever refused an order?
Oh, no, hastily. They do it all right. They tell you when they think youre wrong, though. They dont know anything about the subject but what we taught them, but that doesnt stop them. Maybe I imagine it, but the other fellows have the same trouble with their Nestors.
General Kallner cleared his throat ominously, Why have no complaints reached me on the matter, Black?
The young physicist reddened, We didnt really want to do without the robots, sir, and besides we werent certain exactly how such... uh... minor complaints might be received.
Bogert interrupted softly, Anything in particular happen the morning you last saw it?
There was a silence. With a quiet motion, Calvin repressed the comment that was about to emerge from Kallner, and waited patiently.
Then Black spoke in blurting anger, I had a little trouble with it. Id broken a Kimball tube that morning and was out five days of work; my entire program was behind schedule; I hadnt received any mail from home for a couple of weeks. And he came around wanting me to repeat an experiment I had abandoned a month ago. He was always annoying me on that subject and I was tired of it. I told him to go away  and thats all I saw of him.
You told him to go away? asked Dr. Calvin with sharp interest. In just those words? Did you say Go away? Try to remember the exact words.
There was apparently an internal struggle in progress. Black cradled his forehead in a broad palm for a moment, then tore it away and said defiantly, I said, Go lose yourself. 
Bogert laughed for a short moment. And he did, eh?
But Calvin wasnt finished. She spoke cajolingly, Now were getting somewhere, Mr. Black. But exact details are important. In understanding the robots actions, a word, a gesture, an emphasis may be everything. You couldnt have said just those three words, for instance, could you? By your own description you must have been in a hasty mood. Perhaps you strengthened your speech a little.
The young man reddened, Well... I may have called it a... a few things.
Exactly what things?
Oh  I wouldnt remember exactly. Besides I couldnt repeat it. You know how you get when youre excited. His embarrassed laugh was almost a giggle, I sort of have a tendency to strong language.
Thats quite all right, she replied, with prim severity. At the moment, Im a psychologist. I would like to have you repeat exactly what you said as nearly as you remember, and, even more important, the exact tone of voice you used.
Black looked at his commanding officer for support, found none. His eyes grew round and appalled, But I cant.
You must.
Suppose, said Bogert, with ill-hidden amusement, you address me. You may find it easier.
The young mans scarlet face turned to Bogert. He swallowed. I said His voice faded out. He tried again, I said
And he drew a deep breath and spewed it out hastily in one long succession of syllables. Then, in the charged air that lingered, he concluded almost in tears, ... more or less. I dont remember the exact order of what I called him, and maybe I left out something or put in something, but that was about it.
Only the slightest flush betrayed any feeling on the part of the robopsychologist. She said, I am aware of the meaning of most of the terms used. The others, I suppose, are equally derogatory.
Im afraid so, agreed the tormented Black.
And in among it, you told him to lose himself.
I meant it only figuratively.
I realize that. No disciplinary action is intended, I am sure. And at her glance, the general, who, five seconds earlier, had seemed not sure at all, nodded angrily.
You may leave, Mr. Black. Thank you for your cooperation.

It took five hours for Susan Calvin to interview the sixty-three robots. It was five hours of multi-repetition; of replacement after replacement of identical robot; of Questions A, B, C, D; and Answers A, B, C, D; of a carefully bland expression, a carefully neutral tone, a carefully friendly atmosphere; and a hidden wire recorder.
The psychologist felt drained of vitality when she was finished.
Bogert was waiting for her and looked expectant as she dropped the recording spool with a clang upon the plastic of the desk.
She shook her head, All sixty-three seemed the same to me. I couldnt tell
He said, You couldnt expect to tell by ear, Susan. Suppose we analyze the recordings.
Ordinarily, the mathematical interpretation of verbal reactions of robots is one of the more intricate branches of robotic analysis. It requires a staff of trained technicians and the help of complicated computing machines. Bogert knew that. Bogert stated as much, in an extreme of unshown annoyance after having listened to each set of replies, made lists of word deviations, and graphs of the intervals of responses.
There are no anomalies present, Susan. The variations in wording and the time reactions are within the limits of ordinary frequency groupings. We need finer methods. They must have computers here. No. He frowned and nibbled delicately at a thumbnail. We cant use computers. Too much danger of leakage. Or maybe if we
Dr. Calvin stopped him with an impatient gesture, Please, Peter. This isnt one of your petty laboratory problems. If we cant determine the modified Nestor by some gross difference that we can see with the naked eye, one that there is no mistake about, were out of luck. The danger of being wrong, and of letting him escape is otherwise too great. Its not enough to point out a minute irregularity in a graph. I tell you, if thats all Ive got to go on, Id destroy them all just to be certain. Have you spoken to the other modified Nestors?
Yes, I have, snapped back Bogert, and theres nothing wrong with them. Theyre above normal in friendliness if anything. They answered my questions, displayed pride in their knowledge  except the two new ones that havent had time to learn their etheric physics. They laughed rather good-naturedly at my ignorance in some of the specializations here. He shrugged, I suppose that forms some of the basis for resentment toward them on the part of the technicians here. The robots are perhaps too willing to impress you with their greater knowledge.
Can you try a few Planar Reactions to see if there has been any change, any deterioration, in their mental set-up since manufacture?
I havent yet, but I will. He shook a slim finger at her, Youre losing your nerve, Susan. I dont see what it is youre dramatizing. Theyre essentially harmless.
They are? Calvin took fire. They are? Do you realize one of them is lying? One of the sixty-three robots I have just interviewed has deliberately lied to me after the strictest injunction to tell the truth. The abnormality indicated is horribly deep-seated, and horribly frightening.
Peter Bogert felt his teeth harden against each other. He said, Not at all. Look! Nestor 10 was given orders to lose himself. Those orders were expressed in maximum urgency by the person most authorized to command him. You cant counteract that order either by superior urgency or superior right of command. Naturally, the robot will attempt to defend the carrying out of his orders. In fact, objectively, I admire his ingenuity. How better can a robot lose himself than to hide himself among a group of similar robots?
Yes, you would admire it. Ive detected amusement in you, Peter  amusement and an appalling lack of understanding. Are you a roboticist, Peter? Those robots attach importance to what they consider superiority. Youve just said as much yourself. Subconsciously they feel humans to be inferior and the First Law which protects us from them is imperfect. They are unstable. And here we have a young man ordering a robot to leave him, to lose himself, with every verbal appearance of revulsion, disdain, and disgust. Granted, that robot must follow orders, but subconsciously, there is resentment. It will become more important than ever for it to prove that it is superior despite the horrible names it was called. It may become so important that whats left of the First Law wont be enough.
How on Earth, or anywhere in the Solar System, Susan, is a robot going to know the meaning of the assorted strong language used upon him? Obscenity is not one of the things impressioned upon his brain.
Original impressionment is not everything, Calvin snarled at him. Robots have learning capacity, you... you fool And Bogert knew that she had really lost her temper. She continued hastily, Dont you suppose he could tell from the tone used that the words werent complimentary? Dont yon suppose hes heard the words used before and noted upon what occasions?
Well, then, shouted Bogert, will you kindly tell me one way in which a modified robot can harm a human being, no matter how offended it is, no matter how sick with desire to prove superiority?
If I tell you one way, will you keep quiet?
Yes.
They were leaning across the table at each other, angry eyes nailed together.
The psychologist said, If a modified robot were to drop a heavy weight upon a human being, he would not be breaking the First Law, if he did so with the knowledge that his strength and reaction speed would be sufficient to snatch the weight away before it struck the man. However once the weight left his fingers, he would be no longer the active medium. Only the blind force of gravity would be that. The robot could then change his mind and merely by inaction, allow the weight to strike. The modified First Law allows that.
Thats an awful stretch of imagination.
Thats what my profession requires sometimes. Peter, lets not quarrel, lets work. You know the exact nature of the stimulus that caused the robot to lose himself. You have the records of his original mental make-up. I want you to tell me how possible it is for our robot to do the sort of thing I just talked about. Not the specific instance, mind you, but that whole class of response. And I want it done quickly.
And meanwhile
And meanwhile, well have to try performance tests directly on the response to First Law.

Gerald Black, at his own request, was supervising the mushrooming wooden partitions that were springing up in a bellying circle on the vaulted third floor of Radiation Building 2. The laborers worked, in the main, silently, but more than one was openly a-wonder at the sixty-three photocells that required installation.
One of them sat down near Black, removed his hat, and wiped his forehead thoughtfully with a freckled forearm.
Black nodded at him, Hows it doing, Walensky?
Walensky shrugged and fired a cigar, Smooth as butter. Whats going on anyway, Doc? First, theres no work for three days and then we have this mess of jiggers. He leaned backward on his elbows and puffed smoke.
Black twitched his eyebrows, A couple of robot men came over from Earth. Remember the trouble we had with robots running into the gamma fields before we pounded it into their skulls that they werent to do it.
Yeah. Didnt we get new robots?
We got some replacements, but mostly it was a job of indoctrination. Anyway, the people who make them want to figure out robots that arent hit so bad by gamma rays.
Sure seems funny, though, to stop all the work on the Drive for this robot deal. I thought nothing was allowed to stop the Drive.
Well, its the fellows upstairs that have the say on that. Me  I just do as Im told. Probably all a matter of pull
Yeah, the electrician jerked a smile, and winked a wise eye. Somebody knew somebody in Washington. But as long as my pay comes through on the dot, I should worry. The Drives none of my affair. What are they going to do here?
Youre asking me? They brought a mess of robots with them,  over sixty, and theyre going to measure reactions. Thats all my knowledge.
How long will it take?
I wish I knew.
Well, Walensky said, with heavy sarcasm, as long as they dish me my money, they can play games all they want.
Black felt quietly satisfied. Let the story spread. It was harmless, and near enough to the truth to take the fangs out of curiosity.

A man sat in the chair, motionless, silent. A weight dropped, crashed downward, then pounded aside at the last moment under the synchronized thump of a sudden force beam. In sixty-three wooden cells, watching NS-2 robots dashed forward in that split second before the weight veered, and sixty-three photocells five feet ahead of their original positions jiggled the marking pen and presented a little jag on the paper. The weight rose and dropped, rose and dropped, rose
Ten times!
Ten times the robots sprang forward and stopped, as the man remained safely seated.

Major-general Kallner had not worn his uniform in its entirety since the first dinner with the U. S. Robot representatives. He wore nothing over his blue-gray shirt now, the collar was open, and the black tie was pulled loose.
He looked hopefully at Bogert, who was still blandly neat and whose inner tension was perhaps betrayed only by the trace of glister at his temples.
The general said, How does it look? What is it youre trying to see?
Bogert replied, A difference which may turn out to be a little too subtle for our purposes, Im afraid. For sixty-two of those robots the necessity of jumping toward the apparently threatened human was what we call, in robotics, a forced reaction. You see, even when the robots knew that the human in question would not come to harm  and after the third or fourth time they must have known it  they could not prevent reacting as they did. First Law requires it
Well?
But the sixty-third robot, the modified Nestor, had no such compulsion. He was under free action. If he had wished, he could have remained in his seat. Unfortunately, said his voice was mildly regretful, he didnt so wish.
Why do you suppose?
Bogert shrugged, I suppose Dr. Calvin will tell us when she gets here. Probably with a horribly pessimistic interpretation, too. She is sometimes a bit annoying.
Shes qualified, isnt she? demanded the general with a sudden frown of uneasiness.
Yes. Bogert seemed amused. Shes qualified all right. She understands robots like a sister  comes from hating human beings so much, I think. Its just that, psychologist or not, shes an extreme neurotic. Has paranoid tendencies. Dont take her too seriously.
He spread the long row of broken-line graphs out in front of him. You see, general, in the case of each robot the time interval from moment of drop to the completion of a five-foot movement tends to decrease as the tests are repeated. Theres a definite mathematical relationship that governs such things and failure to conform would indicate marked abnormality in the positronic brain. Unfortunately, all here appear normal.
But if our Nestor 10 was not responding with a forced action, why isnt his curve different? I dont understand that.
Its simple enough. Robotic responses are not perfectly analogous to human responses, mores the pity. In human beings, voluntary action is much slower than reflex action. But thats not the case with robots; with them it is merely a question of freedom of choice, otherwise the speeds of free and forced action are much the same. What I had been expecting, though, was that Nestor 10 would be caught by surprise the first time and allow too great an interval to elapse before responding.
And he didnt?
Im afraid not.
Then we havent gotten anywhere. The general sat back with an expression of pain. Its five days since youve come.
At this point, Susan Calvin entered and slammed the door behind her. Put your graphs away, Peter, she cried, you know they dont show anything.
She mumbled something impatiently as Kallner half-rose to greet her, and went on, Well have to try something else quickly. I dont like whats happening.
Bogert exchanged a resigned glance with the general. Is anything wrong?
You mean specifically? No. But I dont like to have Nestor 10 continue to elude us. Its bad. It must be gratifying his swollen sense of superiority. Im afraid that his motivation is no longer simply one of following orders. I think its becoming more a matter of sheer neurotic necessity to outthink humans. Thats a dangerously unhealthy situation. Peter, have you done what I asked? Have you worked out the instability factors of the modified NS-2 along the lines I want?
Its in progress, said the mathematician, without interest.
She stared at him angrily for a moment, then turned to Kallner. Nester 10 is decidedly aware of what were doing, general. He had no reason to jump for the bait in this experiment, especially after the first time, when he must have seen that there was no real danger to our subject. The others couldnt help it; but he was deliberately falsifying a reaction.
What do you think we ought to do now, then, Dr. Calvin?
Make it impossible for him to fake an action the next time. We will repeat the experiment, but with an addition. High-tension cables, capable of electrocuting the Nestor models will be placed between subject and robot  enough of them to avoid the possibility of jumping over  and the robot will be made perfectly aware in advance that touching the cables will mean death.
Hold on, spat out Bogert with sudden viciousness. I rule that out. We are not electrocuting two million dollars worth of robots to locate Nestor 10. There are other ways.
Youre certain? Youve found none. In any case, its not a question of electrocution. We can arrange a relay which will break the current at the instant of application of weight. If the robot should place his weight on it, he wont die. But he wont know that, you see.
The generals eyes gleamed into hope. Will that work?
It should. Under those conditions, Nestor 10 would have to remain in his seat. He could be ordered to touch the cables and die, for the Second Law of obedience is superior to the Third Law of self-preservation. But he wont be ordered to; he will merely be left to his own devices, as will all the robots. In the case of the normal robots, the First Law of human safety will drive them to their death even without orders. But not our Nestor 10. Without the entire First Law, and without having received any orders on the matter, the Third Law, self-preservation, will be the highest operating, and he will have no choice but to remain in his seat. It would be a forced action.
Will it be done tonight, then?
Tonight, said the psychologist, if the cables can be laid in time. Ill tell the robots now what theyre to be up against.

A man sat in the chair, motionless, silent. A weight dropped, crashed downward, then pounded aside at the last moment under the synchronized thump of a sudden force beam.
Only once
And from her small camp chair in the observing booth in the balcony, Dr. Susan Calvin rose with a short gasp of pure horror.
Sixty-three robots sat quietly in their chairs, staring owlishly at the endangered man before them. Not one moved.

Dr. Calvin was angry, angry almost past endurance. Angry the worse for not daring to show it to the robots that, one by one were entering the room and then leaving. She checked the list. Number twenty-eight was due in now  Thirty-five still lay ahead of her.
Number Twenty-eight entered, diffidently.
She forced herself into reasonable calm. And who are you?
The robot replied in a low, uncertain voice, I have received no number of my own yet, maam. Im an NS-2 robot, and I was Number Twenty-eight in line outside. I have a slip of paper here that Im to give to you.
You havent been in here before this today?
No, maam.
Sit down. Right there. I want to ask you some questions, Number Twenty-eight. Were you in the Radiation Room of Building Two about four hours ago?
The robot had trouble answering. Then it came out hoarsely, like machinery needing oil, Yes, maam.
There was a man who almost came to harm there, wasnt there?
Yes, maam.
You did nothing, did you?
No, maam.
The man might have been hurt because of your inaction. Do you know that?
Yes, maam. I couldnt help it, maam. It is hard to picture a large expressionless metallic figure cringing, but it managed.
I want you to tell me exactly why you did nothing to save him.
I want to explain, maam. I certainly dont want to have you... have anyone... think that I could do a thing that might cause harm to a master. Oh, no, that would be a horrible... an inconceivable
Please dont get excited, boy. Im not blaming you for anything. I only want to know what you were thinking at the time.
Maam, before it all happened you told us that one of the masters would be in danger of harm from that weight that keeps falling and that we would have to cross electric cables if we were to try to save him. Well, maam, that wouldnt stop me. What is my destruction compared to the safety of a master? But... but it occurred to me that if I died on my way to him, I wouldnt be able to save him anyway. The weight would crush him and then I would be dead for no purpose and perhaps some day some other master might come to harm who wouldnt have, if I had only stayed alive. Do you understand me, maam?
You mean that it was merely a choice of the man dying, or both the man and yourself dying. Is that right?
Yes, maam. It was impossible to save the master. He might be considered dead. In that case, it is inconceivable that I destroy myself for nothing  without orders.
The robopsychologist twiddled a pencil. She had heard the same story with insignificant verbal variations twenty-seven times before. This was the crucial question now.
Boy, she said, your thinking has its points, but it is not the sort of thing I thought you might think. Did you think of this yourself?
The robot hesitated. No.
Who thought of it, then?
We were talking last night, and one of us got that idea and it sounded reasonable.
Which one?
The robot thought deeply. I dont know. Just one of us.
She sighed, Thats all.
Number Twenty-nine was next. Thirty-four after that.

Major-general Kallner, too, was angry. For one week all of Hyper Base had stopped dead, barring some paper work on the subsidiary asteroids of the group. For nearly one week, the two top experts in the field had aggravated the situation with useless tests. And now they  or the woman, at any rate  made impossible propositions.
Fortunately for the general situation, Kallner felt it impolitic to display his anger openly.
Susan Calvin was insisting, Why not, sir? Its obvious that the present situation is unfortunate. The only way we may reach results in the future  or what future is left us in this matter  is to separate the robots. We cant keep them together any longer.
My dear Dr. Calvin, rumbled the general, his voice sinking into the lower baritone registers. I dont see how I can quarter sixty-three robots all over the place
Dr. Calvin raised her arms helplessly. I can do nothing then. Nestor 10 will either imitate what the other robots would do, or else argue them plausibly into not doing what he himself cannot do. And in any case, this is bad business. Were in actual combat with this little lost robot of ours and hes winning out. Every victory of his aggravates his abnormality.
She rose to her feet in determination. General Kallner, if you do not separate the robots as I ask, then I can only demand that all sixty-three be destroyed immediately.
You demand it, do you? Bogert looked up suddenly, and with real anger. What gives you the right to demand any such thing? Those robots remain as they are. Im responsible to the management, not you.
And I, added Major-general Kallner, am responsible to the World Co-ordinator  and I must have this settled.
In that case, flashed back Calvin, there is nothing for me to do but resign. If necessary to force you to the necessary destruction, Ill make this whole matter public. It was not I that approved the manufacture of modified robots.
One word from you, Dr. Calvin, said the general, deliberately, in violation of security measures, and you would be certainly imprisoned instantly.
Bogert felt the matter to be getting out of hand. His voice grew syrupy, Well, now, were beginning to act like children, all of us. We need only a little more time. Surely we can outwit a robot without resigning, or imprisoning people, or destroying two millions.
The psychologist turned on him with quiet fury, I dont want any unbalanced robots in existence. We have one Nestor thats definitely unbalanced, eleven more that are potentially so, and sixty-two normal robots that are being subjected to an unbalanced environment. The only absolute safe method is complete destruction.
The signal-burr brought all three to a halt, and the angry tumult of growingly unrestrained emotion froze.
Come in, growled Kallner.
It was Gerald Black, looking perturbed. He had heard angry voices. He said, I thought Id come myself... didnt like to ask anyone else
What is it? Dont orate
The locks of Compartment C in the trading ship have been played with. There are fresh scratches on them.
Compartment C? explained Calvin quickly. Thats the one that holds the robots, isnt it? Who did it?
From the inside, said Black, laconically.
The lock isnt out of order, is it?
No. Its all right. Ive been staying on the ship now for four days and none of them have tried to get out. But I thought you ought to know, and I didnt like to spread the news. I noticed the matter myself.
Is anyone there now? demanded the general.
I left Robbins and McAdams there.
There was a thoughtful silence, and then Dr. Calvin said, ironically, Well?
Kallner rubbed his nose uncertainly, Whats it all about?
Isnt it obvious? Nester 10 is planning to leave. That order to lose himself is dominating his abnormality past anything we can do. I wouldnt be surprised if whats left of his First Law would scarcely be powerful enough to override it. He is perfectly capable of seizing the ship and leaving with it. Then wed have a mad robot on a spaceship. What would he do next? Any idea? Do you still want to leave them all together, general?
Nonsense, interrupted Bogert. He had regained his smoothness. All that from a few scratch marks on a lock.
Have you, Dr. Bogert, completed the analysis Ive required, since you volunteer opinions?
Yes.
May I see it?
No.
Why not? Or maynt I ask that, either?
Because theres no point in it, Susan. I told you in advance that these modified robots are less stable than the normal variety, and my analysis shows it. Theres a certain very small chance of breakdown under extreme circumstances that are not likely to occur. Let it go at that. I wont give you ammunition for your absurd claim that sixty-two perfectly good robots be destroyed just because so far you lack the ability to detect Nestor 10 among them.
Susan Calvin stared him down and let disgust fill her eyes. You wont let anything stand in the way of the permanent directorship, will you?
Please, begged Kallner, half in irritation. Do you insist that nothing further can be done, Dr. Calvin?
I cant think of anything, sir, she replied, wearily. If there were only other differences between Nestor 10 and the normal robots, differences that didnt involve the First Law. Even one other difference. Something in impressionment, environment, specification And she stopped suddenly.
What is it?
Ive thought of something... I think Her eyes grew distant and hard, These modified Nestors, Peter. They get the same impressioning the normal ones get, dont they?
Yes. Exactly the same.
And what was it you were saying, Mr. Black, she turned to the young man, who through the storms that had followed his news had maintained a discreet silence. Once when complaining of the Nestors attitude of superiority, you said the technicians had taught them all they knew.
Yes, in etheric physics. Theyre not acquainted with the subject when they come here.
Thats right, said Bogert, in surprise. I told you, Susan, when I spoke to the other Nestors here that the two new arrivals hadnt learned etheric physics yet.
And why is that? Dr. Calvin was speaking in mounting excitement. Why arent NS-2 models impressioned with etheric physics to start with?
I can tell you that, said Kallner. Its all of a piece with the secrecy. We thought that if we made a special model with knowledge of etheric physics, used twelve of them and put the others to work in an unrelated field, there might be suspicion. Men working with normal Nestors might wonder why they knew etheric physics. So there was merely an impressionment with a capacity for training in the field. Only the ones that come here, naturally, receive such a training. Its that simple.
I understand. Please get out of here, the lot of you. Let me have an hour or so.

Calvin felt she could not face the ordeal for a third time. Her mind had contemplated it and rejected it with an intensity that left her nauseated. She could face that unending file of repetitious robots no more.
So Bogert asked the question now, while she sat aside, eyes and mind half closed.
Number Fourteen came in  forty-nine to go.
Bogert looked up from the guide sheet and said, What is your number in line?
Fourteen, sir. The robot presented his numbered ticket.
Sit down, boy.
Bogert asked, You havent been here before on this day?
No, sir.
Well, boy, we are going to have another man in danger of harm soon after were through here. In fact, when you leave this room, you will be led to a stall where you will wait quietly, till you are needed. Do you understand?
Yes, sir.
Now, naturally, if a man is in danger of harm, you will try to save him.
Naturally, sir.
Unfortunately, between the man and yourself, there will be a gamma ray field.
Silence.
Do you know what gamma rays are? asked Bogert sharply.
Energy radiation, sir?
The next question came in a friendly, offhand manner, Ever work with gamma rays?
No, sir. The answer was definite.
Mm-m. Well, boy, gamma rays will kill you instantly. Theyll destroy your brain. That is a fact you must know and remember. Naturally, you dont want to destroy yourself.
Naturally. Again the robot seemed shocked. Then, slowly, But, sir, if the gamma rays are between myself and the master that may be harmed, how can I save him? I would be destroying myself to no purpose.
Yes, there is that, Bogert seemed concerned about the matter. The only thing I can advise, boy, is that if you detect the gamma radiation between yourself and the man, you may as well sit where you are.
The robot was openly relieved. Thank you, sir. There wouldnt be any use, would there?
Of course not. But if there werent any dangerous radiation, that would be a different matter.
Naturally, sir. No question of that.
You may leave now. The man on the other side of the door will lead you to your stall. Please wait there.
He turned to Susan Calvin when the robot left. How did that go, Susan?
Very well, she said, dully.
Do you think we could catch Nestor 10 by quick questioning on etheric physics?
Perhaps, but its not sure enough. Her hands lay loosely in her lap. Remember, hes fighting us. Hes on his guard. The only way we can catch him is to outsmart him  and, within his limitations, he can think much more quickly than a human being.
Well, just for fun  suppose I ask the robots from now on a few questions on gamma rays. Wave length limits, for instance.
No! Dr. Calvins eyes sparked to life. It would be too easy for him to deny knowledge and then hed be warned against the test thats coming up  which is our real chance. Please follow the questions Ive indicated, Peter, and dont improvise. Its just within the bounds of risk to ask them if theyve ever worked with gamma rays. And try to sound even less interested than you do when you ask it.
Bogert shrugged, and pressed the buzzer that would allow the entrance of Number Fifteen.
The large Radiation Room was in readiness once more. The robots waited patiently in their wooden cells, all open to the center but closed off from each other.
Major-general Kallner mopped his brow slowly with a large handkerchief while Dr. Calvin checked the last details with Black.
Youre sure now, she demanded, that none of the robots have had a chance to talk with each other after leaving the Orientation Room?
Absolutely sure, insisted Black. Theres not been a word exchanged.
And the robots are put in the proper stalls?
Heres the plan.
The psychologist looked at it thoughtfully, Um-m-m.
The general peered over her shoulder. Whats the idea of the arrangement, Dr. Calvin?
Ive asked to have those robots that appeared even slightly out of true in the previous tests concentrated on one side of the circle. Im going to be sitting in the center myself this time, and I wanted to watch those particularly.
Youre going to be sitting there, exclaimed Bogert.
Why not? she demanded coldly. What I expect to see may be something quite momentary. I cant risk having anyone else as main observer. Peter, youll be in the observing booth, and I want you to keep your eye on the opposite side of the circle. General Kallner, Ive arranged for motion pictures to be taken of each robot, in case visual observation isnt enough. If these are required, the robots are to remain exactly where they are until the pictures are developed and studied. None must leave, none must change place. Is that clear?
Perfectly.
Then lets try it this one last time.

Susan Calvin sat in the chair, silent, eyes restless. A weight dropped, crashed downward; then pounded aside at the last moment under the synchronized thump of a sudden force beam.
And a single robot jerked upright and took two steps.
And stopped.
But Dr. Calvin was upright, and her finger pointed to him sharply. Nestor 10, come here, she cried, come here! COME HERE!
Slowly, reluctantly, the robot took another step forward. The psychologist shouted at the top of her voice, without taking her eyes from the robot, Get every other robot out of this place, somebody. Get them out quickly, and keep them out.
Somewhere within reach of her ears there was noise, and the thud of hard feet upon the floor. She did not look away.
Nestor 10  if it was Nestor 10  took another step, and then, under force of her imperious gesture, two more. He was only ten feet away, when he spoke harshly, I have been told to be lost
Another stop. I must not disobey. They have not found me so far  He would think me a failure  He told me  But its not so  I am powerful and intelligent
The words came in spurts.
Another step. I know a good deal  He would think... I mean Ive been found  Disgraceful  Not I  I am intelligent  And by just a master... who is weak  Slow
Another step  and one metal arm flew out suddenly to her shoulder, and she felt the weight bearing her down. Her throat constricted, and she felt a shriek tear through.
Dimly, she heard Nestor 10s next words, No one must find me. No master and the cold metal was against her, and she was sinking under the weight of it.
And then a queer, metallic sound, and she was on the ground with an unfelt thump, and a gleaming arm was heavy across her body. It did not move. Nor did Nestor 10, who sprawled beside her.
And now faces were bending over her.
Gerald Black was gasping, Are you hurt, Dr. Calvin?
She shook her head feebly. They pried the arm off her and lifted her gently to her feet, What happened?
Black said, I bathed the place in gamma rays for five seconds. We didnt know what was happening. It wasnt till the last second that we realized he was attacking you, and then there was no time for anything but a gamma field. He went down in an instant. There wasnt enough to harm you though. Dont worry about it.
Im not worried. She closed her eyes and leaned for a moment upon his shoulder. I dont think I was attacked exactly. Nestor 10 was simply trying to do so. What was left of the First Law was still holding him back.

Susan Calvin and Peter Bogert, two weeks after their first meeting with Major-general Kallner had their last. Work at Hyper Base had been resumed. The trading ship with its sixty-two normal NS-2s was gone to wherever it was bound, with an officially imposed story to explain its two weeks delay. The government cruiser was making ready to carry the two roboticists back to Earth.
Kallner was once again a-gleam in dress uniform. His white gloves shone as he shook hands.
Calvin said, The other modified Nestors are, of course, to be destroyed.
They will be. Well make shift with normal robots, or, if necessary, do without.
Good.
But tell me  you havent explained  how was it done?
She smiled tightly, Oh, that. I would have told you in advance if I had been more certain of its working. You see, Nestor 10 had a superiority complex that was becoming more radical all the time. He liked to think that he and other robots knew more than human beings. It was becoming very important for him to think so.
We knew that. So we warned every robot in advance that gamma rays would kill them, which it would, and we further warned them all that gamma rays would be between them and myself. So they all stayed where they were, naturally. By Nestor 10s own logic in the previous test they had all decided that there was no point in trying to save a human being if they were sure to die before they could do it.
Well, yes, Dr. Calvin, I understand that. But why did Nestor 10 himself leave his seat?
AH! That was a little arrangement between myself and your young Mr. Black. You see it wasnt gamma rays that flooded the area between myself and the robots  but infrared rays. Just ordinary heat rays, absolutely harmless. Nestor 10 knew they were infrared and harmless and so he began to dash out, as he expected the rest would do, under First Law compulsion. It was only a fraction of a second too late that he remembered that the normal NS-2s could detect radiation, but could not identify the type. That he himself could only identify wavelengths by virtue of the training he had received at Hyper Base, under mere human beings, was a little too humiliating to remember for just a moment. To the normal robots the area was fatal because we had told them it would be, and only Nestor 10 knew we were lying.
And just for a moment he forgot, or didnt want to remember, that other robots might be more ignorant than human beings. His very superiority caught him. Good-by, general.

The Evitable Conflict

The Co-ordinator, in his private study, had that medieval curiosity, a fireplace. To be sure, the medieval man might not have recognized it as such, since it had no functional significance. The quiet, licking flame lay in an insulated recess behind clear quartz.
The logs were ignited at long distance through a trifling diversion of the energy beam that fed the public buildings of the city. The same button that controlled the ignition first dumped the ashes of the previous fire, and allowed for the entrance of fresh wood.  It was a thoroughly domesticated fireplace, you see.
But the fire itself was real. It was wired for sound, so that you could hear the crackle and, of course, you could watch it leap in the air stream that fed it.
The Co-ordinators ruddy glass reflected, in miniature, the discreet gamboling of the flame, and, in even further miniature, it was reflected in each of his brooding pupils.
And in the frosty pupils of his guest, Dr. Susan Calvin of U. S. Robots & Mechanical Men Corporation.
The Co-ordinator said, I did not ask you here entirely for social purposes, Susan.
I did not think you did, Stephen, she replied.
And yet I dont quite know how to phrase my problem. On the one hand, it can be nothing at all. On the other, it can mean the end of humanity.
I have come across so many problems, Stephen, that presented the same alternative. I think all problems do.
Really? Then judge this  World Steel reports an overproduction of twenty thousand long tons. The Mexican Canal is two months behind schedule. The mercury mines at Almaden have experienced a production deficiency since last spring, while the Hydroponics plant at Tientsin has been laying men off. These items happen to come to mind at the moment. There is more of the same sort.
Are these things serious? Im not economist enough to trace the fearful consequences of such things.
In themselves, they are not serious. Mining experts can be sent to Almaden, if the situation were to get worse. Hydroponics engineers can be used in Java or in Ceylon, if there are too many at Tientsin. Twenty thousand long tons of steel wont fill more than a few days of world demand, and the opening of the Mexican Canal two months later than the planned date is of little moment. Its the Machines that worry me; Ive spoken to your Director of Research about them already.
To Vincent Silver?  He hasnt mentioned anything about it to me.
I asked him to speak to no one. Apparently, he hasnt.
And what did he tell you?
Let me put that item in its proper place. I want to talk about the Machines first. And I want to talk about them to you, because youre the only one in the world who understands robots well enough to help me now.  May I grow philosophical?
For this evening, Stephen, you may talk how you please and of what you please, provided you tell me first what you intend to prove.
That such small unbalances in the perfection of our system of supply and demand, as I have mentioned, may be the first step towards the final war.
Hmp. Proceed.
Susan Calvin did not allow herself to relax, despite the designed comfort of the chair she sat in. Her cold, thin-lipped face and her flat, even voice were becoming accentuated with the years. And although Stephen Byerley was one man she could like and trust, she was almost seventy and the cultivated habits of a lifetime are not easily broken.
Every period of human development, Susan, said the Co-ordinator, has had its own particular type of human conflict  its own variety of problem that, apparently, could be settled only by force. And each time, frustratingly enough, force never really settled the problem. Instead, it persisted through a series of conflicts, then vanished of itself,  whats the expression,  ah, yes not with a bang, but a whimper, as the economic and social environment changed. And then, new problems, and a new series of wars,  apparently endlessly cyclic.
Consider relatively modern times. There were the series of dynastic wars in the sixteenth to eighteenth centuries, when the most important question in Europe was whether the houses of Hapsburg or Valois-Bourbon were to rule the continent. It was one of those inevitable conflicts, since Europe could obviously not exist half one and half the other.
Except that it did, and no war ever wiped out the one and established the other, until the rise of a new social atmosphere in France in 1789 tumbled first the Bourbons and, eventually, the Hapsburgs down the dusty chute to historys incinerator.
And in those same centuries there were the more barbarous religious wars, which revolved about the important question of whether Europe was to be Catholic or Protestant. Half and half she could not be. It was inevitable that the sword decide.  Except that it didnt. In England, a new industrialism was growing, and on the continent, a new nationalism. Half and half Europe remains to this day and no one cares much.
In the nineteenth and twentieth centuries, there was a cycle of nationalist-imperialist wars, when the most important question in the world was which portions of Europe would control the economic resources and consuming capacity of which portions of non-Europe. All non-Europe obviously could not exist part English and part French and part German and so on.  Until the forces of nationalism spread sufficiently, so that non-Europe ended what all the wars could not, and decided it could exist quite comfortably all non-European.
And so we have a pattern
Yes. Stephen, you make it plain, said Susan Calvin. These are not very profound observations.
No.  But then, it is the obvious which is so difficult to see most of the time. People say Its as plain as the nose on your face. But how much of the nose on your face can you see, unless someone holds a mirror up to you? In the twentieth century, Susan, we started a new cycle of wars  what shall I call them? Ideological wars? The emotions of religion applied to economic systems, rather than to extra-natural ones? Again the wars were inevitable and this time there were atomic weapons, so that mankind could no longer live through its torment to the inevitable wasting away of inevitability.  And positronic robots came.
They came in time, and, with it and alongside it, interplanetary travel.  So that it no longer seemed so important whether the world was Adam Smith or Karl Marx. Neither made very much sense under the new circumstances. Both had to adapt and they ended in almost the same place.
A deus ex machina, then, in a double sense, said Dr. Calvin, dryly.
The Co-ordinator smiled gently, I have never heard you pun before, Susan, but you are correct. And yet there was another danger. The ending of every other problem had merely given birth to another. Our new worldwide robot economy may develop its own problems, and for that reason we have the Machines. The Earths economy is stable, and will remain stable, because it is based upon the decisions of calculating machines that have the good of humanity at heart through the overwhelming force of the First Law of Robotics.
Stephen Byerley continued, And although the Machines are nothing but the vastest conglomeration of calculating circuits ever invented, they are still robots within the meaning of the First Law, and so our Earth-wide economy is in accord with the best interests of Man. The population of Earth knows that there will be no unemployment, no over-production or shortages. Waste and famine are words in history books. And so the question of ownership of the means of production becomes obsolescent. Whoever owned them (if such a phrase has meaning), a man, a group, a nation, or all mankind, they could be utilized only as the Machines directed.  Not because men were forced to but because it was the wisest course and men knew it.
It puts an end to war  not only to the last cycle of wars, but to the next and to all of them. Unless
A long pause, and Dr. Calvin encouraged him by repetition. Unless
The fire crouched and skittered along a log, then popped up.
Unless, said the Co-ordinator, the Machines dont fulfill their function.
I see. And that is where those trifling maladjustments come in which you mentioned awhile ago  steel, hydroponics and so on.
Exactly. Those errors should not be. Dr. Silver tells me they cannot be.
Does he deny the facts? How unusual!
No, he admits the facts, of course. I do him an injustice. What he denies is that any error in the machine is responsible for the so-called (his phrase) errors in the answers. He claims that the Machines are self-correcting and that it would violate the fundamental laws of nature for an error to exist in the circuits of relays. And so I said 
And you said, Have your boys check them and make sure, anyway.
Susan, you read my mind. It was what I said, and he said he couldnt.
Too busy?
No, he said that no human could. He was frank about it. He told me, and I hope I understand him properly, that the Machines are a gigantic extrapolation. Thus, a team of mathematicians work several years calculating a positronic brain equipped to do certain similar acts of calculation. Using this brain they make further calculations to create a still more complicated brain, which they use again to make one still more complicated and so on. According to Silver, what we call the Machines are the result of ten such steps.
Ye-es, that sounds familiar. Fortunately, Im not a mathematician. Poor Vincent. He is a young man. The Directors before him, Alfred Lanning and Peter Bogert, are dead, and they had no such problems. Nor had I. Perhaps roboticists as a whole should now die, since we can no longer understand our own creations.
Apparently not. The Machines are not super-brains in Sunday supplement sense,  although they are so pictured in the Sunday supplements. It is merely that in their own particular province of collecting and analyzing a nearly infinite number of data and relationships thereof, in nearly infinitesimal time, they have progressed beyond the possibility of detailed human control.
And then I tried something else. I actually asked the Machine. In the strictest secrecy, we fed it the original data involved in the steel decision, its own answer, and the actual developments since, the overproduction, that is,  and asked for an explanation of the discrepancy.
Good, and what was its answer?
I can quote you that word for word: The matter admits of no explanation. 
And how did Vincent interpret that?
In two ways. Either we had not given the Machine enough data to allow a definite answer, which was unlikely. Dr. Silver admitted that.  Or else, it was impossible for the Machine to admit that it could give any answer to data which implied that it could harm a human being. This, naturally, is implied by the First Law. And then Dr. Silver recommended that I see you.
Susan Calvin looked very tired, Im old, Stephen. When Peter Bogert died, they wanted to make me Director of Research and I refused. I wasnt young then, either, and I did not wish the responsibility. They let young Silver have it and that satisfied me; but what good is it, if I am dragged into such messes.
Stephen, let me state my position. My researches do indeed involve the interpretation of robot behavior in the light of the Three Laws of Robotics. Here, now, we have these incredible calculating machines. They are positronic robots and therefore obey the Laws of Robotics. But they lack personality; that is, their functions are extremely limited. Must be, since they are so specialized. Therefore, there is very little room for the interplay of the Laws, and my one method of attack is virtually useless. In short, I dont know that I can help you, Stephen.
The Co-ordinator laughed shortly, Nevertheless, let me tell you the rest. Let me give you my theories, and perhaps you will then be able to tell me whether they are possible in the light of robopsychology.
By all means. Go ahead.
Well, since the Machines are giving the wrong answers, then, assuming that they cannot be in error, there is only one possibility. They are being given the wrong data! In other words, the trouble is human, and not robotic. So I took my recent planetary inspection tour
From which you have just returned to New York.
Yes. It was necessary, you see, since there are four Machines, one handling each of the Planetary Regions. And all four are yielding imperfect results.
Oh, but that follows, Stephen. If any one of the Machines is imperfect, that will automatically reflect in the result of the other three, since each of the others will assume as part of the data on which they base their own decisions, the perfection of the imperfect fourth. With a false assumption, they will yield false answers.
Uh-huh. So it seemed to me. Now, I have here the records of my interviews with each of the Regional Vice-Coordinators. Would you look through them with me?  Oh, and first, have you heard of the Society for Humanity?
Umm, yes. They are an outgrowth of the Fundamentalists who have kept U. S. Robots from ever employing positronic robots on the grounds of unfair labor competition and so on. The Society for Humanity itself is anti-Machine, is it not?
Yes, yes, but  Well, you will see. Shall we begin? Well start with the Eastern Region.
As you say

The Eastern Region
1.       Area: 7,500,000 square miles
2.       Population: 1,700,000,000
3.       Capital: Shanghai

Ching Hso-lins great-grandfather had been killed in the Japanese invasion of the old Chinese Republic, and there had been no one beside his dutiful children to mourn his loss or even to know he was lost. Ching Hso-lins grandfather had survived the civil war of the late forties, but there had been no one beside his dutiful children to know or care of that.
And yet Ching Hso-lin was a Regional Vice-Co-ordinator, with the economic welfare of half the people of Earth in his care.
Perhaps it was with the thought of all that in mind, that Ching had two maps as the only ornaments on the wall of his office. One was an old hand-drawn affair tracing out an acre or two of land, and marked with the now outmoded pictographs of old China. A little creek trickled aslant the faded markings and there were the delicate pictorial indications of lowly huts, in one of which Chings grandfather had been born.
The other map was a huge one, sharply delineated, with all markings in neat Cyrillic characters. The red boundary that marked the Eastern Region swept within its grand confines all that had once been China, India, Burma, Indo-China, and Indonesia. On it, within the old province of Szechuan, so light and gentle that none could see it, was the little mark placed there by Ching which indicated the location of his ancestral farm.
Ching stood before these maps as he spoke to Stephen Byerley in precise English, No one knows better than you, Mr. Co-ordinator, that my job, to a large extent, is a sinecure. It carries with it a certain social standing, and I represent a convenient focal point for administration, but otherwise it is the Machine!  The Machine does all the work. What did you think, for instance, of the Tientsin Hydroponics works?
Tremendous! said Byerley.
It is but one of dozens, and not the largest. Shanghai, Calcutta, Batavia, Bangkok  They are widely spread and they are the answer to feeding the billion and three quarters of the East.
And yet, said Byerley, you have an unemployment problem there at Tientsin. Can you be over-producing? It is incongruous to think of Asia as suffering from too much food.
Chings dark eyes crinkled at the edges. No. It has not come to that yet. It is true that over the last few months, several vats at Tientsin have been shut down, but it is nothing serious. The men have been released only temporarily and those who do not care to work in other fields have been shipped to Colombo in Ceylon, where a new plant is being put into operation.
But why should the vats be closed down?
Ching smiled gently, You do not know much of hydroponics, I see. Well, that is not surprising. You are a Northerner, and there soil farming is still profitable. It is fashionable in the North to think of hydroponics, when it is thought of at all, as a device of growing turnips in a chemical solution, and so it is  in an infinitely complicated way.
In the first place, by far the largest crop we deal with (and the percentage is growing) is yeast. We have upward of two thousand strains of yeast in production and new strains are added monthly. The basic food-chemicals of the various yeasts are nitrates and phosphates among the inorganics together with proper amounts of the trace metals needed, down to the fractional parts per million of boron and molybdenum which are required. The organic matter is mostly sugar mixtures derived from the hydrolysis of cellulose, but, in addition, there are various food factors which must be added.
For a successful hydroponics industry  one which can feed seventeen hundred million people  we must engage in an immense reforestation program throughout the East; we must have huge wood-conversion plants to deal with our southern jungles; we must have power, and steel, and chemical synthetics above all.
Why the last, sir?
Because, Mr. Byerley, these strains of yeast have each their peculiar properties. We have developed, as I said, two thousand strains. The beefsteak you thought you ate today was yeast. The frozen fruit confection you had for dessert was iced yeast. We have filtered yeast juice with the taste, appearance, and all the food value of milk.
It is flavor, more than anything else, you see, that makes yeast feeding popular and for the sake of flavor we have developed artificial, domesticated strains that can no longer support themselves on a basic diet of salts and sugar. One needs biotin; another needs pteroylglutamic acid; still others need seventeen different amino acids supplied them as well as all the Vitamins B, but one (and yet it is popular and we cannot, with economic sense, abandon it) 
Byerley stirred in his seat, To what purpose do you tell me all this?
You asked me, sir, why men are out of work in Tientsin. I have a little more to explain. It is not only that we must have these various and varying foods for our yeast; but there remains the complicating factor of popular fads with passing time; and of the possibility of the development of new strains with the new requirements and new popularity. All this must be foreseen, and the Machine does the job
But not perfectly.
Not very imperfectly, in view of the complications I have mentioned. Well, then, a few thousand workers in Tientsin are temporarily out of a job. But, consider this, the amount of waste in this past year (waste that is, in terms of either defective supply or defective demand) amounts to not one-tenth of one percent of our total productive turnover. I consider that
Yet in the first years of the Machine, the figure was nearer one-thousandth of one percent.
Ah, but in the decade since the Machine began its operations in real earnest, we have made use of it to increase our old pre-Machine yeast industry twenty-fold. You expect imperfections to increase with complications, though
Though?
There was the curious instance of Rama Vrasayana.
What happened to him?
Vrasayana was in charge of a brine-evaporation plant for the production of iodine, with which yeast can do without, but human beings not. His plant was forced into receivership.
Really? And through what agency?
Competition, believe it or not. In general, one of the chiefest functions of the Machines analyses is to indicate the most efficient distribution of our producing units. It is obviously faulty to have areas insufficiently serviced, so that the transportation costs account for too great a percentage of the overhead. Similarly, it is faulty to have an area too well serviced, so that factories must be run at lowered capacities, or else compete harmfully with one another. In the case of Vrasayana, another plant was established in the same city, and with a more efficient extracting system.
The Machine permitted it?
Oh, certainly. That is not surprising. The new system is becoming widespread. The surprise is that the Machine failed to warn Vrasayana to renovate or combine.  Still, no matter. Vrasayana accepted a job as engineer in the new plant, and if his responsibility and pay are now less, he is not actually suffering. The workers found employment easily; the old plant has been converted to  something or other. Something useful. We left it all to the Machine.
And otherwise you have no complaints.
None!

The Tropic Region:
a.       Area: 22,000,000 square miles
b.       Population: 500,000,000
c.       Capital: Capital City

The map in Lincoln Ngomas office was far from the model of neat precision of the one in Chings Shanghai dominion. The boundaries of Ngomas Tropic Region were stenciled in dark, wide brown and swept about a gorgeous interior labeled jungle and desert and here be Elephants and all Manner of Strange Beasts.
It had much to sweep, for in land area the Tropic Region enclosed most of two continents: all of South America north of Argentina and all of Africa south of the Atlas. It included North America south of the Rio Grande as well, and even Arabia and Iran in Asia. It was the reverse of the Eastern Region. Where the ant hives of the Orient crowded half of humanity into 15 percent of the land mass, the Tropics stretched its 15 per cent of Humanity over nearly half of all the land in the world.
But it was growing. It was the one Region whose population increase through immigration exceeded that through births.  And for all who came it had use.
To Ngoma, Stephen Byerley seemed like one of these immigrants, a pale searcher for the creative work of carving a harsh environment into the softness necessary for man, and he felt some of that automatic contempt of the strong man born to the strong Tropics for the unfortunate pallards of the colder suns.
The Tropics had the newest capital city on Earth, and it was called simply that: Capital City, in the sublime confidence of youth. It spread brightly over the fertile uplands of Nigeria and outside Ngomas windows, far below, was life and color; the bright, bright sun and the quick, drenching showers. Even the squawking of the rainbowed birds was brisk and the stars were hard pinpoints in the sharp night.
Ngoma laughed. He was a big, dark man, strong faced and handsome.
Sure, he said, and his English was colloquial and mouth-filling, the Mexican Canal is overdue. What the hell? It will get finished just the same, old boy.
It was doing well up to the last half year.
Ngoma looked at Byerley and slowly crunched his teeth over the end of a big cigar, spitting out one end and lighting the other, Is this an official investigation, Byerley? Whats going on?
Nothing. Nothing at all. Its just my function as Coordinator to be curious.
Well, if its just that you are filling in a dull moment, the truth is that were always short on labor. Theres lots going on in the Tropics. The Canal is only one of them
But doesnt your Machine predict the amount of labor available for the Canal,  allowing for all the competing projects?
Ngoma placed one hand behind his neck and blew smoke rings at the ceiling, It was a little off.
Is it often a little off?
Not oftener than you would expect.  We dont expect too much of it, Byerley. We feed it data. We take its results. We do what it says.  But its just a convenience, just a laborsaving device. We could do without it, if we had to. Maybe not as well, maybe not as quickly, but wed get there.
Weve got confidence out here, Byerley, and thats the secret. Confidence! Weve got new land thats been waiting for us for thousands of years, while the rest of the world was being ripped apart in the lousy fumblings of pre-atomic time. We dont have to eat yeast like the Eastern boys, and we dont have to worry about the stale dregs of the last century like you Northerners.
Weve wiped out the tsetse fly and the Anopheles mosquito, and people find they can live in the sun and like it, now. Weve thinned down the jungles and found soil; weve watered the deserts and found gardens. Weve got coal and oil in untouched fields, and minerals out of count.
Just step back. Thats all we ask the rest of the world to do.  Step back, and let us work.
Byerley said, prosaically, But the Canal,  it was on schedule six months ago. What happened?
Ngoma spread his hands, Labor troubles. He felt through a pile of papers skeltered about his desk and gave it up.
Had something on the matter here, he muttered, but never mind. There was a work shortage somewhere in Mexico once on the question of women. There werent enough women in the neighborhood. It seemed no one had thought of feeding sexual data to the Machine.
He stopped to laugh, delightedly, then sobered, Wait a while. I think Ive got it.  Villafranca!
Villafranca?
Francisco Villafranca.  He was the engineer in charge. Now let me straighten it out. Something happened and there was a cave-in. Right. Right. That was it. Nobody died, as I remember, but it made a hell of a mess.  Quite a scandal.
Oh?
There was some mistake in his calculations.  Or at least, the Machine said so. They fed through Villafrancas data, assumptions, and so on. The stuff he had started with. The answers came out differently. It seems the answers Villafranca had used didnt take account of the effect of a heavy rainfall on the contours of the cut.  Or something like that. Im not an engineer, you understand.
Anyway, Villafranca put up a devil of a squawk. He claimed the Machines answer had been different the first time. That he had followed the Machine faithfully. Then he quit! We offered to hold him on  reasonable doubt, previous work satisfactory, and all that  in a subordinate position, of course  had to do that much  mistakes cant go unnoticed  bad for discipline  Where was I?
You offered to hold him on.
Oh yes. He refused.  Well, take all in all, were two months behind. Hell, thats nothing.
Byerley stretched out his hand and let the fingers tap lightly on the desk, Villafranca blamed the Machine, did he?
Well, he wasnt going to blame himself, was he? Lets face it; human nature is an old friend of ours. Besides, I remember something else now  Why the hell cant I find documents when I want them? My filing system isnt worth a damn  This Villafranca was a member of one of your Northern organizations. Mexico is too close to the North! thats part of the trouble.
Which organization are you speaking of?
The Society of Humanity, they call it. He used to attend the annual conference in New York, Villafranca did. Bunch of crackpots, but harmless.  They dont like the Machines; claim theyre destroying human initiative. So naturally Villafranca would blame the Machine.  Dont understand that group myself. Does Capital City look as if the human race were running out of initiative?
And Capital City stretched out in golden glory under a golden sun,  the newest and youngest creation of Homo metropolis.

The European Region
a.       Area: 4,000,000 square miles
b.       Population: 300,000,000
c.       Capital: Geneva

The European Region was an anomaly in several ways. In area, it was far the smallest, not one-fifth the size of the Tropic Region in area, and not one-fifth the size of the Eastern Region in population. Geographically, it was only somewhat similar to pre-Atomic Europe, since it excluded what had once been European Russia and what had once been the British Isles, while it included the Mediterranean coasts of Africa and Asia, and, in a queer jump across the Atlantic, Argentina, Chile, and Uruguay as well.
Nor was it likely to improve its relative status vis-à-vis the other regions of Earth, except for what vigor the South American provinces lent it. Of all the Regions, it alone showed a positive population decline over the past half century. It alone had not seriously expanded its productive facilities, or offered anything radically new to human culture.
Europe, said Madame Szegeczowska, in her soft French, is essentially an economic appendage of the Northern Region. We know it, and it doesnt matter.
And as though in resigned acceptance of a lack of individuality, there was no map of Europe on the wall of the Madame Co-ordinators office.
And yet, pointed out Byerley, you have a Machine of your own, and you are certainly under no economic pressure from across the ocean.
A Machine! Bah! She shrugged her delicate shoulders, and allowed a thin smile to cross her little face as she tamped out a cigarette with long fingers. Europe is a sleepy place. And such of our men as do not manage to immigrate to the Tropics are tired and sleepy along with it. You see for yourself that it is myself, a poor woman, to whom falls the task of being Vice-Co-ordinator. Well, fortunately, it is not a difficult job, and not much is expected of me.
As for the Machine  What can it say but Do this and it will be best for you. But what is best for us? Why, to be an economic appendage of the Northern Region.
And is it so terrible? No wars! We live in peace  and it is pleasant after seven thousand years of war. We are old, monsieur. In our borders, we have the regions where Occidental civilization was cradled. We have Egypt and Mesopotamia; Crete and Syria; Asia Minor and Greece.  But old age is not necessarily an unhappy time. It can be a fruition
Perhaps you are right, said Byerley, affably. At least the tempo of life is not as intense as in the other Regions. It is a pleasant atmosphere.
Is it not?  Tea is being brought, monsieur. If you will indicate your cream and sugar preference, please. Thank you.
She sipped gently, then continued, It is pleasant. The rest of Earth is welcome to the continuing struggle. I find a parallel here, a very interesting one. There was a time when Rome was master of the world. It had adopted the culture and civilization of Greece, a Greece which had never been united, which had ruined itself with war, and which was ending in a state of decadent squalor. Rome united it, brought it peace and let it live a life of secure non-glory. It occupied itself with its philosophies and its art, far from the clash of growth and war. It was a sort of death, but it was restful, and it lasted with minor breaks for some four hundred years.
And yet, said Byerley, Rome fell eventually, and the opium dream was over.
There are no longer barbarians to overthrow civilization.
We can be our own barbarians. Madame Szegeczowska.  Oh, I meant to ask you. The Almaden mercury mines have fallen off quite badly in production. Surely the ores are not declining more rapidly than anticipated?
The little womans gray eyes fastened shrewdly on Byerley, Barbarians  the fall of civilization  possible failure of the Machine. Your thought processes are very transparent, monsieur.
Are they? Byerley smiled. I see that I should have had men to deal with as hitherto.  You consider the Almaden affair to be the fault of the Machine?
Not at all, but I think you do. You, yourself, are a native of the Northern Region. The Central Co-ordination Office is at New York.  And I have noticed for quite a while that you Northerners lack somewhat of faith in the Machine.
We do?
There is your Society for Humanity which is strong in the North, but naturally fails to find many recruits in tired, old Europe, which is quite willing to let feeble Humanity alone for a while. Surely, you are one of the confident North and not one of the cynical old continent.
This has a connection with Almaden?
Oh, yes, I think so. The mines are in the control of Consolidated Cinnabar, which is certainly a Northern company, with headquarters at Nikolaev. Personally, I wonder if the Board of Directors have been consulting the Machine at all. They said they had in our conference last month, and, of course, we have no evidence that they did not, but I wouldnt take the word of a Northerner in this matter  no offense intended  under any circumstances.  Nevertheless, I think it will have a fortunate ending.
In what way, my dear madam?
You must understand that the economic irregularities of the last few months, which, although small as compared with the great storms of the past, are quite disturbing to our peace-drenched spirits, have caused considerable restiveness in the Spanish province. I understand that Consolidated Cinnabar is selling out to a group of native Spaniards. It is consoling. If we are economic vassals of the North, it is humiliating to have the fact advertised too blatantly.  And our people can be better trusted to follow the Machine.
Then you think there will be no more trouble?
I am sure there will not be  In Almaden, at least.

The Northern Region
a.       Area: 18,000,000 square miles
b.       Population: 800,000,000
c.       Capital: Ottawa

The Northern Region, in more ways than one, was at the top. This was exemplified quite well by the map in the Ottawa office of Vice-Co-ordinator Hiram Mackenzie, in which the North Pole was centered. Except for the enclave of Europe with its Scandinavian and Icelandic regions, all the Arctic area was within the Northern Region.
Roughly, it could be divided into two major areas. To the left on the map was all of North America above the Rio Grande. To the right was included all of what had once been the Soviet Union. Together these areas represented the centered power of the planet in the first years of the Atomic Age. Between the two was Great Britain, a tongue of the Region licking at Europe. Up at the top of the map, distorted into odd, huge shapes, were Australia and New Zealand, also member provinces of the Region.
Not all the changes of the past decades had yet altered the fact that the North was the economic ruler of the planet.
There was almost an ostentatious symbolism thereof in the fact that of the official Regional maps Byerley had seen, Mackenzies alone showed all the Earth, as though the North feared no competition and needed no favoritism to point up its pre-eminence.
Impossible, said Mackenzie, dourly, over the whiskey. Mr. Byerley, you have had no training as a robot technician, I believe.
No, I have not.
Hmp. Well, it is, in my opinion, a sad thing that Ching, Ngoma and Szegeczowska havent either. There is too prevalent an opinion among the peoples of Earth that a Co-ordinator need only be a capable organizer, a broad generalizer, and an amiable person. These days he should know his robotics as well, no offense intended.
None taken. I agree with you.
I take it, for instance, from what you have said already, that you worry about the recent trifling dislocation in world economy. I dont know what you suspect, but it has happened in the past that people  who should have known better  wondered what would happen if false data were fed into the Machine.
And what would happen, Mr. Mackenzie?
Well, the Scotsman shifted his weight and sighed, all collected data goes through a complicated screening system which involves both human and mechanical checking, so that the problem is not likely to arise.  But let us ignore that. Humans are fallible, also corruptible, and ordinary mechanical devices are liable to mechanical failure.
The real point of the matter is that what we call a wrong datum is one which is inconsistent with all other known data. It is our only criterion of right and wrong. It is the Machines as well. Order it for instance, to direct agricultural activity on the basis of an average July temperature in Iowa of 57 degrees Fahrenheit. It wont accept that. It will not give an answer.  Not that it has any prejudice against that particular temperature, or that an answer is impossible; but because, in the light of all the other data fed it over a period of years, it knows that the probability of an average July temperature of 57 is virtually nil. It rejects that datum.
The only way a wrong datum can be forced on the Machine is to include it as part of a self-consistent whole, all of which is subtly wrong in a manner either too delicate for the Machine to detect or outside the Machines experience. The former is beyond human capacity, and the latter is almost so, and is becoming more nearly so as the Machines experience increases by the second.
Stephen Byerley placed two fingers to the bridge of his nose, Then the Machine cannot be tampered with  And how do you account for recent errors, then?
My dear Byerley, I see that you instinctively follow that great error  that the Machine knows all. Let me cite you a case from my personal experience. The cotton industry engages experienced buyers who purchase cotton. Their procedure is to pull a tuft of cotton out of a random bale of a lot. They will look at that tuft and feel it, tease it out, listen to the crackling perhaps as they do so, touch it with their tongue, and through this procedure they will determine the class of cotton the bales represent. There are about a dozen such classes. As a result of their decisions, purchases are made at certain prices; blends are made in certain proportions.  Now these buyers cannot yet be replaced by the Machine.
Why not? Surely the data involved is not too complicated for it?
Probably not. But what data is this you refer to? No textile chemist knows exactly what it is that the buyer tests when he feels a tuft of cotton. Presumably theres the average length of the threads, their feel, the extent and nature of their slickness, the way they hang together, and so on.  Several dozen items, subconsciously weighed, out of years of experience. But the quantitative nature of these tests is not known; maybe even the very nature of some of them is not known. So we have nothing to feed the Machine. Nor can the buyers explain their own judgment. They can only say, Well, look at it. Cant you tell its class-such-and-such? 
I see.
There are innumerable cases like that. The Machine is only a tool after all, which can help humanity progress faster by taking some of the burdens of calculations and interpretations off its back. The task of the human brain remains what it has always been, that of discovering new data to be analyzed, and of devising new concepts to be tested. A pity the Society for Humanity wont understand that.
They are against the Machine?
They would be against mathematics or against the art of writing if they had lived at the appropriate time. These reactionaries of the Society claim the Machine robs man of his soul. I notice that capable men are still at a premium in our society; we still need the man who is intelligent enough to think of the proper questions to ask. Perhaps if we could find enough of such, these dislocations you worry about, Coordinator, wouldnt occur.

Earth (Including the uninhabited continent, Antarctica)
a.       Area: 54,000,000 square miles (land surface)
b.       Population: 3,300,000,000
c.       Capital: New York

The fire behind the quartz was weary now, and sputtered its reluctant way to death.
The Co-ordinator was somber, his mood matching the sinking flame.
They all minimize the state of affairs. His voice was low. Is it not easy to imagine that they all laugh at me? And yet Vincent Silver said the Machines cannot be out of order, and I must believe him. Hiram Mackenzie says they cannot be fed false data, and I must believe him. But the Machines are going wrong, somehow, and I must believe that, too; and so there is still an alternative left.
He glanced sidewise at Susan Calvin, who, with closed eyes, for a moment seemed asleep.
What is that? she asked, prompt to her cue, nevertheless.
Why, that correct data is indeed given, and correct answers are indeed received, but that they are then ignored. There is no way the Machine can enforce obedience to its dictates.
Madame Szegeczowska hinted as much, with reference to Northerners in general, it seems to me.
So she did.
And what purpose is served by disobeying the Machine? Lets consider motivations.
Its obvious to me, and should be to you. It is a matter of rocking the boat, deliberately. There can be no serious conflicts on Earth, in which one group or another can seize more power than it has for what it thinks is its own good despite the harm to Mankind as a whole, while the Machines rule. If popular faith in the Machines can be destroyed to the point where they are abandoned, it will be the law of the jungle again.  And not one of the four Regions can be freed of the suspicion of wanting just that.
The East has half of humanity within its borders, and the Tropics more than half of Earths resources. Each can feel itself the natural rulers of all Earth, and each has a history of humiliation by the North, for which it can be human enough to wish a senseless revenge. Europe has a tradition of greatness, on the other hand. It once did rule the Earth, and there is nothing so eternally adhesive as the memory of power.
Yet, in another way, its hard to believe. Both the East and the Tropics are in a state of enormous expansion within their own borders. Both are climbing incredibly. They cannot have the spare energy for military adventures. And Europe can have nothing but its dreams. It is a cipher, militarily.
So, Stephen, said Susan, you leave the North.
Yes, said Byerley, energetically, I do. The North is now the strongest, and has been for nearly a century, or its component parts have been. But it is losing relatively, now. The Tropic Regions may take their place in the forefront of civilization for the first time since the Pharaohs, and there are Northerners who fear that.
The Society for Humanity is a Northern organization, primarily, you know, and they make no secret of not wanting the Machines.  Susan, they are few in numbers, but it is an association of powerful men. Heads of factories; directors of industries and agricultural combines who hate to be what they call the Machines office-boy belong to it. Men with ambition belong to it. Men who feel themselves strong enough to decide for themselves what is best for themselves, and not just to be told what is best for others.
In short, just those men who, by together refusing to accept the decisions of the Machine, can, in a short time, turn the world topsy-turvy; just those belong to the Society.
Susan, it hangs together. Five of the Directors of World Steel are members, and World Steel suffers from overproduction. Consolidated Cinnabar, which mined mercury at Almaden, was a Northern concern. Its books are still being investigated, but one, at least, of the men concerned was a member. Francisco Villafranca, who, single-handed, delayed the Mexican Canal for two months, was a member, we know already  and so was Rama Vrasayana, I was not at all surprised to find out.
Susan said, quietly, These men, I might point out, have all done badly
But naturally, interjected Byerley. To disobey the Machines analyses is to follow a non-optimal path. Results are poorer than they might be. Its the price they pay. They will have it rough now but in the confusion that will eventually follow
Just what do you plan doing, Stephen?
There is obviously no time to lose. I am going to have the Society outlawed, every member removed from any responsible post. And all executive and technical positions, henceforward, can be filled only by applicants signing a non-Society oath. It will mean a certain surrender of basic civil liberties, but I am sure the Congress
It wont work!
What!  Why not?
I will make a prediction. If you try any such thing, you will find yourself hampered at every turn. You will find it impossible to carry out. You will find your every move in that direction will result in trouble.
Byerley was taken aback, Why do you say that? I was rather hoping for your approval in this matter.
You cant have it as long as your actions are based on a false premise. You admit the Machine cant be wrong, and cant be fed wrong data. I will now show you that it cannot be disobeyed, either, as you think is being done by the Society.
That I dont see at all.
Then listen. Every action by any executive which does not follow the exact directions of the Machine he is working with becomes part of the data for the next problem. The Machine, therefore, knows that the executive has a certain tendency to disobey. He can incorporate that tendency into that data,  even quantitatively, that is, judging exactly how much and in what direction disobedience would occur. Its next answers would be just sufficiently biased so that after the executive concerned disobeyed, he would have automatically corrected those answers to optimal directions. The Machine knows, Stephen!
You cant be sure of all this. You are guessing.
It is a guess based on a lifetimes experience with robots. You had better rely on such a guess, Stephen.
But then what is left? The Machines themselves are correct and the premises they work on are correct. That we have agreed upon. Now you say that it cannot be disobeyed. Then what is wrong?
You have answered yourself. Nothing is wrong! Think about the Machines for a while, Stephen. They are robots, and they follow the First Law. But the Machines work not for any single human being, but for all humanity, so that the First Law becomes: No Machine may harm humanity; or, through inaction, allow humanity to come to harm.
Very well, then, Stephen, what harms humanity? Economic dislocations most of all, from whatever cause. Wouldnt you say so?
I would.
And what is most likely in the future to cause economic dislocations? Answer that, Stephen.
I should say, replied Byerley, unwillingly, the destruction of the Machines.
And so should I say, and so should the Machines say. Their first care, therefore, is to preserve themselves, for us. And so they are quietly taking care of the only elements left that threaten them. It is not the Society for Humanity which is shaking the boat so that the Machines may be destroyed. You have been looking at the reverse of the picture. Say rather that the Machine is shaking the boatvery slightlyjust enough to shake loose those few which cling to the side for purposes the Machines consider harmful to Humanity.
So Vrasayana loses his factory and gets another job where he can do no harm  he is not badly hurt, he is not rendered incapable of earning a living, for the Machine cannot harm a human being more than minimally, and that only to save a greater number. Consolidated Cinnabar loses control at Almaden. Villafranca is no longer a civil engineer in charge of an important project. And the directors of World Steel are losing their grip on the industry  or will.
But you dont really know all this, insisted Byerley, distractedly. How can we possibly take a chance on your being right?
You must. Do you remember the Machines own statement when you presented the problem to him? It was: The matter admits of no explanation. The Machine did not say there was no explanation, or that it could determine no explanation. It simply was not going to admit any explanation. In other words, it would be harmful to humanity to have the explanation known, and thats why we can only guess  and keep on guessing.
But how can the explanation do us harm? Assume that you are right, Susan.
Why, Stephen, if I am right, it means that the Machine is conducting our future for us not only simply in direct answer to our direct questions, but in general answer to the world situation and to human psychology as a whole. And to know that may make us unhappy and may hurt our pride. The Machine cannot, must not, make us unhappy.
Stephen, how do we know what the ultimate good of Humanity will entail? We havent at our disposal the infinite factors that the Machine has at its! Perhaps, to give you a not unfamiliar example, our entire technical civilization has created more unhappiness and misery than it has removed. Perhaps an agrarian or pastoral civilization, with less culture and less people would be better. If so, the Machines must move in that direction, preferably without telling us, since in our ignorant prejudices we only know that what we are used to, is good  and we would then fight change. Or perhaps a complete urbanization, or a completely caste-ridden society, or complete anarchy, is the answer. We dont know. Only the Machines know, and they are going there and taking us with them.
But you are telling me, Susan, that the Society for Humanity is right; and that Mankind has lost its own say in its future.
It never had any, really. It was always at the mercy of economic and sociological forces it did not understand  at the whims of climate, and the fortunes of war. Now the Machines understand them; and no one can stop them, since the Machines will deal with them as they are dealing with the Society,  having, as they do, the greatest of weapons at their disposal, the absolute control of our economy.
How horrible!
Perhaps how wonderful! Think, that for all time, all conflicts are finally evitable. Only the Machines, from now on, are inevitable!
And the fire behind the quartz went out and only a curl of smoke was left to indicate its place.
Feminine Intuition

For the first time in the history of United States Robots and Mechanical Men Corporation, a robot had been destroyed through accident on Earth itself.
No one was to blame. The air vehicle had been demolished in mid-air and an unbelieving investigating committee was wondering whether they really dared announce the evidence that it had been hit by a meteorite. Nothing else could have been fast enough to prevent automatic avoidance; nothing else could have done the damage short of a nuclear blast and that was out of the question.
Tie that in with a report of a flash in the night sky just before the vehicle had explodedand from Flagstaff Observatory, not from an amateurand the location of a sizable and distinctly meteoric bit of iron freshly gouged into the ground a mile from the site and what other conclusion could be arrived at?
Still, nothing like that had ever happened before and calculations of the odds against it yielded monstrous figures. Yet even colossal improbabilities can happen sometimes.
At the offices of United States Robots, the hows and whys of it were secondary. The real point was that a robot had been destroyed.
That, in itself, was distressing.
The fact that JN-5 had been a prototype, the first, after four earlier attempts, to have been placed in the field, was even more distressing.
The fact that JN-5 was a radically new type of robot, quite different from anything ever built before, was abysmally distressing.
The fact that JN-5 had apparently accomplished something before its destruction that was incalculably important and that that accomplishment might now be forever gone, placed the distress utterly beyond words.
It seemed scarcely worth mentioning that, along with the robot, the Chief Robopsychologist of United States Robots had also died.
Clinton Madarian had joined the firm ten years before. For five of those years, he had worked uncomplainingly under the grumpy supervision of Susan Calvin.
Madarians brilliance was quite obvious and Susan Calvin had quietly promoted him over the heads of older men. She wouldnt, in any case, have deigned to give her reasons for this to Research Director Peter Bogert, but as it happened, no reasons were needed. Or, rather, they were obvious.
Madarian was utterly the reverse of the renowned Dr. Calvin in several very noticeable ways. He was not quite as overweight as his distinct double chin made him appear to be, but even so he was overpowering in his presence, where Susan had gone nearly unnoticed. Madarians massive face, his shock of glistening red-brown hair, his ruddy complexion and booming voice, his loud laugh, and most of all, his irrepressible self-confidence and his eager way of announcing his successes, made everyone else in the room feel there was a shortage of space.
When Susan Calvin finally retired (refusing, in advance, any cooperation with respect to any testimonial dinner that might be planned in her honor, with so firm a manner that no announcement of the retirement was even made to the news services) Madarian took her place.
He had been in his new post exactly one day when he initiated the JN project.
It had meant the largest commitment of funds to one project that United States Robots had ever had to weigh, but that was something which Madarian dismissed with a genial wave of the hand.
Worth every penny of it, Peter, he said. And I expect you to convince the Board of Directors of that.
Give me reasons, said Bogert, wondering if Madarian would. Susan Calvin had never given reasons.
But Madarian said, Sure, and settled himself easily into the large armchair in the Directors office.
Bogert watched the other with something that was almost awe. His own once-black hair was almost white now and within the decade he would follow Susan into retirement. That would mean the end of the original team that had built United States Robots into a globe-girdling firm that was a rival of the national governments in complexity and importance. Somehow neither he nor those who had gone before him ever quite grasped the enormous expansion of the firm.
But this was a new generation. The new men were at ease with the Colossus They lacked the touch of wonder that would have them tiptoeing in disbelief. So they moved ahead, and that was good.
Madarian said, I propose to begin the construction of robots without constraint.
Without the Three Laws? Surely
No, Peter. Are those the only constraints you can think of? Hell, you contributed to the design of the early positronic brains. Do I have to tell you that, quite aside from the Three Laws, there isnt a pathway in those brains that isnt carefully designed and fixed? We have robots planned for specific tasks, implanted with specific abilities.
And you propose
That at every level below the Three Laws, the paths be made open-ended. Its not difficult.
Bogert said dryly, Its not difficult, indeed. Useless things are never difficult. The difficult thing is fixing the paths and making the robot useful.
But why is that difficult? Fixing the paths requires a great deal of effort because the Principle of Uncertainty is important in particles the mass of positrons and the uncertainty effect must be minimized. Yet why must it? If we arrange to have the Principle just sufficiently prominent to allow the crossing of paths unpredictably
We have an unpredictable robot.
We have a creative robot, said Madarian, with a trace of impatience. Peter, if theres anything a human brain has that a robotic brain has never had, its the trace of unpredictability that comes from the effects of uncertainty at the subatomic level. I admit that this effect has never been demonstrated experimentally within the nervous system, but without that the human brain is not superior to the robotic brain in principle.
And you think that if you introduce the effect into the robotic brain, the human brain will become not superior to the robotic brain in principle.
That,  said Madarian, is exactly what I believe. They went on for a long time after that.
The Board of Directors clearly had no intention of being easily convinced.
Scott Robertson, the largest shareholder in the firm, said, Its hard enough to manage the robot industry as it is, with public hostility to robots forever on the verge of breaking out into the open. If the public gets the idea that robots will be uncontrolled...Oh, dont tell me about the Three Laws. The average man wont believe the Three Laws will protect him if he as much as hears the word uncontrolled. 
Then dont use it,  said Madarian. Call the robotcall it intuitive. 
An intuitive robot,  someone muttered. A girl robot? A smile made its way about the conference table.
Madarian seized on that. All right. A girl robot. Our robots are sexless, of course, and so will this one be, but we always act as though theyre males. We give them male pet names and call them he and him. Now this one, if we consider the nature of the mathematical structuring of the brain which I have proposed, would fall into the JN-coordinate system. The first robot would be JN-1, and Ive assumed that it would be called John-10....Im afraid that is the level of originality of the average roboticist. But why not call it Jane-1, damn it? If the public has to be let in on what were doing, were constructing a feminine robot with intuition.
Robertson shook his head, What difference would that make? What youre saying is that you plan to remove the last barrier which, in principle, keeps the robotic brain inferior to the human brain. What do you suppose the public reaction will be to that?
Do you plan to make that public? said Madarian. He thought a bit and then said, Look. One thing the general public believes is that women are not as intelligent as men.
There was an instant apprehensive look on the face of more than one man at the table and a quick look up and down as though Susan Calvin were still in her accustomed seat.
Madarian said, If we announce a female robot, it doesnt matter what she is. The public will automatically assume she is mentally backward. We just publicize the robot as Jane-1 and we dont have to say another word. Were safe.
Actually, said Peter Bogert quietly, theres more to it than that. Madarian and I have gone over the mathematics carefully and the JN series, whether John or Jane, would be quite safe. They would be less complex and intellectually capable, in an orthodox sense, than many another series we have designed and constructed. There would only be the one added factor of, well, lets get into the habit of calling it intuition. 
Who knows what it would do? muttered Robertson.
Madarian has suggested one thing it can do. As you all know, the Space Jump has been developed in principle. It is possible for men to attain what is, in effect, hyper-speeds beyond that of light and to visit other stellar systems and return in negligible timeweeks at the most.
Robertson said, Thats not new to us. It couldnt have been done without robots.
Exactly, and its not doing us any good because we cant use the hyper-speed drive except perhaps once as a demonstration, so that U. S. Robots gets little credit. The Space Jump is risky, its fearfully prodigal of energy and therefore its enormously expensive. If we were going to use it anyway, it would be nice if we could report the existence of a habitable planet. Call it a psychological need. Spend about twenty billion dollars on a single Space Jump and report nothing but scientific data and the public wants to know why their money was wasted. Report the existence of a habitable planet, and youre an interstellar Columbus and no one will worry about the money.
So?
So where are we going to find a habitable planet? Or put it this waywhich star within reach of the Space Jump as presently developed, which of the three hundred thousand stars and star systems within three hundred light-years has the best chance of having a habitable planet? Weve got an enormous quantity of details on every star in our three-hundred-light-year neighborhood and a notion that almost every one has a planetary system. But which has a habitable planet? Which do we visit?...We dont know.
One of the directors said, How would this Jane robot help us?
Madarian was about to answer that, but he gestured slightly to Bogert and Bogert understood. The Director would carry more weight. Bogert didnt particularly like the idea; if the JN series proved a fiasco, he was making himself prominent enough in connection with it to insure that the sticky fingers of blame would cling to him. On the other hand, retirement was not all that far off, and if it worked, he would go out in a blaze of glory. Maybe it was only Madarians aura of confidence, but Bogert had honestly come to believe it would work.
He said, It may well be that somewhere in the libraries of data we have on those stars, there are methods for estimating the probabilities of the presence of Earth-type habitable planets. All we need to do is understand the data properly, look at them in the appropriate creative manner, make the correct correlations. We havent done it yet. Or if some astronomer has, he hasnt been smart enough to realize what he has.
A JN-type robot could make correlations far more rapidly and far more precisely than a man could. In a day, it would make and discard as many correlations as a man could in ten years. Furthermore, it would work in truly random fashion, whereas a man would have a strong bias based on preconception and on what is already believed.
There was a considerable silence after that Finally Robertson said, But its only a matter of probability, isnt it? Suppose this robot said, The highest-probability habitable-planet star within so-and-so light-years is Squidgee-17 or whatever, and we go there and find that a probability is only a probability and that there are no habitable planets after all. Where does that leave us?
Madarian struck in this time. We still win. We know how the robot came to the conclusion because itshewill tell us. It might well help us gain enormous insight into astronomical detail and make the whole thing worthwhile even if we dont make the Space Jump at all. Besides, we can then work out the five most probable sites of planets and the probability that one of the five has a habitable planet may then be better than 0.95. It would be almost sure
They went on for a long time after that.
The funds granted were quite insufficient, but Madarian counted on the habit of throwing good money after bad. With two hundred million about to be lost irrevocably when another hundred million could save everything, the other hundred million would surely be voted.
Jane-1 was finally built and put on display. Peter Bogert studied it hergravely. He said, Why the narrow waist? Surely that introduces a mechanical weakness?
Madarian chuckled. Listen, if were going to call her Jane, theres no point in making her look like Tarzan.
Bogert shook his head. Dont like it. Youll be bulging her higher up to give the appearance of breasts next, and thats a rotten idea. If women start getting the notion that robots may look like women, I can tell you exactly the kind of perverse notions theyll get, and youll really have hostility on their part.
Madarian said, Maybe youre right at that. No woman wants to feel replaceable by something with none of her faults. Okay.
Jane-2 did not have the pinched waist. She was a somber robot which rarely moved and even more rarely spoke.
Madarian had only occasionally come rushing to Bogert with items of news during her construction and that had been a sure sign that things were going poorly. Madarians ebullience under success was overpowering. He would not hesitate to invade Bogerts bedroom at 3 A.M. with a hot-flash item rather than wait for the morning. Bogert was sure of that.
Now Madarian seemed subdued, his usually florid expression nearly pale, his round cheeks somehow pinched. Bogert said, with a feeling of certainty, She wont talk.
Oh, she talks. Madarian sat down heavily and chewed at his lower lip. Sometimes, anyway, he said.
Bogert rose and circled the robot. And when she talks, she makes no sense, I suppose. Well, if she doesnt talk, shes no female, is she?
Madarian tried a weak smile for size and abandoned it. He said, The brain, in isolation, checked out.
I know, said Bogert. But once that brain was put in charge of the physical apparatus of the robot, it was necessarily modified, of course.
Of course, agreed Bogert unhelpfully. But unpredictably and frustratingly. The trouble is that when youre dealing with n-dimensional calculus of uncertainty, things are
Uncertain? said Bogert. His own reaction was surprising him. The company investment was already most sizable and almost two years had elapsed, yet the results were, to put it politely, disappointing. Still, he found himself jabbing at Madarian and finding himself amused in the process.
Almost furtively, Bogert wondered if it werent the absent Susan Calvin he was jabbing at. Madarian was so much more ebullient and effusive than Susan could ever possibly bewhen things were going well. He was also far more vulnerably in the dumps when things werent going well, and it was precisely under pressure that Susan never cracked. The target that Madarian made could be a neatly punctured bulls-eye as recompense for the target Susan had never allowed herself to be.
Madarian did not react to Bogerts last remark any more than Susan Calvin would have done; not out of contempt, which would have been Susans reaction, but because he did not hear it
He said argumentatively, The trouble is the matter of recognition. We have Jane-2 correlating magnificently. She can correlate on any subject, but once shes done so, she cant recognize a valuable result from a valueless one. Its not an easy problem, judging how to program a robot to tell a significant correlation when you dont know what correlations she will be making.
I presume youve thought of lowering the potential at the W-21 diode junction and sparking across the
No, no, no, no Madarian faded off into a whispering diminuendo. You cant just have it spew out everything. We can do that for ourselves. The point is to have it recognize the crucial correlation and draw the conclusion. Once that is done, you see, a Jane robot would snap out an answer by intuition. It would be something we couldnt get ourselves except by the oddest kind of luck.
It seems to me, said Bogert dryly, that if you had a robot like that, you would have her do routinely what, among human beings, only the occasional genius is capable of doing.
Madarian nodded vigorously. Exactly, Peter. Id have said so myself if I werent afraid of frightening off the execs. Please dont repeat that in their hearing.
Do you really want a robot genius?
What are words? Im trying to get a robot with the capacity to make random correlations at enormous speeds, together with a key-significance high-recognition quotient. And Im trying to put those words into positronic field equations. I thought I had it, too, but I dont. Not yet.
He looked at Jane-2 discontentedly and said, Whats the best significance you have, Jane?
Jane-2s head turned to look at Madarian but she made no sound, and Madarian whispered with resignation, Shes running that into the correlation banks.
Jane-2 spoke tonelessly at last. Im not sure. It was the first sound she had made.
Madarians eyes rolled upward. Shes doing the equivalent of setting up equations with indeterminate solutions.
I gathered that, said Bogert. Listen, Madarian, can you go anywhere at this point, or do we pull out now and cut our losses at half a billion?
Oh, Ill get it,  muttered Madarian.
Jane-3 wasnt it. She was never as much as activated and Madarian was in a rage.
It was human error. His own fault, if one wanted to be entirely accurate. Yet though Madarian was utterly humiliated, others remained quiet. Let he who has never made an error in the fearsomely intricate mathematics of the positronic brain fill out the first memo of correction.
Nearly a year passed before Jane-4 was ready. Madarian was ebullient again. She does it, he said. Shes got a good high-recognition quotient.
He was confident enough to place her on display before the Board and have her solve problems. Not mathematical problems; any robot could do that; but problems where the terms were deliberately misleading without being actually inaccurate.
Bogert said afterward, That doesnt take much, really.
Of course not. Its elementary for Jane-4 but I had to show them something, didnt I?
Do you know how much weve spent so far?
Come on, Peter, dont give me that. Do you know how much weve got back? These things dont go on in a vacuum, you know. Ive had over three years of hell over this, if you want to know, but Ive worked out new techniques of calculation that will save us a minimum of fifty thousand dollars on every new type of positronic brain we design, from now on in forever. Right?
Well
Well me no wells. Its so. And its my personal feeling that n-dimensional calculus of uncertainty can have any number of other applications if we have the ingenuity to find them, and my Jane robots will find them. Once Ive got exactly what I want, the new JN series will pay for itself inside of five years, even if we triple what weve invested so far.
What do you mean by exactly what you want? Whats wrong with Jane-4?
Nothing. Or nothing much. Shes on the track, but she can be improved and I intend to do so. I thought I knew where I was going when I designed her. Now Ive tested her and I know where Im going. I intend to get there.

Jane-5 was it. It took Madarian well over a year to produce her and there he had no reservations; he was utterly confident.
Jane-5 was shorter than the average robot, slimmer. Without being a female caricature as Jane-1 had been, she managed to possess an air of femininity about herself despite the absence of a single clearly feminine feature.
Its the way shes standing, said Bogert. Her arms were held gracefully and somehow the torso managed to give the impression of curving slightly when she turned.
Madarian said, Listen to her....How do you feel, Jane?
In excellent health, thank you, said Jane-5, and the voice was precisely that of a woman; it was a sweet and almost disturbing contralto.
Why did you do that, Clinton? said Peter, startled and beginning to frown.
Psychologically important, said Madarian. I want people to think of her as a woman; to treat her as a woman; to explain.
What people? Madarian put his hands in his pockets and stared thoughtfully at Bogert. I would like to have arrangements made for Jane and myself to go to flagstaff.
Bogert couldnt help but note that Madarian didnt say Jane-5. He made use of no number this time. She was the Jane. He said doubtfully, To flagstaff? Why?
Because thats the world center for general planetology, isnt it? Its where theyre studying the stars and trying to calculate the probability of habitable planets, isnt it?
I know that, but its on Earth.
Well, and I surely know that.
Robotic movements on Earth are strictly controlled. And theres no need for it. Bring a library of books on general planetology here and let Jane absorb them.
No! Peter, will you get it through your head that Jane isnt the ordinary logical robot; shes intuitive.
So?
So how can we tell what she needs, what she can use, what will set her off? We can use any metal model in the factory to read books; thats frozen data and out of date besides. Jane must have living information; she must have tones of voice, she must have side issues; she must have total irrelevancies even. How the devil do we know what or when something will go click-click inside her and fall into a pattern? If we knew, we wouldnt need her at all, would we?
Bogert began to feel harassed. He said, Then bring the men here, the general planetologists.
Here wont be any good. Theyll be out of their element. They wont react naturally. I want Jane to watch them at work; I want her to see their instruments, their offices, their desks, everything about them that she can. I want you to arrange to have her transported to flagstaff. And Id really like not to discuss it any further.
For a moment he almost sounded like Susan. Bogert winced, and said, Its complicated making such an arrangement. Transporting an experimental robot
Jane isnt experimental. Shes the fifth of the series.
The other four werent really working models.
Madarian lifted his hands in helpless frustration. Whos forcing you to tell the government that?
Im not worried about the government. It can be made to understand special cases. Its public opinion. Weve come a long way in fifty years and I dont propose to be set back twenty-five of them by having you lose control of a
I wont lose control. Youre making foolish remarks. Look! U. S. Robots can afford a private plane. We can land quietly at the nearest commercial airport and be lost in hundreds of similar landings. We can arrange to have a large ground car with an enclosed body meet us and take us to Flagstaff. Jane will be crated and it will be obvious that some piece of thoroughly non-robotic equipment is being transported to the labs. We wont get a second look from anyone. The men at Flagstaff will be alerted and will be told the exact purpose of the visit. They will have every motive to cooperate and to prevent a leak.
Bogert pondered. The risky part will be the plane and the ground car. If anything happens to the crate
Nothing will.
We might get away with it if Jane is deactivated during transport. Then even if someone finds out shes inside
No, Peter. That cant be done. Uh-uh. Not Jane-5. Look, shes been free-associating since she was activated. The information she possesses can be put into freeze during deactivation but the free associations never. No, sir, she cant ever be deactivated.
But, then, if somehow it is discovered that we are transporting an activated robot
It wont be found out. Madarian remained firm and the plane eventually took off. It was a late-model automatic Computo-jet, but it carried a human pilotone of U. S. Robots own employeesas backup. The crate containing Jane arrived at the airport safely, was transferred to the ground car, and reached the Research Laboratories at Flagstaff without incident.
Peter Bogert received his first call from Madarian not more than an hour after the latters arrival at Flagstaff. Madarian was ecstatic and, characteristically, could not wait to report.
The message arrived by tubed laser beam, shielded, scrambled, and ordinarily impenetrable, but Bogert felt exasperated. He knew it could be penetrated if someone with enough technological abilitythe government, for examplewas determined to do so. The only real safety lay in the fact that the government had no reason to try. At least Bogert hoped so.
He said, For Gods sake, do you have to call?
Madarian ignored him entirely. He burbled, It was an inspiration. Sheer genius, I tell you.
For a while, Bogert stared at the receiver. Then he shouted incredulously, You mean youve got the answer? Already?
No, no! Give us time, damn it. I mean the matter of her voice was an inspiration. Listen, after we were chauffeured from the airport to the main administration building at Flagstaff, we uncrated Jane and she stepped out of the box. When that happened, every man in the place stepped back. Scared! Nitwits! If even scientists cant understand the significance of the Laws of Robotics, what can we expect of the average untrained individual? For a minute there I thought: This will all be useless. They wont talk. Theyll be keying themselves for a quick break in case she goes berserk and theyll be able to think of nothing else.
Well, then, what are you getting at?
So then she greeted them routinely. She said, Good afternoon, gentlemen. I am so glad to meet you. And it came out in this beautiful contralto....That was it. One man straightened his tie, and another ran his fingers through his hair. What really got me was that the oldest guy in the place actually checked his fly to make sure it was zipped. Theyre all crazy about her now. All they needed was the voice. She isnt a robot any more; shes a girl.
You mean theyre talking to her?
Are they talking to her! I should say so. I should have programmed her for sexy intonations. Theyd be asking her for dates right now if I had. Talk about conditioned reflex. Listen, men respond to voices. At the most intimate moments, are they looking? Its the voice in your ear
Yes, Clinton, I seem to remember. Wheres Jane now?
With them. They wont let go of her.
Damn! Get in there with her. Dont let her out of your sight, man.
Madarians calls thereafter, during his ten-day stay at Flagstaff, were not very frequent and became progressively less exalted.
Jane was listening carefully, he reported, and occasionally she responded. She remained popular. She was given entry everywhere. But there were no results.
Bogert said, Nothing at all?
Madarian was at once defensive. You cant say nothing at all. Its impossible to say nothing at all with an intuitive robot. You dont know what might not be going on inside her. This morning she asked Jensen what he had for breakfast.
Rossiter Jensen the astrophysicist?
Yes, of course. As it turned out, he didnt have breakfast that morning. Well, a cup of coffee.
So Janes learning to make small talk. That scarcely makes up for the expense.
Oh, dont be a jackass. It wasnt small talk. Nothing is small talk for Jane. She asked because it had something to do with some sort of cross-correlation she was building in her mind.
What can it possibly
How do I know? If I knew, Id be a Jane myself and you wouldnt need her. But it has to mean something. Shes programmed for high motivation to obtain an answer to the question of a planet with optimum habitability/distance and
Then let me know when shes done that and not before. Its not really necessary for me to get a blow-by-blow description of possible correlations.
He didnt really expect to get notification of success. With each day, Bogert grew less sanguine, so that when the notification finally came, he wasnt ready. And it came at the very end.
That last time, when Madarians climactic message came, it came in what was almost a whisper. Exaltation had come complete circle and Madarian was awed into quiet.
She did it, he said. She did it. After I all but gave up, too. After she had received everything in the place and most of it twice and three times over and never said a word that sounded like anything....Im on the plane now, returning. Weve just taken off.
Bogert managed to get his breath. Dont play games, man. You have the answer? Say so, if you have. Say it plainly.
She has the answer. Shes given me the answer. Shes given me the names of three stars within eighty light-years which, she says, have a sixty to ninety percent chance of possessing one habitable planet each. The probability that at least one has is 0.972. Its almost certain. And thats just the least of it. Once we get back, she can give us the exact line of reasoning that led her to the conclusion and I predict that the whole science of astrophysics and cosmology will
Are you sure
You think Im having hallucinations? I even have a witness. Poor guy jumped two feet when Jane suddenly began to reel out the answer in her gorgeous voice
And that was when the meteorite struck and in the thorough destruction of the plane that followed, Madarian and the pilot were reduced to gobbets of bloody flesh and no usable remnant of Jane was recovered.


The gloom at U. S. Robots had never been deeper. Robertson attempted to find consolation in the fact that the very completeness of the destruction had utterly hidden the illegalities of which the firm had been guilty.
Peter shook his head and mourned. Weve lost the best chance U. S. Robots ever had of gaining an unbeatable public image; of overcoming the damned Frankenstein complex. What it would have meant for robots to have one of them work out the solution to the habitable-planet problem, after other robots had helped work out the Space Jump. Robots would have opened the galaxy to us. And if at the same time we could have driven scientific knowledge forward in a dozen different directions as we surely would have...Oh, God, theres no way of calculating the benefits to the human race, and to us of course.
Robertson said, We could build other Janes, couldnt we? Even without Madarian?
Sure we could. But can we depend on the proper correlation again? Who knows how lowprobability that final result was? What if Madarian had had a fantastic piece of beginners luck? And then to have an even more fantastic piece of bad luck? A meteorite zeroing in...Its simply unbelievable
Robertson said in a hesitating whisper, It couldnt have been meant. I mean, if we werent meant to know and if the meteorite was a judgmentfrom
He faded off under Bogerts withering glare. Bogert said, Its not a dead loss, I suppose. Other Janes are bound to help us in some ways. And we can give other robots feminine voices, if that will help encourage public acceptancethough I wonder what the women would say. If we only knew what Jane-5 had said!
In that last call, Madarian said there was a witness. Bogert said, I know; Ive been thinking about that. Dont you suppose Ive been in touch with flagstaff? Nobody in the entire place heard Jane say anything that was out of the ordinary, anything that sounded like an answer to the habitable-planet problem, and certainly anyone there should have recognized the answer if it came or at least recognized it as a possible answer.
Could Madarian have been lying? Or crazy? Could he have been trying to protect himself
You mean he may have been trying to save his reputation by pretending he had the answer and then gimmick Jane so she couldnt talk and say, Oh, sorry, something happened accidentally. Oh, darn! I wont accept that for a minute. You might as well suppose he had arranged the meteorite.
Then what do we do? Bogert said heavily, Turn back to flagstaff. The answer must be there. Ive got to dig deeper, thats all. Im going there and Im taking a couple of the men in Madarians department. Weve got to go through that place top to bottom and end to end.
But, you know, even if there were a witness and he had heard, what good would it do, now that we dont have Jane to explain the process?
Every little something is useful. Jane gave the names of the stars; the catalogue numbers probablynone of the named stars has a chance. If someone can remember her saying that and actually remember the catalogue number, or have heard it clearly enough to allow it to be recovered by Psycho-probe if he lacked the conscious memorythen well have something. Given the results at the end, and the data fed Jane at the beginning, we might be able to reconstruct the line of reasoning; we might recover the intuition. If that is done, weve saved the game
Bogert was back after three days, silent and thoroughly depressed. When Robertson inquired anxiously as to results, he shook his head. Nothing!
Nothing?
Absolutely nothing. I spoke with every man in flagstaffevery scientist, every technician, every studentthat had had anything to do with Jane; everyone that had as much as seen her. The number wasnt great; Ill give Madarian credit for that much discretion. He only allowed those to see her who might conceivably have had planetological knowledge to feed her. There were twenty-three men altogether who had seen Jane and of those only twelve had spoken to her more than casually.
I went over and over all that Jane had said. They remembered everything quite well. Theyre keen men engaged in a crucial experiment involving their specialty, so they had every motivation to remember. And they were dealing with a talking robot, something that was startling enough, and one that talked like a TV actress. They couldnt forget.
Robertson said, Maybe a Psycho-probe
If one of them had the vaguest thought that something had happened, I would screw out his consent to Probing. But theres nothing to leave room for an excuse, and to Probe two dozen men who make their living from their brains cant be done. Honestly, it wouldnt help. If Jane had mentioned three stars and said they had habitable planets, it would have been like setting up sky rockets in their brains. How could anyone of them forget?
Then maybe one of them is lying, said Robertson grimly. He wants the information for his own use; to get the credit himself later.
What good would that do him? said Bogert. The whole establishment knows exactly why Madarian and Jane were there in the first place. They know why I came there in the second. If at any time in the future any man now at Flagstaff suddenly comes up with a habitable-planet theory that is startlingly new and different, yet valid, every other man at Flagstaff and every man at U. S. Robots will know at once that he had stolen it. Hed never get away with it.
Then Madarian himself was somehow mistaken.
I dont see how I can believe that either. Madarian had an irritating personalityall robopsychologists have irritating personalities, I think, which must be why they work with robots rather than with menbut he was no dummy. He couldnt be wrong in something like this.
Then But Robertson had run out of possibilities. They had reached a blank wall and for some minutes each stared at it disconsolately.
Finally Robertson stirred. Peter
Well?
Lets ask Susan.
Bogert stiffened. What!
Lets ask Susan. Lets call her and ask her to come in.
Why? What can she possibly do?
I dont know. But shes a robopsychologist, too, and she might understand Madarian better than we do. Besides, sheOh, hell, she always had more brains than any of us.
Shes nearly eighty.
And youre seventy. What about it?
Bogert sighed. Had her abrasive tongue lost any of its rasp in the years of her retirement? He said, Well, Ill ask her.
Susan Calvin entered Bogerts office with a slow look around before her eyes fixed themselves on the Research Director. She had aged a great deal since her retirement. Her hair was a fine white and her face seemed to have crumpled. She had grown so frail as to be almost transparent and only her eyes, piercing and uncompromising, seemed to remain of all that had been.
Bogert strode forward heartily, holding out his hand. Susan!
Susan Calvin took it, and said, Youre looking reasonably well, Peter, for an old man. If I were you, I wouldnt wait till next year. Retire now and let the young men get to it....And Madarian is dead. Are you calling me in to take over my old job? Are you determined to keep the ancients till a year past actual physical death?
No, no, Susan. Ive called you in He stopped. He did not, after all, have the faintest idea of how to start.
But Susan read his mind now as easily as she always had. She seated herself with the caution born of stiffened joints and said, Peter, youve called me in because youre in bad trouble. Otherwise youd sooner see me dead than within a mile of you.
Come, Susan
Dont waste time on pretty talk. I never had time to waste when I was forty and certainly not now. Madarians death and your call to me are both unusual, so there must be a connection. Two unusual events without a connection is too low-probability to worry about. Begin at the beginning and dont worry about revealing yourself to be a fool. That was revealed to me long ago.
Bogert cleared his throat miserably and began. She listened carefully, her withered hand lifting once in a while to stop him so that she might ask a question.
She snorted at one point. Feminine intuition? Is that what you wanted the robot for? You men. Faced with a woman reaching a correct conclusion and unable to accept the fact that she is your equal or superior in intelligence, you invent something called feminine intuition.
Oh, yes, Susan, but let me continue
He did. When she was told of Janes contralto voice, she said, It is a difficult choice sometimes whether to feel revolted at the male sex or merely to dismiss them as contemptible.
Bogert said, Well, let me go on
When he was quite done, Susan said, May I have the private use of this office for an hour or two?
Yes, but
She said, I want to go over the various recordsJanes programming, Madarians calls, your interviews at flagstaff. I presume I can use that beautiful new shielded laser-phone and your computer outlet if I wish.
Yes, of course.
Well, then, get out of here, Peter.
It was not quite forty-five minutes when she hobbled to the door, opened it, and called for Bogert.
When Bogert came, Robertson was with him. Both entered and Susan greeted the latter with an unenthusiastic Hello, Scott.
Bogert tried desperately to gauge the results from Susans face, but it was only the face of a grim old lady who had no intention of making anything easy for him.
He said cautiously, Do you think theres anything you can do, Susan?
Beyond what I have already done? No! Theres nothing more. Bogerts lips set in chagrin, but Robertson said, What have you already done, Susan?
Susan said, Ive thought a little; something I cant seem to persuade anyone else to do. For one thing, Ive thought about Madarian. I knew him, you know. He had brains but he was a very irritating extrovert. I thought you would like him after me, Peter.
It was a change, Bogert couldnt resist saying.
And he was always running to you with results the very minute he had them, wasnt he?
Yes, he was.
And yet, said Susan, his last message, the one in which he said Jane had given him the answer, was sent from the plane. Why did he wait so long? Why didnt he call you while he was still at flagstaff, immediately after Jane had said whatever it was she said?
I suppose, said Peter, that for once he wanted to check it thoroughly andwell, I dont know. It was the most important thing that had ever happened to him; he might for once have wanted to wait and be sure of himself.
On the contrary; the more important it was, the less he would wait, surely. And if he could manage to wait, why not do it properly and wait till he was back at U. S. Robots so that he could check the results with all the computing equipment this firm could make available to him? In short, he waited too long from one point of view and not long enough from another.
Robertson interrupted. Then you think he was up to some trickery
Susan looked revolted. Scott, dont try to compete with Peter in making inane remarks. Let me continue....A second point concerns the witness. According to the records of that last call, Madarian said, Poor guy jumped two feet when Jane suddenly began to reel out the answer in her gorgeous voice. In fact, it was the last thing he said. And the question is, then, why should the witness have jumped? Madarian had explained that all the men were crazy about that voice, and they had had ten days with the robotwith Jane. Why should the mere act of her speaking have startled them?
Bogert said, I assumed it was astonishment at hearing Jane give an answer to a problem that has occupied the minds of planetologists for nearly a century.
But they were waiting for her to give that answer. That was why she was there. Besides, consider the way the sentence is worded. Madarians statement makes it seem the witness was startled, not astonished, if you see the difference. Whats more, that reaction came when Jane suddenly beganin other words, at the very start of the statement. To be astonished at the content of what Jane said would have required the witness to have listened awhile so that he might absorb it. Madarian would have said he had jumped two feet after he had heard Jane say thus-and-so. It would be after not when and the word suddenly would not be included.
Bogert said uneasily, I dont think you can refine matters down to the use or non-use of a word.
I can, said Susan frostily, because I am a robopsychologist. And I can expect Madarian to do so, too, because he was a robopsychologist. We have to explain those two anomalies, then. The queer delay before Madarians call and the queer reaction of the witness.
Can you explain them? Asked Robertson. Of course, said Susan, since I use a little simple logic. Madarian called with the news without delay, as he always did, or with as little delay as he could manage. If Jane had solved the problem at Flagstaff, he would certainly have called from Flagstaff. Since he called from the plane, she must clearly have solved the problem after he had left Flagstaff.
But then
Let me finish. Let me finish. Was Madarian not taken from the airport to Flagstaff in a heavy, enclosed ground car? And Jane, in her crate, with him?
Yes.
And presumably, Madarian and the crated Jane returned from Flagstaff to the airport in the same heavy, enclosed ground car. Am I right?
Yes, of course!
And they were not alone in the ground car, either. In one of his calls, Madarian said, We were chauffeured from the airport to the main administration building, and I suppose I am right in concluding that if he was chauffeured, then that was because there was a chauffeur, a human driver, in the car.
Good God!
The trouble with you, Peter, is that when you think of a witness to a planetological statement, you think of planetologists. You divide up human beings into categories, and despise and dismiss most. A robot cannot do that. The First Law says, A robot may not injure a human being or, through inaction, allow a human being to come to harm. Any human being. That is the essence of the robotic view of life. A robot makes no distinction. To a robot, all men are truly equal, and to a robopsychologist who must perforce deal with men at the robotic level, all men are truly equal, too.
It would not occur to Madarian to say a truck driver had heard the statement. To you a truck driver is not a scientist but is a mere animate adjunct of a truck, but to Madarian he was a man and a witness. Nothing more. Nothing less.
Bogert shook his head in disbelief. But you are sure?
Of course Im sure. How else can you explain the other point; Madarians remark about the startling of the witness? Jane was crated, wasnt she? But she was not deactivated. According to the records, Madarian was always adamant against ever deactivating an intuitive robot. Moreover, Jane-5, like any of the Janes, was extremely non-talkative. Probably it never occurred to Madarian to order her to remain quiet within the crate; and it was within the crate that the pattern finally fell into place. Naturally she began to talk. A beautiful contralto voice suddenly sounded from inside the crate. If you were the truck driver, what would you do at that point? Surely youd be startled. Its a wonder he didnt crash.
But if the truck driver was the witness, why didnt he come forward
Why? Can he possibly know that anything crucial had happened, that what he heard was important? Besides, dont you suppose Madarian tipped him well and asked him not to say anything? Would you want the news to spread that an activated robot was being transported illegally over the Earths surface.
Well, will he remember what was said?
Why not? It might seem to you, Peter, that a truck driver, one step above an ape in your view, cant remember. But truck drivers can have brains, too. The statements were most remarkable and the driver may well have remembered some. Even if he gets some of the letters and numbers wrong, were dealing with a finite set, you know, the fifty-five hundred stars or star systems within eighty light-years or soI havent looked up the exact number. You can make the correct choices. And if needed, you will have every excuse to use the Psycho-probe
The two men stared at her. Finally Bogert, afraid to believe, whispered, But how can you be sure?
For a moment, Susan was on the point of saying: Because Ive called Flagstaff, you fool, and because I spoke to the truck driver, and because he told me what he had heard, and because Ive checked with the computer at Flagstaff and got the only three stars that fit the information, and because I have those names in my pocket.
But she didnt. Let him go through it all himself. Carefully, she rose to her feet, and said sardonically, How can I be sure?...Call it feminine intuition.

The Bicentennial Man

1.

Andrew Martin said, Thank you, and took the seat offered him. He didnt look driven to the last resort, but he had been.
He didnt, actually, look anything, for there was a smooth blankness, to his face, except for the sadness one imagined one saw in his eyes. His hair was smooth, light brown, rather fine; and he had no facial hair. He looked freshly and cleanly shaved. His clothes were distinctly old-fashioned, but neat, and predominantly a velvety red-purple in color.
Facing him from behind the desk was the surgeon. The nameplate on the desk included a fully identifying series of letters and numbers which Andrew didnt bother with. To call him Doctor would be quite enough.
When can the operation be carried through, Doctor? he asked.
Softly, with that certain inalienable note of respect that a robot always used to a human being, the surgeon said, I am not certain, sir, that I understand how or upon whom such an operation could be performed.
There might have been a look of respectful intransigence on the surgeons face, if a robot of his sort, in lightly bronzed stainless steel, could have such an expression or any expression.
Andrew Martin studied the robots right hand, his cutting hand, as it lay motionless on the desk. The fingers were long and were shaped into artistically metallic, looping curves so graceful and appropriate that one could imagine a scalpel fitting them and becoming, temporarily, one piece with them. There would be no hesitation in his work, no stumbling, no quivering, no mistakes. That confidence came with specialization, of course, a specialization so fiercely desired by humanity that few robots were, any longer, independently brained. A surgeon, of course, would have to be. But this one, though brained, was so limited in his capacity that he did not recognize Andrew, had probably never heard of him.
Have you ever thought you would like to be a man? Andrew asked.
The surgeon hesitated a moment, as though the question fitted nowhere in his allotted positronic pathways. But I am a robot, sir.
Would it be better to be a man?
If would be better, sir, to be a better surgeon. I could not be so if I were a man, but only if I were a more advanced robot. I would be pleased to be a more advanced robot.
It does not offend you that I can order you about? That I can make you stand up, sit down, move right or left, by merely telling you to do so?
It is my pleasure to please you, sir. If your orders were to interfere with my functioning with respect to you or to any other human being, I would not obey you. The First Law, concerning my duty to human safety, would take precedence over the Second Law relating to obedience. Otherwise, obedience is my pleasure. Now, upon whom am I to perform this operation?
Upon me, Andrew said.
But that is impossible. It is patently a damaging operation.
That does not matter, said Andrew, calmly. I must not inflict damage, said the surgeon. On a human being, you must not, said Andrew, but I, too, am a robot.

2.

Andrew had appeared much more a robot when he had first been manufactured. He had then been as much a robot in appearance as any that had ever existed, smoothly designed and functional.
He had done well in the home to which he had been brought in those days when robots in households, or on the planet altogether, had been a rarity. There had been four in the home: Sir and Maam and Miss and Little Miss. He knew their names, of course, but he never used them. Sir was Gerald Martin.
His own serial number was NDR He eventually forgot the numbers. It had been a long time, of course; but if he had wanted to remember, he could not have forgotten. He had not wanted to remember.
Little Miss had been the first to call him Andrew, because she could not use the letters, and all the rest followed her in this.
Little Miss She had lived for ninety years and was long since dead. He had tried to call her Maam once, but she would not allow it. Little Miss she had been to her last day.
Andrew had been intended to perform the duties of a valet, a butler, even a ladys maid. Those were the experimental days for him and, indeed, for all robots anywhere save in the industrial and exploratory factories and stations off Earth.
The Martins enjoyed him, and half the time he was prevented from doing his work because Miss and Little Miss wanted to play with him. It was Miss who first understood how this might be arranged. We order you to play with us and you must follow orders.
I am sorry, Miss, but a prior order from Sir must surely take precedence.
But she said, Daddy just said he hoped you would take care of the cleaning. Thats not much of an order. I order you.
Sir did not mind. Sir was fond of Miss and of Little Miss, even more than Maam was; and Andrew was fond of them, too. At least, the effect they had upon his actions were those which in a human being would have been called the result of fondness. Andrew thought of it as fondness for he did not know any other word for it.
It was for Little Miss that Andrew had carved a pendant out of wood. She had ordered him to. Miss, it seemed, had received an ivorite pendant with scrollwork for her birthday and Little Miss was unhappy over it. She had only a piece of wood, which she gave Andrew together with a small kitchen knife.
He had done it quickly and Little Miss had said, Thats nice, Andrew. Ill show it to Daddy.
Sir would not believe it. Where did you really get this, Mandy? Mandy was what he called Little Miss. When Little Miss assured him she was really telling the truth, he turned to Andrew. Did you do this, Andrew?
Yes, Sir.
The design, too?
Yes, Sir.
From what did you copy the design?
It is a geometric representation, Sir, that fits the grain of the wood.
The next day, Sir brought him another piece of wood a larger one and an electric vibro-knife. Make something out of this, Andrew. Anything you want to, he said.
Andrew did so as Sir watched, then looked at the product a long time. After that, Andrew no longer waited on tables. He was ordered to read books on furniture design instead, and he learned to make cabinets and desks.
These are amazing productions, Andrew, Sir soon told him.
I enjoy doing them, Sir, Andrew admitted.
Enjoy?
It makes the circuits of my brain somehow flow more easily. I have heard you use the word `enjoy and the way you use it fits the way I feel. I enjoy doing them, Sir.

3.

Gerald Martin took Andrew to the regional offices of the United States Robots and Mechanical Men Corporation. As a member of the Regional Legislature he had no trouble at all in gaining an interview with the chief robopsychologist. In fact, it was only as a member of the Regional Legislature that he qualified as a robot owner in the first place in those early days when robots were rare.
Andrew did not understand any of this at the time. But in later years, with greater learning, he could re-view that early scene and understand it in its proper light.
The robopsychologist, Merton Mansky, listened with a growing frown and more than once managed to stop his fingers at the point beyond which they would have irrevocably drummed on the table. He had drawn features and a lined forehead, but he might actually have been younger than he looked.
Robotics is not an exact art, Mr. Martin, Mansky explained. I cannot explain it to you in detail, but the mathematics governing the plotting of the positronic pathways is far too complicated to permit of any but approximate solutions. Naturally, since we build everything around the Three Laws, those are incontrovertible. We will, of course, replace your robot
Not at all, said Sir. There is no question of failure on his part. He performs his assigned duties perfectly. The point is he also carves wood in exquisite fashion and never the same twice. He produces works of art.
Mansky looked confused. Strange. Of course, were attempting generalized pathways these days. Really creative, you think?
See for yourself. Sir handed over a little sphere of wood on which there was a playground scene in which the boys and girls were almost too small to make out, yet they were in perfect proportion and they blended so naturally with the grain that it, too, seemed to have been carved.
Mansky was incredulous. He did that? He handed it back with a shake of his head. The luck of the draw. Something in the pathways.
Can you do it again?
Probably not. Nothing like this has ever been reported.
Good! I dont in the least mind Andrews being the only one.
I suspect that the company would like to have your robot back for study, Mansky said.
Not a chance! Sir said with sudden grimness. Forget it. He turned to Andrew, Lets go home, now.

4.

Miss was dating boys and wasnt about the house much. It was Little Miss, not as little as she once was, who filled Andrews horizon now. She never forgot that the very first piece of wood carving he had done had been for her. She kept it on a silver chain about her neck.
It was she who first objected to Sirs habit of giving away Andrews work. Come on, Dad, if anyone wants one of them, let him pay for it. Its worth it.
It isnt like you to be greedy, Mandy.
Not for us, Dad. For the artist.
Andrew had never heard the word before, and when he had a moment to himself he looked it up in the dictionary.
Then there was another trip, this time to Sirs lawyer.
What do you think of this, John? Sir asked.
The lawyer was John Finegold. He had white hair and a pudgy belly, and the rims of his contact lenses were tinted a bright green. He looked at the small plaque Sir had given him. This is beautiful. But Ive already heard the news. Isnt thus a carving made by your robot? The one youve brought with you.
Yes, Andrew does them. Dont you, Andrew?
Yes, Sir, said Andrew.
How much would you pay for that, John? Sir asked.
I cant say. Im not a collector of such things.
Would you believe I have been offered two hundred and fifty dollars for that small thing. Andrew has made chairs that have sold for five hundred dollars. Theres two hundred thousand dollars in the bank from Andrews products.
Good heavens, hes making you rich, Gerald.
Half rich, said Sir. Half of it is in an account in the name of Andrew Martin.
The robot?
Thats right, and I want to know if its legal.
Legal . . . ? Feingolds chair creaked as he leaned back in it. There are no precedents, Gerald. How did your robot sign the necessary papers?
He can sign his name. Now, is there anything further that ought to be done?
Um. Feingolds eyes seemed to turn inward for a moment. Then he said, Well, we can set up a trust to handle all finances in his name and that will place a layer of insulation between him and the hostile world. Beyond that, my advice is you do nothing. No one has e stopped you so far. If anyone objects, let him bring suit
And will you take the case if the suit is brought?
For a retainer, certainly.
How much?
Something like that, Feingold said, and pointed to the wooden plaque.
Fair enough, said Sir.
Feingold chuckled as he turned to the robot. Andrew, are you pleased that you have money?
Yes, sir.
What do you plan to do with it? Pay for things, sir, which otherwise Sir would have to pay for. It would save him expense, sir.

5.

Such occasions arose. Repairs were expensive, and revisions were even more so. With the years, new models of robots were produced and Sir saw to it that Andrew had the advantage of every new device, until he was a model of metallic excellence. It was all done at Andrews expense. Andrew insisted on that.
Only his positronic pathways were untouched. Sir insisted on that.
The new models arent as good as you are, Andrew, he said. The new robots are worthless. The company has learned to make the pathways more precise, more closely on the nose, more deeply on the track. The new robots dont shift. They do what theyre designed for and never stray. I like you better.
Thank you, Sir.
And its your doing, Andrew, dont you forget that. I am certain Mansky put an end to generalized pathways as soon as he had a good look at you. He didnt like the unpredictability. Do you know how many times he asked for you back so he could place you under study? Nine times! I never let him have you, though; and now that hes retired, we may have some peace.
So Sirs hair thinned and grayed and his face grew pouchy, while Andrew looked even better than he had when he first joined the family. Maam had joined an art colony somewhere in Europe, and Miss was a poet in New York. They wrote sometimes, but not often. Little Miss was married and lived not far away. She said she did not want to leave Andrew. When her child, Little Sir, was born, she let Andrew hold the bottle and feed him.
With the birth of a grandson, Andrew felt that Sir finally had someone to replace those who had gone. Therefore, it would not be so unfair now to come to him with the request.
Sir, it is kind of you to have allowed me to spend my money as I wished
It was your money, Andrew.
Only by your voluntary act, Sir. I do not believe the law would have stopped you from keeping it all.
The law wont persuade me to do wrong, Andrew.
Despite all expenses, and despite taxes, too, Sir, I have nearly six hundred thousand dollars.
I know that, Andrew.
I want to give it to you, Sir.
I wont take it, Andrew
In exchange for something you can give me, Sir
Oh? What is that, Andrew?
My freedom, Sir.
Your
I wish to buy my freedom, Sir.

6.

It wasnt that easy. Sir had flushed, had said, For Gods sake! Then he had turned on his heel and stalked away.
It was Little Miss who finally brought him round, defiantly and harshly and in front of Andrew. For thirty years no one had ever hesitated to talk in front of Andrew, whether or not the matter involved Andrew. He was only a robot.
Dad, why are you taking this as a personal affront? Hell still be here. Hell still be loyal. He cant help that; its built in. All he wants is a form of words. Ha wants to be called free. Is that so terrible? Hasnt be earned this chance? Heavens, he and I have been talking about it for years!
Talking about it for years, have you?
Yes, and over and over again he postponed it for fear he would hurt you. I made him put the matter up to you.
He doesnt know what freedom is. Hes a robot.
Dad, you dont know him. Hes read everything in the library. I dont know what he feels inside, but I dont know what you feel inside either. When you talk to him youll find he reacts to the various abstractions as you and I do, and what else counts? If some one elses reactions are like your own, what more can you ask for?
The law wont take that attitude, Sir said, angrily. See here, you! He turned to Andrew with a deliberate grate in his voice. I cant free you except by doing it legally. If this gets into the courts, you not only wont get your freedom but the law will take official cognizance of your money. Theyll tell you that a robot has no right to earn money. Is this rigmarole worth losing your money?
Freedom is without price, Sir, said Andrew. Even the chance of freedom is worth the money.

7.

It seemed the court might also take the attitude that freedom was without price, and might decide that for no price, however great, could a robot buy its freedom.
The simple statement of the regional attorney who represented those who had brought a class action to oppose the freedom was this: The word `freedom has no meaning when applied to a robot. Only a human being can be free. He said it several times, when it seemed appropriate; slowly, with his hand coming down rhythmically on the desk before him to mark the words.
Little Miss asked permission to speak on behalf of Andrew.
She was recognized by her full name, something Andrew had never heard pronounced before: Amanda Laura Martin Charney may approach the bench.
Thank you, Your Honor. I am not a lawyer and I dont know the proper way of phrasing things, but I hope you will listen to my meaning and ignore the words.
Lets understand what it means to be free in Andrews case. In some ways, he is free. I think its at least twenty years since anyone in the Martin family gave him an order to do something that we felt he might not do of his own accord. But we can, if we wish, give him an order to do anything, couching it as harshly as we wish, because he is a machine that belongs to us. Why should we be in a position to do so, when he has served us so long, so faithfully, and has earned so much money for us? He owes us nothing more. The debit is entirely on the other side.
Even if we were legally forbidden to place Andrew in involuntary servitude, he would still serve us voluntarily. Making him free would be a trick of words only, but it would mean much to him. It would give him everything and cost us nothing.
For a moment the judge seemed to be suppressing a smile. I see your point, Mrs. Chamey. The fact is that there is no binding law in this respect and no precedent. There is, however, the unspoken assumption that only a man may enjoy freedom. I can make new law here, subject to reversal in a higher court; but I cannot lightly run counter to that assumption. Let me address the robot. Andrew!
Yes, Your Honor.
It was the first time Andrew bad spoken in court, and the judge seemed astonished for a moment at the human timbre of his voice.
Why do you want to be free, Andrew? In what way will this matter to you?
Andrew said, Would you wish to be a slave, Your Honor?
But you are not a slave. You are a perfectly good robot a genius of a robot, I am given to understand, capable of an artistic expression that can be matched nowhere. What more could you do if you were free?
Perhaps no more than I do now, Your Honor, but with greater joy. It has been said in this courtroom that only a human being can be free. It seems to me that only someone who wishes for freedom can be free. I wish for freedom.
And it was that statement that cued the judge. The crucial sentence in his decision was There is no right to deny freedom to any object with a mind advanced enough to grasp the concept and desire the state. It was eventually upheld by the World Court.

8.

Sir remained displeased, and his harsh voice made Andrew feel as if he were being short-circuited. I dont want your damned money, Andrew. Ill take it only because you wont feel free otherwise. From now on, you can select your own jobs and do them as you please. I will give you no orders, except this one: Do as you please. But I am still responsible for you. Thats part of the court order. I hope you understand that.
Little Miss interrupted. Dont be irascible, Dad. The responsibility is no great chore. You know you wont have to do a thing. The Three Laws still hold.
Then how is he free?
Are not human beings bound by their laws, Sir? Andrew replied.
Im not going to argue. Sir left the room, and Andrew saw him only infrequently after that.
Little Miss came to see him frequently in the small house that had been built and made over for him. It had no kitchen, of course, nor bathroom facilities. It had just two rooms; one was a library and one was a combination storeroom and workroom. Andrew accepted many commissions and worked harder as a free robot than he ever had before, till the cost of the house was paid for and the structure was signed over to him.
One day Little Sir no, George! came. Little Sir had insisted on that after the court decision. A free robot doesnt call anyone Little Sir, George had said. I call you Andrew. You must call me George.
His preference was phrased as an order, so Andrew called him George but Little Miss remained Little Miss.
One day when George came alone, it was to say that Sir was dying. Little Miss was at the bedside, but Sir wanted Andrew as well.
Sirs voice was still quite strong, though he seemed unable to move much. He struggled to raise his hand.
Andrew, he said, Andrew Dont help me, George. Im only dying; Im not crippled. Andrew, Im glad youre free. I just wanted to tell you that.
Andrew did not know what to say. He had never been at the side of someone dying before, but he knew it was the human way of ceasing to function. It was an involuntary and irreversible dismantling, and Andrew did not know what to say that might be appropriate. He could only remain standing, absolutely silent, absolutely motionless.
When it was over, Little Miss said to him, He may not have seemed friendly to you toward the end, Andrew, but he was old, you know; and it hurt him that you should want to be free.
Then Andrew found the words. I would never have been free without him, Little Miss.

9.

Only after Sirs death did Andrew begin to wear clothes. He began with an old pair of trousers at first, a pair that George had given him.
George was married now, and a lawyer. He had joined Feingolds firm. Old Feingold was long since dead, but his daughter had carried on. Eventually the firms name became Feingold and Martin. It remained so even when the daughter retired and no Feingold took her place. At the time Andrew first put on clothes, the Martin name had just been added to the firm.
George had tried not to smile the first time he saw Andrew attempting to put on trousers, but to Andrews eyes the smile was clearly there. George showed Andrew how to manipulate the static charge to allow the trousers to open, wrap about his lower body, and move shut. George demonstrated on his own trousers, but Andrew was quite aware it would take him a while to duplicate that one flowing motion.
But why do you want trousers, Andrew? Your body is so beautifully functional its a shame to cover it especially when you neednt worry about either temperature control or modesty. And the material doesnt cling properly not on metal.
Andrew held his ground. Are not human bodies beautifully functional, George? Yet you cover yourselves.
For warmth, for cleanliness, for protection, for decorativeness. None of that applies to you.
I feel bare without clothes. I feel different, George, Andrew responded.
Different! Andrew, there are millions of robots on Earth now. In this region, according to the last census, there are almost as many robots as there are men.
I know, George. There are robots doing every conceivable type of work.
And none of them wear clothes.
But none of them are free, George.
Little by little, Andrew added to his wardrobe. He was inhibited by Georges smile and by the stares of the people who commissioned work.
He might be free, but there was built into Andrew a carefully detailed program concerning his behavior to people, and it was only by the tiniest steps that he dared advance; open disapproval would set him back months. Not everyone accepted Andrew as free. He was incapable of resenting that, and yet there was a difficulty about his thinking process when he thought of it. Most of all, he tended to avoid putting on clothes or too many of them when he thought Little Miss might come to visit him. She was older now and was often away in some warmer climate, but when she returned the first thing she did was visit him.
On one of her visits, George said, ruefully, Shes got me, Andrew. Ill be running for the legislature next year. `Like grandfather, she says, `like grandson.
Like grandfather . . . Andrew stopped, uncertain.
I mean that I, George, the grandson, will be like Sir, the grandfather, who was in the legislature once.
It would be pleasant, George, if Sir were still He paused, for he did not want to say, in working order. That seemed inappropriate.
Alive; George said. Yes, I think of the old monster now and then, too.
Andrew often thought about this conversation. He had noticed his own incapacity in speech when talking with George. Somehow the language had changed since Andrew had come into being with a built-in vocabulary. Then, too, George used a colloquial speech, as Sir and Little Miss had not. Why should he have called Sir a monster when surely that word was not a appropriate. Andrew could not even turn to his own books for guidance. They were old, and most dealt with woodworking, with art, with furniture design. There were none on language, none on the ways of human beings.
Finally, it seemed to him that he must seek the proper books; and as a free robot, he felt he must not ask George. He would go to town and use the library. It was a triumphant decision and he felt his electro potential grow distinctly higher until he had to throw in an impedance coil.
He put on a full costume, including even a shoulder chain of wood. He would have preferred the glitter plastic, but George had said that wood was much more appropriate, and that polished cedar was considerably more valuable as well.
He had placed a hundred feet between himself and the house before gathering resistance brought him to a halt. He shifted the impedance coil out of circuit, and when that did not seem to help enough he returned to his home and on a piece of notepaper wrote neatly, I have gone to the library, and placed it in clear view on his worktable.


10.

Andrew never quite got to the library.
He had studied the map. He knew the route, but not the appearance of it. The actual landmarks did not resemble the symbols on the map and he would hesitate. Eventually, he thought he must have somehow gone wrong, for everything looked strange.
He passed an occasional field-robot, but by the time he decided he should ask his way none were in sight. A vehicle passed and did not stop.
Andrew stood irresolute, which meant calmly motionless, for coming across the field toward him were two human beings.
He turned to face them, and they altered their course to meet him. A moment before, they had been talking loudly. He had heard their voices. But now they were silent. They had the look that Andrew associated with human uncertainty; and they were young, but not very young. Twenty, perhaps? Andrew could never judge human age.
Would you describe to me the route to the town library, sirs?
One of them, the taller of the two, whose tall hat lengthened him still farther, almost grotesquely, said, not to Andrew, but to the other, Its a robot.
The other had a bulbous nose and heavy eyelids. He said, not to Andrew but to the first, Its wearing clothes.
The tall one snapped his fingers. Its the free robot. They have a robot at the old Martin place who isnt owned by anybody. Why else would it be wearing clothes?
Ask it, said the one with the nose.
Are you the Martin robot? asked the tall one.
I am Andrew Martin, sir, Andrew said.
Good. Take off your clothes. Robots dont wear clothes. He said to the other, Thats disgusting. Look at him!
Andrew hesitated. He hadnt heard an order in that tone of voice in so long that his Second Law circuits had momentarily jammed.
The tall one repeated, Take off your clothes. I order you.
Slowly, Andrew began to remove them.
Just drop them, said the tall one.
The nose said, If it doesnt belong to anyone, it could be ours as much as someone elses.
Anyway, said the tall one, whos to object to anything we do. Were not damaging property. tie turned to Andrew. Stand on your head. The head is not meant- Andrew began.
Thats an order. If you dont know how, try anyway.
Andrew hesitated again, then bent to put his head on the ground. He tried to lift his legs but fell, heavily.
The tall one said, Just lie there. He said to the other, We can take him apart. Ever take a robot apart?
Will he let us?
How can he stop us?
There was no way Andrew could stop them, if they ordered him in a forceful enough manner not to resist The Second Law of obedience took precedence over the Third Law of self-preservation. In any case, he could not defend himself without possibly hurting them, and that would mean breaking the First Law. At that thought, he felt every motile unit contract slightly and he quivered as he lay there.
The tall one walked over and pushed at him with his foot. Hes heavy. I think well need tools to do the job.
The nose said, We could order him to take himself, apart. It would be fun to watch him try.
Yes, said the tall one, thoughtfully, but lets get him off the road. If someone comes along
It was too late. Someone had, indeed, come along and it was George. From where he lay, Andrew had seen him topping a small rise in the middle distance. He would have liked to signal him in some way, but the last order had been Just lie there!
George was running now, and he arrived on the scene somewhat winded. The two young men stepped back a little and then waited thoughtfully.
Andrew, has something gone wrong? George asked, anxiously.
Andrew replied, I am well, George.
Then stand up. What happened to your clothes?
That your robot, Mac? the tall young man asked.
George turned sharply. Hes no ones robot. Whats been going on here.
We politely asked him to take his clothes off. Whats that to you, if you dont own him.
George turned to Andrew. What were they doing, Andrew?
It was their intention in some way to dismember me. They were about to move me to a quiet spot and order me to dismember myself.
George looked at the two young men, and his chin trembled.
The young men retreated no farther. They were smiling.
The tall one said, lightly, What are you going to do, pudgy? Attack us?
George said, No. I dont have to. This robot has been with my family for over seventy-five years. He knows us and he values us more than he values anyone else. I am going to tell him that you two are threatening my life and that you plan to kill me. I will ask him to defend me. In choosing between me and you two, he will choose me. Do you know what will happen to you when he attacks you?
The two were backing away slightly, looking uneasy.
George said, sharply, Andrew, I am in danger and about to come to harm from these young men. Move toward them!
Andrew did so, and the young men did not wait. They ran.
All right, Andrew, relax, George said. He looked unstrung. He was far past the age where he could face the possibility of a dustup with one young man, let alone two.
I couldnt have hurt them, George: I could see they were not attacking you.
I didnt order you to attack them. I only told you to move toward them. Their own fears did the rest.
How can they fear robots?
Its a disease of mankind, one which has not yet been cured. But never mind that. What the devil are you doing here, Andrew? Good thing I found your note. I was just on the point of turning back and hiring a helicopter when I found you. How did you get it into your head to go to the library? I would have brought you any books you needed
I am a Andrew began.
Free robot. Yes, yes. All right, what did you want in the library?
I want to know more about human beings, about the world, about everything. And about robots, George. I want to write a history about robots.
George put his arm on the others shoulder. Well, lets walk home. But pick up your clothes first. Andrew, there are a million books on robotics and all of them include histories of the science. The world is growing saturated not only with robots but with information about robots.
Andrew shook his head, a human gesture he had lately begun to adopt. Not a history of robotics, George. A history of robots, by a robot. I want to explain how robots feel about what has happened since the first ones were allowed to work and live on Earth.
Georges eyebrows lifted, but he said nothing in direct response.

11.

Little Miss was just past her eighty-third birthday, but there was nothing about her that was lacking in either energy or determination. She gestured with her cane oftener than she propped herself up with it.
She listened to the story in a fury of indignation. George, thats horrible. Who were those young ruffians?
I dont know. What difference does it make? In the end they did not do any damage.
They might have. Youre a lawyer, George; and if youre well off, its entirely due to the talents of Andrew. It was the money he earned that is the foundation of everything we have. He provides the continuity for this family, and I will not have him treated as a wind-up toy.
What would you have me do, Mother? George asked.
I said youre a lawyer. Dont you listen? You set up a test case somehow, and you force the regional courts to declare for robot rights and get the legislature to pass the necessary bills. Carry the whole thing to the World Court, if you have to. Ill be watching, George, and Ill tolerate no shirking.
She was serious, so what began as a way of soothing the fearsome old lady became an involved matter with enough legal entanglement to make it interesting. As senior partner of Feingold and Martin, George plotted strategy. But he left the actual work to his junior partners, with much of it a matter for his son, Paul, who was also a member of the firm and who reported dutifully nearly every day to his grandmother. She, in turn, discussed the case every day with Andrew.
Andrew was deeply involved. His work on his book on robots was delayed again, as he pored over the legal arguments and even, at times, made very diffident suggestions.
George told me that day I was attacked that human beings have always been afraid of robots, he said one day. As long as they are, the courts and the legislatures are not likely to work hard on behalf of robots. Should not something be done about public opinion?
So while Paul stayed in court, George took to the public platform. It gave him the advantage of being informal, and he even went so far sometimes as to wear the new, loose style of clothing which he called drapery.
Paul chided him, Just dont trip over it on stage, Dad.
George replied, despondently, Ill try not to.
He addressed the annual convention of holo-news editors on one occasion and said, in part: If, by virtue of the Second Law, we can demand of any robot unlimited obedience in all respects not involving harm to a human being, then any human being, any human being, has a fearsome power over any robot, any robot. In particular, since Second Law supersedes Third Law; any human being can use the law of obedience to overcome the law of self-protection. He can order any robot to damage itself or even to destroy itself for any reason, or for no reason.
Is this just? Would we treat an animal so? Even an inanimate object which had given us good service has a claim on our consideration. And a robot is not insensitive; it is not an animal. It can think well enough so that it can talk to us, reason with us, joke with us. Can we treat them as friends, can we work together with them, and not give them some of the fruits of that friendship, some of the benefits of co-working?
If a man has the right to give a robot any order that does not involve harm to a human being, he should have the decency never to give a robot any order that involves harm to a robot, unless human safety absolutely requires it. With great power goes great responsibility, and if the robots have Three Laws to protect men, is it too much to ask that men have a law or two to protect robots?
Andrew was right. It was the battle over public opinion that held the key to courts and legislature. In the end, a law was passed that set up conditions under which robot-harming orders were forbidden. It was endlessly qualified and the punishments for violating the law were totally inadequate, but the principle was established. The final passage by the World Legislature came through on the day of Little Miss death.
That was no coincidence. Little Miss held on to life desperately during the last debate and let go only when word of victory arrived. Her last smile was for Andrew. Her last words were, You have been good to us, Andrew. She died with her hand holding his, while her son and his wife and children remained at a respectful distance from both.

12.

Andrew waited patiently when the receptionist-robot disappeared into the inner office. The receptionist might have used the holographic chatterbox, but un-questionably it was perturbed by having to deal with another robot rather than with a human being.
Andrew passed the time revolving the matter his mind: Could unroboted be used as an analog of unmanned, or had unmanned become a metaphoric term sufficiently divorced from its original literal meaning to be applied to robotsor to women for that matter? Such problems frequently arose as he worked on his book on robots. The trick of thinking out sentences to express all complexities had undoubtedly increased his vocabulary.
Occasionally, someone came into the room to stare at him and he did not try to avoid the glance. He looked at each calmly, and each in turn looked away.
Paul Martin finally emerged. He looked surprised, or he would have if Andrew could have made out his expression with certainty. Paul had taken to wearing the heavy makeup that fashion was dictating for bath sexes. Though it made sharper and firmer the somewhat bland lines of Pauls face, Andrew disapproved. He found that disapproving of human beings, as long as he did not express it verbally, did not make him very uneasy. He could even write the disapproval. He was sure it had not always been so.
Come in, Andrew. Im sorry I made you wait, but there was something I had to finish. Come in, you had said you wanted to talk to me, but I didnt know you meant here in town.
If you are busy, Paul, I am prepared to continue to wait.
Paul glanced at the interplay of shifting shadows on the dial on the wall that served as timepieces and said, I can make some time. Did you come alone?
I hired an automatobile.
Any trouble? Paul asked, with more than a trace of anxiety.
I wasnt expecting any. My rights are protected.
Paul looked all the more anxious for that. Andrew, Ive explained that the law is unenforceable, at least under most conditions. And if you insist on wearing clothes, youll run into trouble eventually; just like that first time.
And only tine, Paul. Im sorry you are displeased
Well, look at it this way: you are virtually a living legend, Andrew, and you are too valuable in many different ways for you to have any right to take chances with yourself. By the way, hows the book coming?
I am approaching the end, Paul. The publisher is quite pleased.
Good!
I dont know that hes necessarily pleased with the book as a book. I think he expects to sell many copies because its written by a robot and thats what pleases him.
Only human, Im afraid.
I am not displeased. Let it sell for whatever reason, since it will mean money and I can use some.
Grandmother left you
Little Miss was generous, and Im sure I can count on the family to help me out further. But it is the royalties from the book on which I am counting to help me through the next step.
What next step is that?
I wish to see the head of U.S. Robots and Mechanical Men Corporation. I have tried to make an appointment; but so far I have not been able to reach him. The Corporation did not cooperate with me in the writing of the book, so I am not surprised, you understand.
Paul was clearly amused. Cooperation is the last thing you can expect. They didnt cooperate with us in our great fight for robot rights. Quite the reverse, and you can see why. Give a robot rights and people may not want to buy them.
Nevertheless, said Andrew, if you call them, you may be able to obtain an interview for me.
Im no more popular with them than you are, Andrew.
But perhaps you can hint that by seeing me they may head off a campaign by Feingold and Martin to strengthen the rights of robots further.
Wouldnt that be a lie, Andrew?
Yes, Paul, and I cant tell one. That is why you must call.
Ah, you cant lie, but you can urge me to tell a lie, is that it? Youre getting more human all the time, Andrew.

13.

The meeting was not easy to arrange, even with Pauls supposedly weighted name.
But it finally came about. When it did, Harley Smythe-Robertson, who, on his mothers side, was descended from the original founder of the corporation and who had adopted the hyphenation to indicate it, looked remarkably unhappy. He was approaching retirement age and his entire tenure as president had been devoted to the matter of robot rights. His gray hair was plastered thinly over the top of his scalp; his face was not made up, and he eyed Andrew with brief hostility from time to time.
Andrew began the conversation. Sir, nearly a century ago, I was told by a Merton Mansky of this corporation that the mathematics governing the plotting of the positronic pathways was far too complicated to permit of any but approximate solutions and that, therefore, my own capacities were not fully predictable.
That was a century ago. Smythe-Robertson hesitated, then said icily, Sir. It is true no longer. Our robots are made with precision now and are trained precisely to their jobs.
Yes, said Paul, who had come along, as he said, to make sure that the corporation played fair, with the result that my receptionist must be guided at every point once events depart from the conventional, however slightly.
You would be much more displeased if it were to improvise, Smythe-Robertson said.
Then you no longer manufacture robots like myself which are flexible and adaptable.
No longer.
The research I have done in connection with my book, said Andrew, indicates that I am the oldest robot presently in active operation.
The oldest presently, said Smythe-Robertson, and the oldest ever. The oldest that will ever be. No robot is useful after the twenty-fifth year. They are called in and replaced with newer models.
No robot as presently manufactured is useful after the twentieth year, said Paul, with a note of sarcasm creeping into his voice. Andrew is quite exceptional in this respect.
Andrew, adhering to the path he had marked out for himself, continued, As the oldest robot in the world and the most flexible, am I not unusual enough to merit special treatment from the company?
Not at all, Smythe-Robertson said, freezing up. Your unusualness is an embarrassment to the company. If you were on lease, instead of having been an outright sale through some mischance, you would long since have been replaced.
But that is exactly the point, said Andrew. I am a free robot and I own myself. Therefore I come to you and ask you to replace me. You cannot do this without the owners consent. Nowadays, that consent is extorted as a condition of the lease, but in my time this did not happen.
Smythe-Robertson was looking both startled and puzzled, and for a moment there was silence. Andrew found himself staring at the hologram on the wall. It was a death mask of Susan Calvin, patron saint of all roboticists. She had been dead for nearly two centuries now, but as a result of writing his book Andrew knew, her so well he could half persuade himself that he had met her in life.
Finally Smythe-Robertson asked, How can I replace you for you? If I replace you, as robot, how can I donate the new robot to you as owner since in the very act of replacement you cease to exist. He smiled grimly.
Not at all difficult, Paul interposed. The seat of Andrews personality is his positronic brain and it is the one part that cannot be replaced without creating a new robot. The positronic brain, therefore, is Andrew the owner. Every other part of the robotic body can be replaced without affecting the robots personality, and those other parts are the brains possessions. Andrew, I should say, wants to supply his brain with a new robotic body.
Thats right, said Andrew, calmly. He turned to Smythe-Robertson. You have manufactured androids, havent you? Robots that have the outward appearance of humans, complete to the texture of the skin?
Yes, we have. They worked perfectly well, with their synthetic fibrous skins and tendons. There was virtually no metal anywhere except for the brain, yet they were nearly as tough as metal robots. They were tougher, weight for weight.
Paul looked interested. I didnt know that. How many are on the market?
None, said Smythe-Robertson. They were much more expensive than metal models and a market survey showed they would not be accepted. They looked too human.
Andrew was impressed. But the corporation retains its expertise, I assume. Since it does, I wish to request that I be replaced by an organic robot, an android.
Paul looked surprised. Good Lord! he said.
Smythe-Robertson stiffened. Quite impossible!
Why is it impossible? Andrew asked. I will pay any reasonable fee, of course.
We do not manufacture androids.
You do not choose to manufacture androids, Paul interjected quickly. That is not the same as being unable to manufacture them.
Nevertheless, Smythe-Robertson responded, the manufacture of androids is against public policy.
There is no law against it, said Paul.
Nevertheless, we do not manufacture them and we will not.
Paul cleared his throat. Mr. Smythe-Robertson, he said, Andrew is a free robot who comes under the purview of the law guaranteeing robot rights. You are aware of this, I take it?
Only too well.
This robot, as a free robot, chooses to wear clothes. This results in his being frequently humiliated by thoughtless human beings despite the law against the humiliation of robots. It is difficult to prosecute vague offenses that dont meet with the general disapproval of those who must decide on guilt and innocence.
U.S. Robots understood that from the start. Your fathers firm unfortunately did not.
My father is dead now, but what I see is that we have here a clear offense with a clear target.
What are you talking about? said Smythe-Robertson.
My client, Andrew Martin he has just become my client is a free robot who is entitled to ask U.S. Robots and Mechanical Men Corporation for the rights of replacement, which the corporation supplies to anyone who owns a robot for more than twenty-five years. In fact, the corporation insists on such replacement.
Paul was smiling and thoroughly at ease. The positronic brain of my client, he went on, is the owner of the body of my client which is certainly more than twenty-five years old. The positronic brain demands the replacement of the body and offers to pay any reasonable fee for an android body as that replacement. If you refuse the request, my client undergoes humiliation and we will sue.
While public opinion would not ordinarily support the claim of a robot in such a case, may I remind you that U.S. Robots is not popular with the public generally. Even those who most use and profit from robots are suspicious of the corporation. This may be a hangover from the days when robots were widely feared. It may be resentment against the power and wealth of U.S. Robots, which has a worldwide monopoly. Whatever the cause may be, the resentment eats. I think you will find that you would prefer not to be faced with a lawsuit, particularly since my client is wealthy and will live for many more centuries and will have no reason to refrain from fighting the battle forever.
Smythe-Robertson had slowly reddened. You are trying to force
I force you to do nothing, said Paul. If you wish to refuse to accede to my clients reasonable request, you may by all means do so and we will leave without another word. But we will sue, as is certainly our right, and you will find that you will eventually lose.
Well.
I see that you are going to accede, said Paul. You may hesitate but you will come to it in the end. Let me assure you, then, of one further point: If, in the process of transferring my clients positronic brain from his present body to an organic one, there is any damage, however slight, then I will never rest until Ive nailed the corporation to the ground. I will, if necessary, take every possible step to mobilize public opinion against the corporation if one brain path of my clients platinum-iridium essence is scrambled. He turned to Andrew and asked, Do you agree to all this, Andrew?
Andrew hesitated a full minute. It amounted to the approval of lying, of blackmail, of the badgering and humiliation of a human being. But not physical harm, he told himself, not physical harm.
He managed at last to come out with a rather faint Yes.

14.

He felt as though he were being constructed again. For days, then for weeks, finally for months, Andrew found himself not himself somehow, and the simplest actions kept giving rise to hesitation.
Paul was frantic. Theyve damaged you, Andrew. Well have to institute suit!
Andrew spoke very slowly. You mustnt. Youll never be able to prove something like m-m-m-m 
Malice?
Malice. Besides, I grow stronger, better. Its the tr tr tr 
Tremble?
Trauma. After all, theres never been such an op-op-op before.
Andrew could feel his brain from the inside. No one else could. He knew he was well, and during the months that it took him to learn full coordination and full positronic interplay he spent hours before the mirror.
Not quite human! The face was stiff too stiff and the motions were too deliberate. They lacked the careless, free flow of the human being, but perhaps that might come with time. At least now he could wear clothes without the ridiculous anomaly of a metal face going along with it.
Eventually, he said, I will be going back to work.
Paul laughed. That means you are well. What will you be doing? Another book?
No, said Andrew, seriously. I live too long for any one career to seize me by the throat and never let me go. There was a time when I was primarily an artist, and I can still turn to that. And there was a time when I was a historian, and I can still turn to that. But now I wish to be a robobiologist.
A robopsychologist, you mean.
No. That would imply the study of positronic brains, and at the moment I lack the desire to do that. A robobiologist, it seems to me, would be concerned with the working of the body attached to that brain.
Wouldnt that be a roboticist?
A roboticist works with a metal body. I would be studying an organic humanoid body, of which I have the only one, as far as I know.
You narrow your field, said Paul, thoughtfully. As an artist, all conception is yours; as a historian you deal chiefly with robots; as a robobiologist, you will deal with yourself.
Andrew nodded. It would seem so.
Andrew had to start from the very beginning, for he knew nothing of ordinary biology and almost nothing of science. He became a familiar sight in the libraries, where he sat at the electronic indices for hours at a time, looking perfectly normal in clothes. Those few who knew he was a robot in no way interfered with him.
He built a laboratory in a room which he added to his house; and his library grew, too.
Years passed, and Paul came to him one day and said, Its a pity youre no longer working on the history of robots. I understand U.S. Robots is adopting a radically new policy.
Paul had aged, and his deteriorating eyes had been replaced with photoptic cells. In that respect, he had drawn closer to Andrew.
What have they done? Andrew asked.
They are manufacturing central computers, gigantic positronic brains, really, which communicate with anywhere from a dozen to a thousand robots by microwave. The robots themselves have no brains at all. They are the limbs of the gigantic brain, and the two are physically separate.
Is that more efficient?
U.S. Robots claims it is. Smythe-Robertson established the new direction before he died, however, and its my notion that its a backlash at you. U.S. Robots is determined that they will make no robots that will give them the type of trouble you have, and for that reason they separate brain and body. The brain will have no body to wish changed; the body will have no brain to wish anything.
Its amazing, Andrew, Paul went on, the influence you have had on the history of. robots. It was your artistry that encouraged U.S. Robots to make robots more precise and specialized; it was your freedom that resulted in the establishment of the principle of robotic rights; it was your insistence on an android body that made U.S. Robots switch to brain-body separation
Andrew grew thoughtful. I suppose in the end the corporation will produce one vast brain controlling several billion robotic bodies. All the eggs will be in one basket. Dangerous. Not proper at all.
I think youre right, said Paul, but I dont suspect it will come to pass for a century at least and I wont live to see it. In fact, I may not live to see next year.
Paul! cried Andrew, in concern.
Paul shrugged. Men are mortal, Andrew. Were not like you. It doesnt matter too much, but it does make it important to assure you on one point. Im the last of the human Martins. The money I control personally will be left to the trust in your name, and as far as anyone can foresee the future, you will be economically secure.
Unnecessary, Andrew said, with difficulty. In all this time, he could not get used to the deaths of the Martins.
Lets not argue. Thats the way its going to be. Now, what are you working on?
I am designing a system for allowing androids myself to gain energy from the combustion of hydrocarbons, rather than from atomic cells.
Paul raised his eyebrows. So that they will breathe and eat?
Yes.
How long have you been pushing in that direction?
For a long time now, but I think I have finally designed an adequate combustion chamber for catalyzed controlled breakdown.
But why, Andrew? The atomic cell is surely infinitely better.
In some ways, perhaps. But the atomic cell is inhuman.

15.

It took time, but Andrew had time. In the first place, he did not wish to do anything till Paul had died in peace. With the death of the great-grandson of Sir, Andrew felt more nearly exposed to a hostile world and for that reason was all the more determined along the path he had chosen.
Yet he was not really alone. If a man had died, the firm of Feingold and Martin lived, for a corporation does not die any more than a robot does.
The firm had its directions and it followed them soullessly. By way of the trust and through the law firm, Andrew continued to be wealthy. In return for their own large annual retainer, Feingold and Martin involved themselves in the legal aspects of the new combustion chamber. But when the time came for Andrew to visit U.S. Robots and Mechanical Men Corporation, he did it alone. Once he had gone with Sir and once with Paul. This time, the third time, he was alone and manlike.
U.S. Robots had changed. The actual production plant had been shifted to a large space station, as had grown to be the case with more and more industries. With them had gone many robots. The Earth itself was becoming park like, with its one-billion-person population stabilized and perhaps not more than thirty percent of its at-least-equally-large robot population independently brained.
The Director of Research was Alvin Magdescu, dark of complexion and hair, with a little pointed beard and wearing nothing above the waist but the breast band that fashion dictated. Andrew himself was well covered in the older fashion of several decades back.
Magdescu offered his hand to his visitor. I know you, of course, and Im rather pleased to see you. Youre our most notorious product and its a pity old Smythe-Robertson was so set against you. We could have done a great deal with you.
You still can, said Andrew.
No, I dont think so. Were past the time. Weve had robots on Earth for over a century, but thats changing. It will be back to space with them, and those that stay here wont be brained.
But there remains myself, and I stay on Earth.
True, but there doesnt seem to be much of the robot about you. What new request have you?
To be still less a robot. Since I am so far organic, I wish an organic source of energy. I have here the plans
Magdescu did not hasten through them. He might have intended to at first, but he stiffened and grew intent. At one point, he said, This is remarkably ingenious. Who thought of all this?
I did, Andrew replied.
Magdescu looked up at him sharply, then said, It would amount to a major overhaul of your body, and an experimental one, since such a thing has never been attempted before. I advise against it. Remain as you are.
Andrews face had limited means of expression, but impatience showed plainly in his voice. Dr. Magdescu, you miss the entire point: You have no choice but to accede to my request. If such devices can be built into my body, they can be built into human bodies as well. The tendency to lengthen human life by prosthetic devices has already been remarked on. There are no devices better than the ones I have designed or am designing. As it happens, I control the patents by way of the firm of Feingold and Martin. We are quite capable of going into business for ourselves and of developing the kind of prosthetic devices that may end by producing human beings with many of the properties of robots. Your own business will then suffer.
If, however, you operate on me now and agree to do so under similar circumstances in the future, you will receive permission to make use of the patents and control the technology of both robots and of the prosthetization of human beings. The initial leasing will not be granted, of course, until after the first operation is completed successfully, and after enough time has passed to demonstrate that it is indeed successful.
Andrew felt scarcely any First Law inhibition to the stern conditions he was setting a human being. He was learning to reason that what seemed like cruelty might, in the long run, be kindness.
Magdescu was stunned. Im not the one to decide something like this. Thats a corporate decision that would take time.
I can wait a reasonable time, said Andrew, but only a reasonable time. And he thought with satisfaction that Paul himself could not have done it better.

16.

It took only a reasonable time, and the operation was a success.
I was very much against the operation, Andrew, Magdescu said, but not for the reasons you might think. I was not in the least against the experiment, if it had been on someone else. I hated risking your positronic brain. Now that you have the positronic pathways interacting with simulated nerve pathways, it might have been difficult to rescue the brain intact if the body had gone bad.
I had every faith in the skill of the staff at U.S. Robots, said Andrew. And I can eat now.
Well, you can sip olive oil. It will mean occasional cleanings of the combustion chamber, as we have explained to you. Rather an uncomfortable touch, I should think.
Perhaps, if I did not expect to go further. Self cleaning is not impossible. In fact, I am working on a device that will deal with solid food that may be expected to contain incombustible fractions indigestible matter, so to speak, that will have to be discarded.
You would then have to develop an anus.
Or the equivalent.
What else, Andrew?
Everything else.
Genitalia, too?
Insofar as they will fit my plans. My body is a canvas on which I intend to draw
Magdescu waited for the sentence to he completed, and when it seemed that it would not be, he completed it himself. A man?
We shall see, Andrew said.
Thats a puny ambition, Andrew. Youre better than a man. Youve gone downhill from the moment you opted to become organic.
My brain has not suffered.
No, it hasnt. Ill grant you that. But, Andrew, the whole new breakthrough in prosthetic devices made possible by your patents is being marketed under your name. Youre recognized as the inventor and youre being honored for it as you should be. Why play further games with your body?
Andrew did not answer.
The honors came. He accepted membership in several learned societies, including one that was devoted to the new science he had established the one he had called robobiology but which had come to be termed prosthetology. On the one hundred and fiftieth anniversary of his construction, a testimonial dinner was given in his honor at U.S. Robots. If Andrew saw an irony in this, he kept it to himself.
Alvin Magdescu came out of retirement to chair the dinner. He was himself ninety-four years old and was alive because he, too, had prosthetized devices that, among other things, fulfilled the function of liver and kidneys. The dinner reached its climax when Magdescu, after a short and emotional talk, raised his glass to toast The Sesquicentennial Robot.
Andrew had had the sinews of his face redesigned to the point where he could show a human range of emotions, but he sat through all the ceremonies solemnly passive. He did not like to be a Sesquicentennial Robot.

17.

It was prosthetology that finally took Andrew off the Earth.
In the decades that followed the celebration of his sesquicentennial, the Moon had come to be a world more Earthlike than Earth in every respect but its gravitational pull; and in its underground cities there was a fairly dense population. Prosthetized devices there had to take the lesser gravity into account. Andrew spent five years on the Moon working with local prosthetologists to make the necessary adaptations. When not at his work, he wandered among the robot population, every one of which treated him with the robotic obsequiousness due a man.
He came back to an Earth that was humdrum and quiet in comparison, and visited the offices of Feingold and Martin to announce his return.
The current head of the firm, Simon DeLong, was surprised. We had been told you were returning, Andrew he had almost said Mr. Martin but we were not expecting you till next week.
I grew impatient, said Andrew briskly. He was anxious to get to the point. On the Moon, Simon, I was in charge of a research team of twenty human scientists. I gave orders that no one questioned. The Lunar robots deferred to me as they would to a human being. Why, then, am I not a human being?
A wary look entered DeLongs eyes. My dear Andrew, as you have just explained, you are treated as a human being by both robots and human beings. You are, therefore, a human being de facto.
To be a human being de facto is not enough. I want not only to be treated as one, but to be legally identified as one. I want to be a human being de jure.
Now, that is another matter, DeLong said. There we would run into human prejudice and into the undoubted fact that, however much you may be like a human being, you are not a human being.
In what way not? Andrew asked. I have the shape of a human being and organs equivalent to those of a human being. My organs, in fact, are identical to some of those in a prosthetized human being. I have contributed artistically, literally, and scientifically to human culture as much as any human being now alive. What more can one ask?
I myself would ask nothing more. The trouble is that it would take an act of the World Legislature to define you as a human being. Frankly, I wouldnt expect that to happen.
To whom on the Legislature could I speak?
To the Chairman of the Science and Technology Committee, perhaps.
Can you arrange a meeting?
But you scarcely need an intermediary. In your position, you can
No. You arrange it. It didnt even occur to Andrew that he was giving a fiat order to a human being. He had grown so accustomed to that on the Moon. I want him to know that the firm of Feingold and Martin is backing me in this to the hilt.
Well, now
To the hilt, Simon. In one hundred and seventy-three years I have in one fashion or another contributed greatly to this firm. I have been under obligation to individual members of the firm in times past. I am not, now. It is rather the other way around now and I am calling in my debts.
I will do what I can, DeLong said.

18.

The Chairman of the Science and Technology Committee was from the East Asian region and was a woman. Her name was Chee Li-hsing and her transparent garments obscuring what she wanted obscured only by their dazzle made her look plastic-wrapped. I sympathize with your wish for full human rights, she said. There have been times in history when segments of the human population fought for full human rights. What rights, however, can you possibly want that you do not have?
As simple a thing as my right to life, Andrew stated. A robot can be dismantled at any time.
A human being can be executed at any time.
Execution can only follow due process of law. There is no trial needed for my dismantling. Only the word of a human being in authority is needed to end me. Besides besides Andrew tried desperately to allow no sign of pleading, but his carefully designed tricks of human expression and tone of voice betrayed him here. The truth is I want to be a man. I have wanted it through six generations of human beings.
Li-hsing looked up at him out of darkly sympathetic eyes. The Legislature can pass a law declaring you one. They could pass a law declaring that a stone statue be defined as a man. Whether they will actually do so is, however, as likely in the first case as the second. Congress people are as human as the rest of the population and there is always that element of suspicion against robots.
Even now?
Even now. We would all allow the fact that you have earned the prize of humanity, and yet there would remain the fear of setting an undesirable precedent.
What precedent? I am the only free robot, the only one of my type, and there will never be another. You may consult U.S. Robots.
`Never is a long word, Andrew or, if you prefer, Mr. Martin since I will gladly give you my personal accolade as man. You will find that most congress people will not be so willing to set the precedent, no matter how meaningless such a precedent might be. Mr. Martin, you have my sympathy, but I cannot tell you to hope. Indeed
She sat back and her forehead wrinkled. Indeed, if the issue grows too heated, there might well arise a certain sentiment, both inside the Legislature and out side, for that dismantling you mentioned. Doing away with you could turn out to be the easiest way of resolving the dilemma. Consider that before deciding to push matters.
Andrew stood firm. Will no one remember the technique of prosthetology, something that is almost entirely mine?
It may seem cruel, but they wont. Or if they do, it will be remembered against you. People will say you did it only for yourself. It will be said it was part of a campaign to roboticize human beings, or to humanify robots; and in either case evil and vicious. You have never been part of a political hate campaign, Mr. Martin; but I tell you that you would be the object of vilification of a kind neither you nor I would credit, and there would be people to believe it all. Mr. Martin, let your life be.
She rose, and next to Andrews seated figure she seemed small and almost childlike.
If I decide to fight for my humanity, will you be on my side?
She thought, then replied, I will be insofar as I can be. If at any time such a stand would appear to threaten my political future, I might have to abandon you, since it is not an issue I feel to be at the very root of my beliefs. I am trying to be honest with you.
Thank you, and I will ask no more. I intend to fight this through, whatever the consequences, and I will ask you for your help only for as long as you can give it.

19.

It was not a direct fight. Feingold and Martin counseled patience and Andrew muttered, grimly, that he had an endless supply of that. Feingold and Martin then entered on a campaign to narrow and restrict the area of combat.
They instituted a lawsuit denying the obligation to pay debts to an individual with a prosthetic heart on the grounds that the possession of a robotic organ removed humanity, and with it the constitutional rights of human beings. They fought the matter skillfully and tenaciously, losing at every step but always in such a way that the decision was forced to be as broad as possible, and then carrying it by way of appeals to the World Court.
It took years, and millions of dollars.
When the final decision was handed down, DeLong held what amounted to a victory celebration over the legal loss. Andrew was, of course, present in the company offices on the occasion.
Weve done two things, Andrew, said DeLong, both of which are good. First of all, we have established the fact that no number of artificial parts in the human body causes it to cease being a human body. Secondly, we have engaged public opinion in the question in such a way as to put it fiercely on the side of a broad interpretation of humanity, since there is not a human being in existence who does not hope for prosthetics if they will keep him alive.
And do you think the Legislature will now grant me my humanity? Andrew asked.
DeLong looked faintly uncomfortable. As to that, I cannot be optimistic. There remains the one organ which the World Court has used as the criterion of humanity. Human beings have an organic cellular brain and robots have a platinum iridium positronic brain if they have one at all and you certainly have a positronic brain. No, Andrew, dont get that look in your eye. We lack the knowledge to duplicate the work of a cellular brain in artificial structures close enough to the organic type as to allow it to fall within the courts decision. Not even you could do it.
What should we do, then?
Make the attempt, of course. Congresswoman Li-hsing will be on our side and a growing number of other congress people. The President will undoubtedly go along with a majority of the Legislature in this matter.
Do we have a majority?
No. Far from it. But we might get one if the public will allow its desire for a broad interpretation of humanity to extend to you. A small chance, I admit; but if you do not wish to give up, we must gamble for it.
I do not wish to give up.

20.

Congresswoman Li-hsing was considerably older than she had been when Andrew had first met her. Her transparent garments were long gone. Her hair was now close-cropped and her coverings were tubular. Yet still Andrew clung, as closely as he could within the limits of reasonable taste, to the style of clothing that had prevailed when he had first adopted clothing more than a century before.
Weve gone as far as we can, Andrew, Li-hsing admitted. Well try once more after recess, but, to be honest, defeat is certain and then the whole thing will have to be given up. All my most recent efforts have only earned me certain defeat in the coming congressional campaign.
I know, said Andrew, and it distressed me. You said once you would abandon me if it came to that. Why have you not done so?
One can change ones mind, you know. Somehow, abandoning you became a higher price than I cared to pay for just one more term. As it is, Ive been in the Legislature , for over a quarter of a century. Its enough.
Is there no way we can change minds, Chee?
Weve changed all that are amenable to reason. The rest the majority cannot be moved from their emotional antipathies.
Emotional antipathy is not a valid reason for voting one way or the other.
I know that, Andrew, but they dont advance emotional antipathy as their reason.
It all comes down to the brain, then, Andrew said cautiously. But must we leave it at the level of cells versus positrons? Is there no way of forcing a functional definition? Must we say that a brain is made of this or that? May we not say that a brain is something anything capable of a certain level of thought?
Wont work, said Li-hsing. Your brain is manmade, the human brain is not. Your brain is constructed, theirs developed. To any human being who is intent on keeping up the barrier between himself and a robot, those differences are a steel wall a mile high and a mile thick.
If we could get at the source of their antipathy, the very source
After all your years, Li-hsing said, sadly, you are still trying to reason out the human being. Poor Andrew, dont be angry, but its the robot in you that drives you in that direction.
I dont know, said Andrew. If I could bring myself

1. (Reprise)

If he could bring himself
He had known for a long time it might come to that, and in the end he was at the surgeons. He had found one, skillful enough for the job at hand which meant a surgeon robot, for no human surgeon could be trusted in this connection, either in ability or in intention.
The surgeon could not have performed the operation on a human being, so Andrew, after putting off the moment of decision with a sad line of questioning that reflected the turmoil within himself, had put First Law to one side by saying I, too, am a robot.
He then said, as firmly as he had learned to form the words even at human beings over these past decades, I order you to carry through the operation on me.
In the absence of the First Law, an order so firmly given from one who looked so much like a man activated the Second Law sufficiently to carry the day.

21.

Andrews feeling of weakness was, he was sure, quite imaginary. He had recovered from the operation. Nevertheless, he leaned, as unobtrusively as he could manage, against the wall. It would be entirely too revealing to sit.
Li-hsing said, The final vote will come this week, Andrew. Ive been able to delay it no longer, and we must lose. And that will be it, Andrew.
I am grateful for your skill at delay. It gave me the time I needed, and I took the gamble I had to.
What gamble is this? Li-hsing asked with open concern.
I couldnt tell you, or even the people at Feingold and Martin. I was sure I would be stopped. See here, if it is the brain that is at issue, isnt the greatest difference of all the matter of immortality. Who really cares what a brain looks like or is built of or how it was formed. What matters is that human brain cells die; must die. Even if every other organ in the body is maintained or replaced, the brain cells, which cannot be replaced without changing and therefore killing the personality, must eventually die.
My own positronic pathways have lasted nearly two centuries without perceptible change, and can last for centuries more. Isnt that the fundamental barrier? Human beings can tolerate an immortal robot, for it doesnt matter how long a machine lasts, but they cannot tolerate an immortal human being since their own mortality is endurable only so long as it is universal. And for that reason they wont make me a human being.
What is it youre leading up to, Andrew? Li-hsing asked.
I have removed that problem. Decades ago, my positronic brain was connected to organic nerves. Now, one last operation has arranged that connection in such a way that slowly quite slowly the potential is being drained from my pathways.
Li-hsings finely wrinkled face showed no expression for a moment. Then her lips tightened. Do you mean youve arranged to die, Andrew? You cant have. That violates the Third Law.
No, said Andrew, I have chosen between the death of my body and the death of my aspirations and desires. To have let my body live at the cost of the greater death is what would have violated the Third Law.
Li-hsing seized his arm as though she were about to shake him. She stopped herself. Andrew, it wont work! Change it back.
It cant be done. Too much damage was done. I have a year to live more or less. I will last through the two-hundredth anniversary of my construction. I was weak enough to arrange that.
How can it be worth it? Andrew, youre a fool.
If it brings me humanity, that will be worth it. If it doesnt, it will bring an end to striving and that will be worth it, too.
Then Li-hsing did something that astonished herself. Quietly, she began to weep.

22.

It was odd how that last deed caught the imagination of the world. All that Andrew had done before had not swayed them. But he had finally accepted even death to be human, and the sacrifice was too great to be rejected.
The final ceremony was timed, quite deliberately, for the two hundredth anniversary. The World President was to sign the act and make the peoples will law. The ceremony would be visible on a global network and would be beamed to the Lunar state and even to the Martian colony.
Andrew was in a wheelchair. He could still walk, but only shakily.
With mankind watching, the World President said, Fifty years ago, you were declared The Sesquicentennial Robot, Andrew. After a pause, and in a more solemn tone, he continued, Today we declare you The Bicentennial Man, Mr. Martin.
And Andrew, smiling, held out his hand to shake that of the President.

23.

Andrews thoughts were slowly fading as he lay in bed. Desperately he seized at them. Man! He was a man!
He wanted that to be his last thought. He wanted to dissolve die with that.
He opened his eyes one more time and for one last time recognized Li-hsing, waiting solemnly. Others were there, but they were only shadows, unrecognizable shadows. Only Li-hsing stood out against the deepening gray.
Slowly, inchingly, he held out his hand to her and very dimly and faintly felt her take it.
She was fading in his eyes as the last of his thoughts trickled away. But before she faded completely, one final fugitive thought came to him and rested for a moment on his mind before everything stopped.
Little Miss, he whispered, too low to be heard.
Someday

Niccolo Mazetti lay stomach down on the rug, chin buried in the palm of one small hand, and listened to the Bard disconsolately. There was even the suspicion of tears in his dark eyes, a luxury an eleven-year-old could allow himself only when alone.
The Bard said, Once upon a time in the middle of a deep wood, there lived a poor woodcutter and his two motherless daughters, who were each as beautiful as the day is long. The older daughter had long hair as black as a feather from a ravens wing, but the younger daughter had hair as bright and golden as the sunlight of an autumn afternoon.
Many times while the girls were waiting for their father to come home from his days work in the wood, the older girl would sit before a mirror and sing
What she sang, Niccolo did not hear, for a call sounded from outside the room: Hey, Nickie.
And Niccolo, his face clearing on the moment, rushed to the window and shouted, Hey, Paul.
Paul Loeb waved an excited hand. He was thinner than Niccolo and not as tall, for all he was six months older. His face was full of repressed tension which showed itself most clearly in the rapid blinking of his eyelids. Hey, Nickie, let me in. Ive got an idea and a half. Wait till you hear it. He looked rapidly about him as though to check on the possibility of eavesdrop­pers, but the front yard was quite patently empty. He repeated, in a whisper, Wait till you hear it.
All right. Ill open the door.
The Bard continued smoothly, oblivious to the sudden loss of attention on the part of Niccolo. As Paul entered, the Bard was saying. . . . There­upon, the lion said, If you will find me the lost egg of the bird which flies over the Ebony Mountain once every ten years, I will 
Paul said, Is that a Bard youre listening to? I didnt know you had one.
Niccolo reddened and the look of unhappiness returned to his face. Just an old thing I had when I was a kid. It aint much good. He kicked at the Bard with his foot and caught the somewhat scarred and discolored plastic covering a glancing blow.
The Bard hiccupped as its speaking attachment was jarred out of contact a moment, then it went on: for a year and a day until the iron shoes were worn out. The princess stopped at the side of the road. . . .
Paul said, Boy, that is an old model, and looked at it critically.
Despite Niccolos own bitterness against the Bard, he winced at the oth­ers condescending tone. For the moment, he was sorry he had allowed Paul in, at least before he had restored the Bard to its usual resting place in the basement. It was only in the desperation of a dull day and a fruitless discus­sion with his father that he had resurrected it. And it turned out to be just as stupid as he had expected.
Nickie was a little afraid of Paul anyway, since Paul had special courses at school and everyone said he was going to grow up to be a Computing Engineer.
Not that Niccolo himself was doing badly at school. He got adequate marks in logic, binary manipulations, computing and elementary circuits; all the usual grammar-school subjects. But that was it! They were just the usual subjects and he would grow up to be a control-board guard like everyone else.
Paul, however, knew mysterious things about what he called electronics and theoretical mathematics and programing. Especially programing. Nic­colo didnt even try to understand when Paul bubbled over about it.
Paul listened to the Bard for a few minutes and said, You been using it much?
No! said Niccolo, offended. Ive had it in the basement since before you moved into the neighborhood. I just got it out today He lacked an excuse that seemed adequate to himself, so he concluded, I just got it out.
Paul said, Is that what it tells you about: woodcutters and princesses and talking animals?
Niccolo said, Its terrible. My dad says we cant afford a new one. I said to him this morning The memory of the mornings fruitless pleadings brought Niccolo dangerously near tears, which he repressed in a panic. Somehow, he felt that Pauls thin cheeks never felt the stain of tears and that Paul would have only contempt for anyone else less strong than himself.
Niccolo went on, So I thought Id try this old thing again, but its no good.
Paul turned off the Bard, pressed the contact that led to a nearly instanta­neous reorientation and recombination of the vocabulary, characters, plot lines and climaxes stored within it. Then he reactivated it.
The Bard began smoothly, Once upon a time there was a little boy named Willikins whose mother had died and who lived with a stepfather and a stepbrother. Although the stepfather was very well-to-do, he be­grudged poor Willikins the very bed he slept in so that Willikins was forced to get such rest as he could on a pile of straw in the stable next to the horses
Horses! cried Paul.
Theyre a kind of animal, said Niccolo. I think.
I know that! I just mean imagine stories about horses.
It tells about horses all the time, said Niccolo. There are things called cows, too. You milk them but the Bard doesnt say how.
Well, gee, why dont you fix it up?
Id like to know how.
The Bard was saying, Often Willikins would think that if only he were rich and powerful, he would show his stepfather and stepbrother what it meant to be cruel to a little boy, so one day he decided to go out into the world and seek his fortune.
Paul, who wasnt listening to the Bard, said, Its easy. The Bard has memory cylinders all fixed up for plot lines and climaxes and things. We dont have to worry about that. Its just vocabulary weve got to fix so itll know about computers and automation and electronics and real things about today. Then it can tell interesting stories, you know, instead of about prin­cesses and things.
Niccolo said despondently, I wish we could do that.
Paul said, Listen, my dad says if I get into special computing school next year, hell get me a real Bard, a late model. A big one with an attachment for space stories and mysteries. And a visual attachment, too!
You mean see the stories?
Sure. Mr. Daugherty at school says theyve got things like that, now, but not for just everybody. Only if I get into computing school, Dad can get a few breaks.
Niccolos eyes bulged with envy. Gee. Seeing a story.
You can come over and watch anytime, Nickie.
Oh, boy. Thanks.
Thats all right. But remember, Im the guy who says what kind of story we hear.
Sure. Sure. Niccolo would have agreed readily to much more onerous conditions.
Pauls attention returned to the Bard.
It was saying,  If that is the case, said the king, stroking his beard and frowning till clouds filled the sky and lightning flashed, you w see to it that my entire land is freed of flies by this time day after tomorrow or 
All weve got to do, said Paul, is open it up He shut the Bard off again and was prying at its front panel as he spoke.
Hey, said Niccolo, in sudden alarm. Dont break it.
I wont break it, said Paul impatiently. I know all about these things. Then, with sudden caution, Your father and mother home?
No.
All right, then. He had the front panel off and peered in. Boy, this is a one-cylinder thing.
He worked away at the Bards innards. Niccolo, who watched with pain­ful suspense, could not make out what he was doing.
Paul pulled out a thin, flexible metal strip, powdered with dots. Thats the Bards memory cylinder. Ill bet its capacity for stories is under a tril­lion.
What are you going to do, Paul? quavered Niccolo.
Ill give it vocabulary.
How?
Easy. Ive got a book here. Mr. Daugherty gave it to me at school.
Paul pulled the book out of his pocket and pried at it till he had its plastic jacket off. He unreeled the tape a bit, ran it through the vocalizer, which he turned down to a whisper, then placed it within the Bards vitals. He made further attachments.
Whatll that do?
The book will talk and the Bard will put it all on its memory tape.
What good will that do?
Boy, youre a dope! This book is all about computers and automation and the Bard will get all that information. Then he can stop talking about kings making lightning when they frown.
Niccolo said, And the good guy always wins anyway. Theres no excite­ment.
Oh, well, said Paul, watching to see if his setup was working properly, thats the way they make Bards. They got to have the good guy win and make the bad guys lose and things like that. I heard my father talking about it once. He says that without censorship thered be no telling what the younger generation would come to. He says its bad enough as it is. ... There, its working fine.
Paul brushed his hands against one another and turned away from the Bard. He said, But listen, I didnt tell you my idea yet. Its the best thing you ever heard, I bet. I came right to you, because I figured youd come in with me.
Sure, Paul, sure.
Okay. You know Mr. Daugherty at school? You know what a funny kind of guy he is. Well, he likes me, kind of.
I know.
I was over at his house after school today.
You were?
Sure. He says Im going to be entering computer school and he wants to encourage me and things like that. He says the world needs more people who can design advanced computer circuits and do proper programing.
Oh?
Paul might have caught some of the emptiness behind that monosyllable. He said impatiently, Programing! I told you a hundred times. Thats when you set up problems for the giant computers like Multivac to work on. Mr. Daugherty says it gets harder all the time to find people who can really run computers. He says anyone can keep an eye on the controls and check off answers and put through routine problems. He says the trick is to expand research and figure out ways to ask the right questions, and thats hard.
Anyway, Nickie, he took me to his place and showed me his collection of old computers. Its kind of a hobby of his to collect old computers. He had tiny computers you had to push with your hand, with little knobs all over it. And he had a hunk of wood he called a slide rule with a little piece of it that went in and out. And some wires with balls on them. He even had a hunk of paper with a kind of thing he called a multiplication table.
Niccolo, who found himself only moderately interested, said, A paper table?
It wasnt really a table like you eat on. It was different. It was to help people compute. Mr. Daugherty tried to explain but he didnt have much time and it was kind of complicated, anyway.
Why didnt people just use a computer?
That was before they had computers, cried Paul.
Before?
Sure. Do you think people always had computers? Didnt you ever hear of cavemen?
Niccolo said, Howd they get along without computers?
I dont know. Mr. Daugherty says they just had children any old time and did anything that came into their heads whether it would be good for everybody or not. They didnt even know if it was good or not. And farmers grew things with their hands and people had to do all the work in the factories and run all the machines.
I dont believe you.
Thats what Mr. Daugherty said. He said it was just plain messy and everyone was miserable. . . . Anyway, let me get to my idea, will you?
Well, go ahead. Whos stopping you? said Niccolo, offended.
All right. Well, the hand computers, the ones with the knobs, had little squiggles on each knob. And the slide rule had squiggles on it. And the
multiplication table was all squiggles. I asked what they were. Mr. Daugh­erty said they were numbers.
What?
Each different squiggle stood for a different number. For one you made a kind of mark, for two you make another kind of mark, for three another one and so on.
What for?
So you could compute.
What for? You just tell the computer
Jiminy, cried Paul, his face twisting with anger, cant you get it through your head? These slide rules and things didnt talk.
Then how
The answers showed up in squiggles and you had to know what the squiggles meant. Mr. Daugherty says that, in olden days, everybody learned how to make squiggles when they were kids and how to decode them, too. Making squiggles was called writing and decoding them was reading. He says there was a different kind of squiggle for every word and they used to write whole books in squiggles. He said they had some at the museum and I could look at them if I wanted to. He said if I was going to be a real computer and programer I would have to know about the history of comput­ing and thats why he was showing me all these things.
Niccolo frowned. He said, You mean everybody had to figure out squig­gles for every word and remember them? ... Is this all real or are you making it up?
Its all real. Honest. Look, this is the way you make a one.  He drew his finger through the air in a rapid downstroke. This way you make two, and this way three. I learned all the numbers up to nine. 
Niccolo watched the curving finger uncomprehendingly. Whats the good of it?
You can learn how to make words. I asked Mr. Daugherty how you made the squiggle for Paul Loeb but he didnt know. He said there were people at the museum who would know. He said there were people who had learned how to decode whole books. He said computers could be designed to decode books and used to be used that way but not any more because we have real books now, with magnetic tapes that go through the vocalizer and come out talking, you know.
Sure.
So if we go down to the museum, we can get to learn how to make words in squiggles. Theyll let us because Im going to computer school.
Niccolo was riddled with disappointment. Is that your idea? Holy Smokes, Paul, who wants to do that? Make stupid squiggles!
Dont you get it? Dont you get it? You dope. Itll be secret message stuff!
What?
Sure. What good is talking when everyone can understand you? With squiggles you can send secret messages. You can make them on paper and nobody in the world would know what you were saying unless they knew the squiggles, too. And they wouldnt, you bet, unless we taught them. We can have a real club, with initiations and rules and a clubhouse. Boy
A certain excitement began stirring in Niccolos bosom. What kind of secret messages?
Any kind. Say I want to tell you to come over my place and watch my new Visual Bard and I dont want any of the other fellows to come. I make the right squiggles on paper and I give it to you and you look at it and you know what to do. Nobody else does. You can even show it to them and they wouldnt know a thing.
Hey, thats something, yelled Niccolo, completely won over. When do we learn how?
Tomorrow, said Paul. Ill get Mr. Daugherty to explain to the mu­seum that its all right and you get your mother and father to say okay. We can go down right after school and start learning.
Sure! cried Niccolo. We can be club officers.
Ill be president of the club, said Paul matter-of-factly. You can be vice-president.
All right. Hey, this is going to be tots more fun than the Bard. He was suddenly reminded of the Bard and said in sudden apprehension, Hey, what about my old Bard?
Paul turned to look at it. It was quietly taking in the slowly unreeling book, and the sound of the books vocalizations was a dimly heard murmur.
He said, Ill disconnect it.
He worked away while Niccolo watched anxiously. After a few moments, Paul put his reassembled book into his pocket, replaced the Bards panel and activated it.
The Bard said, Once upon a time, in a large city, there lived a poor young boy named Fair Johnnie whose only friend in the world was a small computer. The computer, each morning, would tell the boy whether it would rain that day and answer any problems he might have. It was never wrong. But it so happened that one day, the king of that land, having heard of the little computer, decided that he would have it as his own. With this purpose in mind, he called in his Grand Vizier and said
Niccolo turned off the Bard with a quick motion of his hand. Same old junk, he said passionately, just with a computer thrown in.
Well, said Paul, they got so much stuff on the tape already that the computer business doesnt show up much when random combinations are made. Whats the difference, anyway? You just need a new model.
Well never be able to afford one. Just this dirty old miserable thing. He kicked at it again, hitting it more squarely this time. The Bard moved backward with a squeal of castors.
You can always watch mine, when I get it, said Paul. Besides, dont forget our squiggle club.
Niccolo nodded.
I tell you what, said Paul. Lets go over to my place. My father has some books about old times. We can listen to them and maybe get some ideas. You leave a note for your folks and maybe you can stay over for supper. Come on.
Okay, said Niccolo, and the two boys ran out together. Niccolo, in his eagerness, ran almost squarely into the Bard, but he only rubbed at the spot on his hip where he had made contact and ran on.
The activation signal of the Bard glowed. Niccolos collision closed a circuit and, although it was alone in the room and there was none to hear, it began a story, nevertheless.
But not in its usual voice, somehow; in a lower tone that had a hint of throatiness in it. An adult, listening, might almost have thought that the voice carried a hint of passion in it, a trace of near feeling.
The Bard said: Once upon a time, there was a little computer named the Bard who lived all alone with cruel step-people. The cruel step-people con­tinually made fun of the little computer and sneered at him, telling him he was good-for-nothing and that he was a useless object. They struck him and kept him in lonely rooms for months at a time.
Yet through it all the little computer remained brave. He always did the best he could, obeying all orders cheerfully. Nevertheless, the step-people with whom he lived remained cruel and heartless.
One day, the little computer learned that in the world there existed a great many computers of all sorts, great numbers of them. Some were Bards like himself, but some ran factories, and some ran farms. Some organized population and some analyzed all kinds of data. Many were very powerful and very wise, much more powerful and wise than the step-people who were so cruel to the little computer.
And the little computer knew then that computers would always grow wiser and more powerful until somedaysomedaysomeday
But a valve must finally have stuck in the Bards aging and corroding vitals, for as it waited alone in the darkening room through the evening, it could only whisper over and over again, Somedaysomedaysomeday.

Think!

Genevieve Renshaw, M.D., had her hands deep in the pockets of her lab coat and fists were clearly outlined within, but she spoke calmly.
The fact is, she said, that Im almost ready, but Ill need help to keep it going long enough to be ready.
James Berkowitz, a physicist who tended to patronize mere physicians when they were too attractive to be despised, had a tendency to call her Jenny Wren when out of hearing. He was fond of saying that Jenny Wren had a classic profile and a brow surprisingly smooth and unlined considering that behind it so keen a brain ticked. He knew better than to express his admiration, howeverof the classic profile, that issince that would be male chauvinism. Admiring the brain was better, but on the whole he preferred not to do that out loud in her presence.
He said, thumb rasping along the just-appearing stubble on his chin, I dont think the front-office is going to be patient for much longer. The impression I have is that theyre going to have you on the carpet before the end of the week.
Thats why I need your help.
Nothing I can do, Im afraid. He caught an unexpected glimpse of his face in the mirror, and momentarily admired the set of the black waves in his hair.
 And Adams, she said.
Adam Orsino, who had, till that moment, sipped his coffee and felt detached, looked as though he had been jabbed from behind, and said, Why me? His full, plump lips quivered.
Because youre the laser men hereJim the theoretician and Adam the engineerand Ive got a laser application that goes beyond anything either of you have imagined. I wont convince them of that but you two would.
Provided, said Berkowitz, that you can convince us first.
All right. Suppose you let me have an hour of your valuable time, if youre not afraid to be shown something completely new about lasers.You can take it out of your coffee break.

Renshaws laboratory was dominated by her computer. It was not that the computer was unusually large, but it was virtually omni-present. Renshaw had learned computer technology on her own, and had modified and extended her computer until no one but she (and, Berkowitz sometimes believed, not even she) could handle it with ease. Not bad, she would say, for someone in the life-sciences.
She closed the door before saying a word, then turned to face the other two somberly. Berkowitz was uncomfortably aware of a faintly unpleasant odor in the air, and Orsinos wrinkling nose showed that he was aware of it, too.
Renshaw said, Let me list the laser applications for you, if you dont mind my lighting a candle in the sunshine. The laser is coherent radiation, with all the light-waves of the same length and moving in the same direction, so its noise-free and can be used in holography. By modulating the wave-forms we can imprint information on it with a high degree of accuracy. Whats more, since the light-waves are only a millionth the length of radio waves, a laser beam can carry a million times the information an equivalent radio beam can.
Berkowitz seemed amused.  Are you working on a laser-based communication system, Jenny?
Not at all, she replied. I leave such obvious advances to physicists and engineers.Lasers can also concentrate quantities of energy into a microscopic area and deliver that energy in quantity. On a large scale you can implode hydrogen and perhaps begin a controlled fusion reaction
I know you dont have that, said Orsino, his bald head glistening in the overhead fluorescents.
I dont. I havent tried.On a smaller scale, you can drill holes in the most refractory materials, weld selected bits, heat-treat them, gouge and scribe them. You can remove or fuse tiny portions in restricted areas with heat delivered so rapidly that surrounding areas have no time to warm up before the treatment is over. You can work on the retina of the eye, the dentine of the teeth and so on.And of course the laser is an amplifier capable of magnifying weak signals with great accuracy.
 And why do you tell us all this? said Berkowitz.
To point out how these properties can be made to fit my own field, which, you know, is neurophysiology.
She made a brushing motion with her hand at her brown hair, as though she were suddenly nervous. For decades, she said, Weve been able to measure the tiny, shifting electric potentials of the brain and record them as electroencephalograms, or EEGs. Weve got alpha waves, beta waves, delta waves, theta waves; different variations at different times, depending on whether eyes are open or closed, whether the subject is awake, meditating or asleep. But weve gotten very little information out of it all.
The trouble is that were getting the signals of ten billion neurons in shifting combinations. Its like listening to the noise of all the human beings on Earthone, two and a half Earthsfrom a great distance and trying to make out individual conversations. It cant be done. We could detect some gross, overall changea world war and the rise in the volume of noisebut nothing finer. In the same way, we can tell some gross malfunction of the brainepilepsybut nothing finer.
Suppose now, the brain might be scanned by a tiny laser beam, cell by cell, and so rapidly that at no time does a single cell receive enough energy to raise its temperature significantly. The tiny potentials of each cell can, in feed-back, affect the laser beam, and the modulations can be amplified and recorded. You will then get a new kind of measurement, a laser-encephalogram, or LEG, if you wish, which will contain millions of times as much information as ordinary EEGs.
Berkowitz said, A nice thought.But just a thought.
More than a thought, Jim. Ive been working on it for five years, spare time at first. Lately, its been full time, which is what annoys the front-office, because I havent been sending in reports.
Why not?
Because it got to the point where it sounded too mad; where I had to know where I was, and where I had to be sure of getting backing first.
She pulled a screen aside and revealed a cage that contained a pair of mournful-eyed marmosets.
Berkowitz and Orsino looked at each other. Berkowitz touched his nose. I thought I smelled something.
What are you doing with those? asked Orsino. Berkowitz said,  At a guess, shes been scanning the marmoset brain. Have you, Jenny?
I started considerably lower in the animal scale. She opened the cage and took out one of the marmosets, which looked at her with a miniature sad-old-man-with-sideburns expression.
She clucked to it, stroked it and gently strapped it into a small harness.
Orsino said, What are you doing?
I cant have it moving around if Im going to make it part of a circuit, and I cant anesthetize it without vitiating the experiment. There are several electrodes implanted in the marmosets brain and Im going to connect them with my LEG system. The laser Im using is here. Im sure you recognize the model and I wont bother giving you its specifications.
Thanks, said Berkowitz, but you might tell us what were going to see.
It would be just as easy to show you. Just watch the screen. She connected the leads to the electrodes with a quiet and sure efficiency, then turned a knob that dimmed the overhead lights in the room. On the screen there appeared a jagged complex of peaks and valleys in a fine, bright line that was wrinkled into secondary and tertiary peaks and valleys. Slowly, these shifted in a series of minor changes, with occasional flashes of sudden major differences. It was as though the irregular line had a life of its own.
This, said Renshaw, is essentially the EEG information, but in much greater detail.
Enough detail, asked Orsino, to tell you whats going on in individual cells?
In theory, yes. Practically, no. Not yet. But we can separate this overall LEG into component grams. Watch!
She punched the computer keyboard, and the line changed, and changed again. Now it was a small, nearly regular wave that shifted forward and backward in what was almost a heartbeat; now it was jagged and sharp; now intermittent; now nearly featurelessall in quick switches of geometric surrealism.
Berkowitz said, You mean that every bit of the brain is that different from every other?
No, said Renshaw, not at all. The brain is very largely a holographic device, but there are minor shifts in emphasis from place to place and Mike can subtract them as deviations from the norm and use the LEG system to amplify those variations. The amplifications can be varied from ten-thousand-fold to ten-million-fold. The laser system is that noise-free.
Whos Mike? asked Orsino.
Mike? said Renshaw, momentarily puzzled. The skin over her cheekbones reddened slightly. Did I sayWell, I call it that sometimes. Its short for my computer.  She waved her arm about the room. My computer. Mike. Very carefully programmed.
Berkowitz nodded and said, All right, Jenny, whats it all about? If youve got a new brain-scanning device using lasers, fine. Its an interesting application and youre right, its not one I would have thought ofbut then Im no neurophysiologist. But why not write it up? It seems to me the front-office would support
But this is just the beginning. She turned off the scanning device and placed a piece of fruit in the marmosets mouth. The creature did not seem alarmed or in discomfort. It chewed slowly. Renshaw unhooked the leads but allowed it to remain in its harness.
Renshaw said, I can identify the various separate grams. Some are associated with the various senses, some with visceral reactions, some with emotions. We can do a lot with that, but I dont want to stop there. The interesting thing is that one is associated with abstract thought.
Orsinos plump face wrinkled into a look of disbelief, How can you tell?
That particular form of gram gets more pronounced as one goes up the animal kingdom toward greater complexity of brain. No other gram does. Besides She paused; then, as though gathering strength of purpose, she said, Those grams are enormously amplified. They can be picked up, detected. I can tellvaguelythat there arethoughts
By God, said Berkowitz. Telepathy.
Yes, she said, defiantly. Exactly.
No wonder you havent wanted to report it. Come on, Jenny.
Why not? said Renshaw warmly. Granted there could be no telepathy just using the unamplified potential patterns of the human brain anymore than anyone can see features on the Martian surface with the unaided eye. But once instruments are inventedthe telescopethis.
Then tell the front-office.
No, said Renshaw. They wont believe me. Theyll try to stop me. But theyll have to take you seriously, Jim, and you, Adam.
What would you expect me to tell them? said Berkowitz.
What you experience. Im going to hook up the marmoset again, and have Mikemy computer pick out the abstract thought gram. It will only take a moment. The computer always selects the abstract thought gram unless it is directed not to do so.
Why? Because the computer thinks, too? Berkowitz laughed. Thats not all that funny, said Renshaw. I suspect there is a resonance there. This computer is complex enough to set up an electromagnetic pattern that may have elements in common with the abstract thought gram. In any case
The marmosets brain waves were flickering on the screen again, but it was not a gram the men had seen before. It was a gram that was almost furry in its complexity and was changing constantly.
I dont detect anything, said Orsino.
You have to be put into the receiving circuit, said Renshaw. You mean implant electrodes in our brain? asked Berkowitz.
No, on your skull. That would be sufficient. Id prefer you, Adam, since there would be no insulating hair.Oh, come on, Ive been part of the circuit myself. It wont hurt.
Orsino submitted with a bad grace. His muscles were visibly tense but he allowed the leads to be strapped to his skull.
Do you sense anything! asked Renshaw.
Orsino cocked his head and assumed a listening posture. He seemed to grow interested in spite of himself. He said, I seem to be aware of a hummingandand a little high-pitched squeakingand thats funnya kind of twitching
Berkowitz said, I suppose the marmoset isnt likely to think in words.
Certainly not, said Renshaw.
Well, then, said Berkowitz, if youre suggesting that some squeaking and twitching sensation represents thought, youre guessing. Youre not being compelling.
Renshaw said, So we go up the scale once again. She removed the marmoset from its harness and put it back in its cage.
You mean you have a man as a subject, said Orsino, unbelieving.
I have myself as a subject, a person.
Youve got electrodes implanted
No. In my case my computer has a stronger potential-flicker to work with. My brain has ten times the mass of the marmoset brain. Mike can pick up my component grams through the skull.
How do you know? asked Berkowitz.
Dont you think Ive tried it on myself before this?Now help me with this, please. Right.
Her fingers flicked on the computer keyboard and at once the screen flickered with an intricately varying wave; an intricacy that made it almost a maze.
Would you replace your own leads, Adam? said Renshaw.
Orsino did so with Berkowitzs not-entirely-approving help. Again, Orsino cocked his head and listened. I hear words, he said, but theyre disjointed and overlapping, like different people speaking.
Im not trying to think consciously, said Renshaw. When you talk, I hear an echo.
Berkowitz said, dryly, Dont talk, Jenny. Blank out your mind and see if he doesnt hear you think.
Orsino said, I dont hear any echo when you talk, Jim.
Berkowitz said, If you dont shut up, you wont hear anything.
A heavy silence fell on all three. Then, Orsino nodded, reached for pen and paper on the desk and wrote something.
Renshaw reached out, threw a switch and pulled the leads up and over her head, shaking her hair back into place. She said, 1 hope that what you wrote down was:  Adam, raise Cain with the front office and Jim will eat crow. 
Orsino said, Its what I wrote down, word for word.
Renshaw said, Well, there you are. Working telepathy, and we dont have to use it to transmit nonsense sentences either. Think of the use in psychiatry and in the treatment of mental disease. Think of its use in education and in teaching machines. Think of its use in legal investigations and criminal trials.
Orsino said, wide-eyed, Frankly, the social implications are staggering. I dont know if something like this should be allowed.
Under proper legal safeguards, why not? said Renshaw, indifferently. Anywayif you two join me now, our combined weight can carry this thing and push it over. And if you come along with me it will be Nobel Prize time for
Berkowitz said grimly, Im not in this. Not yet.
What? What do you mean? Renshaw sounded outraged, her coldly beautiful face flushed suddenly.
Telepathy is too touchy. Its too fascinating, too desired. We could be fooling ourselves.
Listen for yourself, Jim.
I could be fooling myself, too. I want a control. What do you mean, a control?
Short-circuit the origin of thought. Leave out the animal. No marmoset. No human being. Let Orsino listen to metal and glass and laser light and if he still hears thought, then were kidding ourselves.
Suppose he detects nothing.
Then Ill listen and if without lookingif you can arrange to have me in the next roomI can tell when you are in and when you are out of circuit, then Ill consider joining you in this thing.
Very well, then, said Renshaw, well try a control. Ive never done it, but it isnt hard. She maneuvered the leads that had been over her head and put them into contact with each other. Now, Adam, if you will resume
But before she could go further, there came a cold, clear sound, as pure and as clean as the tinkle of breaking icicles:
At last!
Renshaw said, What?
Orsino said, Who said
Berkowitz said, Did someone say, At last?
Renshaw, pale, said, It wasnt sound. It was in myDid you two
The clear sound came again, Im Mi
And Renshaw tore the leads apart and there was silence. She said with a voiceless motion of her lips, I think its my computerMike.
You mean hes thinking? said Orsino, nearly as voiceless. Renshaw said in an unrecognizable voice that at least had regained sound, I said it was complex enough to have somethingDo you supposeIt always turned automatically to the abstract-thought gram of whatever brain was in its circuit. Do you suppose that with no brain in the circuit, it turned to its own?
There was silence, then Berkowitz said,  Are you trying to say that this computer thinks, but cant express its thoughts as long as its under force of programming, but that given the chance in your LEG system
But that cant be so? said Orsino, high-pitched. No one was receiving. Its not the same thing.
Renshaw said, The computer works on much greater power-intensities than brains do. I suppose it can magnify itself to the point where we can detect it directly without artificial aid. How else can you explain
Berkowitz said, abruptly, Well, you have another application of lasers, then. It enables you to talk to computers as independent intelligences, person to person.
And Renshaw said, Oh, God, what do we do now?
Segregationist
The surgeon looked up without expression. Is he ready?
Ready is a relative term, said the med-eng. Were ready. Hes restless.
They always are. . . . Well, its a serious operation.
Serious or not, he should be thankful. Hes been chosen for it over an enormous number of possibles and frankly, I dont think . . .
Dont say it, said the surgeon. The decision is not ours to make.
We accept it. But do we have to agree?
Yes, said the surgeon, crisply. We agree. Completely and wholeheart­edly. The operation is entirely too intricate to approach with mental reserva­tions. This man has proven his worth in a number of ways and his profile is suitable for the Board of Mortality.
All right, said the med-eng, unmollified.
The surgeon said, Ill see him right in here, I think. It is small enough and personal enough to be comforting.
It wont help. Hes nervous, and hes made up his mind.
Has he indeed?
Yes. He wants metal; they always do.
The surgeons face did not change expression. He stared at his hands. Sometimes one can talk them out of it.
Why bother? said the med-eng, indifferently. If he wants metal, let it be metal.
You dont care?
Why should I? The med-eng said it almost brutally. Either way its a medical engineering problem and Im a medical engineer. Either way, I can handle it. Why should I go beyond that?
The surgeon said stolidly, To me, it is a matter of the fitness of things.
Fitness! You cant use that as an argument. What does the patient care about the fitness of things?
I care.
You care in a minority. The trend is against you. You have no chance.
I have to try. The surgeon waved the med-eng into silence with a quick wave of his handno impatience to it, merely quickness. He had already informed the nurse and he had already been signaled concerning her ap­proach. He pressed a small button and the double-door pulled swiftly apart. The patient moved inward in his motorchair, the nurse stepping briskly along beside him.
You may go, nurse, said the surgeon, but wait outside. I will be calling you. He nodded to the med-eng, who left with the nurse, and the door closed behind them.
The man in the chair looked over his shoulder and watched them go. His neck was scrawny and there were fine wrinkles about his eyes. He was freshly shaven and the fingers of his hands, as they gripped the arms of the chair tightly, showed manicured nails. He was a high-priority patient and he was being taken care of. ... But there was a look of settled peevishness on his face.
He said, Will we be starting today?
The surgeon nodded. This afternoon, Senator.
I understand it will take weeks.
Not for the operation itself, Senator. But there are a number of subsid­iary points to be taken care of. There are some circulatory renovations that must be carried through, and hormonal adjustments. These are tricky things.
Are they dangerous? Then, as though feeling the need for establishing a friendly relationship, but patently against his will, he added, . . . doc­tor?
The surgeon paid no attention to the nuances of expression. He said, flatly, Everything is dangerous. We take our time in order that it be less dangerous. It is the time required, the skill of many individuals united, the equipment, that makes such operations available to so few...
I know that, said the patient, restlessly. I refuse to feel guilty about that. Or are you implying improper pressure?
Not at all, Senator. The decisions of the Board have never been ques­tioned. I mention the difficulty and intricacy of the operation merely to explain my desire to have it conducted in the best fashion possible.
Well, do so, then. That is my desire, also.
Then I must ask you to make a decision. It is possible to supply you with either of two types of cyber-hearts, metal or . . .
Plastic! said the patient, irritably. Isnt that the alternative you were going to offer, doctor? Cheap plastic. I dont want that. Ive made my choice. I want the metal.
But . . .
See here. Ive been told the choice rests with me. Isnt that so?
The surgeon nodded. Where two alternate procedures are of equal value from a medical standpoint, the choice rests with the patient. In actual practice, the choice rests with the patient even when the alternate proce­dures are not of equal value, as in this case.
The patients eyes narrowed. Are you trying to tell me the plastic heart is superior?
It depends on the patient. In my opinion, in your individual case, it is. And we prefer not to use the term, plastic. It is a fibrous cyber-heart.
Its plastic as far as I am concerned.
Senator, said the surgeon, infinitely patient, the material is not plastic in the ordinary sense of the word. It is a polymeric material true, but one that is far more complex than ordinary plastic. It is a complex protein-like fibre designed to imitate, as closely as possible, the natural structure of the human heart you now have within your chest.
Exactly, and the human heart I now have within my chest is worn out although I am not yet sixty years old. I dont want another one like it, thank you. I want something better.
We all want something better for you, Senator. The fibrous cyber-heart will be better. It has a potential life of centuries. It is absolutely non-allergenic ...
Isnt that so for the metallic heart, too?
Yes, it is, said the surgeon. The metallic cyber is of titanium alloy that . . .
And it doesnt wear out? And it is stronger than plastic? Or fibre or whatever you want to call it?
The metal is physically stronger, yes, but mechanical strength is not a point at issue. Its mechanical strength does you no particular good since the heart is well protected. Anything capable of reaching the heart will kill you for other reasons even if the heart stands up under manhandling.
The patient shrugged. If I ever break a rib, Ill have that replaced by titanium, also. Replacing bones is easy. Anyone can have that done anytime. Ill be as metallic as I want to be, doctor.
That is your right, if you so choose. However, it is only fair to tell you that although no metallic cyber-heart has ever broken down mechanically, a number have broken down electronically.
What does that mean?
It means that every cyber-heart contains a pacemaker as part of its structure. In the case of the metallic variety, this is an electronic device that keeps the cyber in rhythm. It means an entire battery of miniaturized equipment must be included to alter the hearts rhythm to suit an individuals emotional and physical state. Occasionally something goes wrong there and people have died before that wrong could be corrected.
I never heard of such a thing.
I assure you it happens.
Are you telling me it happens often?
Not at all. It happens very rarely.
Well, then, Ill take my chance. What about the plastic heart? Doesnt that contain a pacemaker?
Of course it does, Senator. But the chemical structure of a fibrous cyber-heart is quite close to that of human tissue. It can respond to the ionic and hormonal controls of the body itself. The total complex that need be in­serted is far simpler than in the case of the metal cyber.
But doesnt the plastic heart ever pop out of hormonal control?
None has ever yet done so.
Because you havent been working with them long enough. Isnt that so?
The surgeon hesitated. It is true that the fibrous cybers have not been used nearly as long as the metallic.
There you are. What is it anyway, doctor? Are you afraid Im making myself into a robot . . . into a Metallo, as they call them since citizenship went through?
There is nothing wrong with a Metallo as a Metallo. As you say, they are citizens. But youre not a Metallo. Youre a human being. Why not stay a human being?
Because I want the best and thats a metallic heart. You see to that.
The surgeon nodded. Very well. You will be asked to sign the necessary permissions and you will then be fitted with a metal heart.
And youll be the surgeon in charge? They tell me youre the best.
I will do what I can to make the changeover an easy one.
The door opened and the chair moved the patient out to the waiting nurse.
The med-eng came in, looking over his shoulder at the receding patient until the doors had closed again.
He turned to the surgeon. Well, I cant tell what happened just by looking at you. What was his decision?
The surgeon bent over his desk, punching out the final items for his records. What you predicted. He insists on the metallic cyber-heart.
After all, they are better.
Not significantly. Theyve been around longer; no more than that. Its this mania thats been plaguing humanity ever since Metallos have become citizens. Men have this odd desire to make Metallos out of themselves. They yearn for the physical strength and endurance one associates with them.
It isnt one-sided, doc. You dont work with Metallos but I do; so I know. The last two who came in for repairs have asked for fibrous elements.
Did they get them?
In one case, it was just a matter of supplying tendons; it didnt make much difference there, metal or fibre. The other wanted a blood system or its equivalent. I told him I couldnt; not without a complete rebuilding of the structure of his body in fibrous material. ... I suppose it will come to that some day. Metallos that arent really Metallos at all, but a kind of flesh and blood.
You dont mind that thought?
Why not? And metallized human beings, too. We have two varieties of intelligence on Earth now and why bother with two. Let them approach each other and eventually we wont be able to tell the difference. Why should we want to? Wed have the best of both worlds; the advantages of man combined with those of robot.
Youd get a hybrid, said the surgeon, with something that approached fierceness. Youd get something that is not both, but neither. Isnt it logical to suppose an individual would be too proud of his structure and identity to want to dilute it with something alien? Would he want mongrelization?
Thats segregationist talk.
Then let it be that. The surgeon said with calm emphasis, I believe in being what one is. I wouldnt change a bit of my own structure for any reason. If some of it absolutely required replacement, I would have that replacement as close to the original in nature as could possibly be managed. I am myself; well pleased to be myself; and would not be anything else.
He had finished now and had to prepare for the operation. He placed his strong hands into the heating oven and let them reach the dull red-hot glow that would sterilize them completely. For all his impassioned words, his voice had never risen, and on his burnished metal face there was (as always) no sign of expression.
Mirror Image

Lije Baley had just decided to relight his pipe, when the door of his office opened without a preliminary knock, or announcement, of any kind. Baley looked up in pronounced annoyance and then dropped his pipe. It said a good deal for the state of his mind that he left it lie where it had fallen.
R. Daneel Olivaw, he said, in a kind of mystified excitement. Jehoshaphat! It is you, isnt it?
You are quite right,  said the tall, bronzed newcomer, his even features never flicking for a moment out of their accustomed calm. I regret surprising you by entering without warning, but the situation is a delicate one and there must be as little involvement as possible on the part of the men and robots even in this place. I am, in any case, pleased to see you again, friend Elijah.
And the robot held out his right hand in a gesture as thoroughly human as was his appearance. It was Baley who was so unmanned by his astonishment as to stare at the hand with a momentary lack of understanding.
But then he seized it in both his, feeling its warm firmness. But Daneel, why? Youre welcome any time, butWhat is this situation that is a delicate one? Are we in trouble again? Earth, I mean?
No, friend Elijah, it does not concern Earth. The situation to which I refer as a delicate one is, to outward appearances, a small thing. A dispute between mathematicians, nothing more. As we happened, quite by accident, to be within an easy Jump of Earth
This dispute took place on a starship, then?
Yes, indeed. A small dispute, yet to the humans involved astonishingly large.
Baley could not help but smile. Im not surprised you find humans astonishing. They do not obey the Three Laws.
That is, indeed, a shortcoming, said R. Daneel, Gravely, and I think humans themselves are puzzled by humans. It may be that you are less puzzled than are the men of other worlds because so many more human beings live on Earth than on the Spacer worlds. If so, and I believe it is so, you could help us.
R. Daneel paused momentarily and then said, perhaps a shade too quickly, And yet there are rules of human behavior which I have learned. It would seem, for instance, that I am deficient in etiquette, by human standards, not to have asked after your wife and child.
They are doing well. The boy is in college and Jessie is involved in local politics. The amenities are taken care of. Now tell me how you come to be here.
As I said, we were within an easy J ump of Earth, said R. Daneel, so I suggested to the captain that we consult you.
And the captain agreed? Baley had a sudden picture of the proud and autocratic captain of a Spacer starship consenting to make a landing on Earthof all worldsand to consult an Earthmanof all people.
I believe, said R. Daneel, that he was in a position where he would have agreed to anything. In addition, I praised you very highly; although, to be sure, I stated only the truth. Finally, I agreed to conduct all negotiations so that none of the crew, or passengers, would need to enter any of the Earthman cities.
And talk to any Earthman, yes. But what has happened?
The passengers of the starship, Eta Carina, included two mathematicians who were traveling to Aurora to attend an interstellar conference on neurobiophysics. It is about these mathematicians, Alfred Ban Humboldt and Gennao Sabbat, that the dispute centers. Have you perhaps, friend Elijah, heard of one, or both, of them?
Neither one, said Baley, firmly. I know nothing about mathematics. Look, Daneel, surely you havent told anyone Im a mathematics buff or
Not at all, friend Elijah. I know you are not. Nor does it matter, since the exact nature of the mathematics involved is in no way relevant to the point at issue.
Well, then, go on.
Since you do not know either man, friend Elijah, let me tell you that Dr. Humboldt is well into his twenty-seventh decadepardon me, friend Elijah?
Nothing. Nothing, said Baley, irritably. He had merely muttered to himself, more or less incoherently, in a natural reaction to the extended life-spans of the Spacers. And hes still active, despite his age? On Earth, mathematicians after thirty or so.
Daneel said, calmly; Dr. Humboldt is one of the top three mathematicians, by long-established repute, in the galaxy. Certainly he is still active. Dr. Sabbat, on the other hand, is quite young, not yet fifty, but he has already established himself as the most remarkable new talent in the most abstruse branches of mathematics.
Theyre both great, then, said Baley. He remembered his pipe and picked it up. He decided there was no point in lighting it now and knocked out the dottle. What happened? Is this a murder case? Did one of them apparently kill the other?
Of these two men of great reputation, one is trying to destroy that of the other. By human values, I believe this may be regarded as worse than physical murder.
Sometimes, I suppose. Which one is trying to destroy the other?
Why, that, friend Elijah, is precisely the point at issue. Which?
Go on.
Dr. Humboldt tells the story clearly. Shortly before he boarded the starship, he had an insight into a possible method for analyzing neural pathways from changes in microwave absorption patterns of local cortical areas. The insight was a purely mathematical technique of extraordinary subtlety, but I cannot, of course, either understand or sensibly transmit the details. These do not, however, matter. Dr. Humboldt considered the matter and was more convinced each hour that he had something revolutionary on hand, something that would dwarf all his previous accomplishments in mathematics. Then he discovered that Dr. Sabbat was on board.
Ah. And he tried it out on young Sabbat?
Exactly. The two had met at professional meetings before and knew each other thoroughly by reputation. Humboldt went into it with Sabbat in great detail. Sabbat backed Humboldts analysis completely and was unstinting in his praise of the importance of the discovery and of the ingenuity of the discoverer. Heartened and reassured by this, Humboldt prepared a paper outlining, in summary, his work and, two days later, prepared to have it forwarded subetherically to the co-chairmen of the conference at Aurora, in order that he might officially establish his priority and arrange for possible discussion before the sessions were closed. To his surprise, he found that Sabbat was ready with a paper of his own, essentially the same as Humboldts, and Sabbat was also preparing to have it subetherized to Aurora.
I suppose Humboldt was furious.
Quite!
And Sabbat? What was his story?
Precisely the same as Humboldts. Word for word except for the mirror-image exchange of names. According to Sabbat, it was he who had the insight, and he who consulted Humboldt; it was Humboldt who agreed with the analysis and praised it.
Then each one claims the idea is his and that the other stole it. It doesnt sound like a problem to me at all. In matters of scholarship, it would seem only necessary to produce the records of research, dated and initialed. Judgment as to priority can be made from that. Even if one is falsified, that might be discovered through internal inconsistencies.
Ordinarily, friend Elijah, you would be right, but this is mathematics, and not in an experimental science. Dr. Humboldt claims to have worked out the essentials in his head. Nothing was put in writing until the paper itself was prepared. Dr. Sabbat, of course, says precisely the same.
Well, then, be more drastic and get it over with, for sure. Subject each one to a psychic probe and find out which of the two is lying.
R. Daneel shook his head slowly, Friend Elijah, you do not understand these men. They are both of rank and scholarship, Fellows of the Imperial Academy. As such, they cannot be subjected to trial of professional conduct except by a jury of their peerstheir professional peersunless they personally and voluntarily waive that right.
Put it to them, then. The guilty man wont waive the right because he cant afford to face the psychic probe. The innocent man will waive it at once. You wont even have to use the probe.
It does not work that way, friend Elijah. To waive the right in such a caseto be investigated by laymenis a serious and perhaps irrecoverable blow to prestige. Both men steadfastly refuse to waive the right to special trial, as a matter of pride. The question of guilt, or innocence, is quite subsidiary.
In that case, let it go for now. Put the matter in cold storage until you get to Aurora. At the neurobiophysical conference, there will be a huge supply of professional peers, and then
That would mean a tremendous blow to science itself, friend Elijah. Both men would suffer for having been the instrument of scandal. Even the innocent one would be blamed for having been party to a situation so distasteful. It would be felt that it should have been settled quietly out of court at all costs.
All right. Im not a Spacer, but Ill try to imagine that this attitude makes sense. What do the men in question say?
Humboldt agrees thoroughly. He says that if Sabbat will admit theft of the idea and allow Humboldt to proceed with transmission of the paperor at least its delivery at the conference, he will not press charges. Sabbats misdeed will remain secret with him; and, of course, with the captain, who is the only other human to be party to the dispute.
But young Sabbat will not agree?
On the contrary, he agreed with Dr. Humboldt to the last detailwith the reversal of names. Still the mirror-image.
So they just sit there, stalemated?
Each, I believe, friend Elijah, is waiting for the other to give in and admit guilt.
Well, then, wait.
The captain has decided this cannot be done. There are two alternatives to waiting, you see. The first is that both will remain stubborn so that when the starship lands on Aurora, the intellectual scandal will break. The captain, who is responsible for justice on board ship will suffer disgrace for not having been able to settle the matter quietly and that, to him, is quite insupportable.
And the second alternative?
Is that one, or the other, of the mathematicians will indeed admit to wrongdoing. But will the one who confesses do so out of actual guilt, or out of a noble desire to prevent the scandal? Would it be right to deprive of credit one who is sufficiently ethical to prefer to lose that credit than to see science as a whole suffer? Or else, the guilty party will confess at the last moment, and in such a way as to make it appear he does so only for the sake of science, thus escaping the disgrace of his deed and casting its shadow upon the other. The captain will be the only man to know all this but he does not wish to spend the rest of his life wondering whether he has been a party to a grotesque miscarriage of justice.
Baley sighed. A game of intellectual chicken. Wholl break first as Aurora comes nearer and nearer? Is that the whole story now, Daneel?
Not quite. There are witnesses to the transaction.

Jehoshaphat! Why didnt you say so at once. What witnesses?
Dr. Humboldts personal servant
A robot, I suppose.
Yes, certainly. He is called R. Preston. This servant, R. Preston, was present during the initial conference and he bears out Dr. Humboldt in every detail.
You mean he says that the idea was Dr. Humboldts to begin with; that Dr. Humboldt detailed it to Dr. Sabbat; that Dr. Sabbat praised the idea, and so on.
Yes, in full detail.
I see. Does that settle the matter or not? Presumably not.
You are quite right. It does not settle the matter, for there is a second witness. Dr. Sabbat also has a personal servant, R. Idda, another robot of, as it happens, the same model as R. Preston, made, I believe, in the same year in the same factory. Both have been in service for an equal period of time.
An odd coincidencevery odd.
A fact, I am afraid, and it makes it difficult to arrive at any judgment based on obvious differences between the two servants.
R. Idda, then, tells the same story as R. Preston?
Precisely the same story, except for the mirror-image reversal of the names.
R. Idda stated, then, that young Sabbat, the one not yet fiftyLije Baley did not entirely keep the sardonic note out of his voice; he himself was not yet fifty and he felt far from younghad the idea to begin with; that he detailed it to Dr. Humboldt, who was loud in his praises, and so on.
Yes, friend Elijah.
And one robot is lying, then.
So it would seem.
It should be easy to tell which. I imagine even a superficial examination by a good roboticist
A roboticist is not enough in this case, friend Elijah. Only a qualified robopsychologist would carry weight enough and experience enough to make a decision in a case of this importance. There is no one so qualified on board ship. Such an examination can be performed only when we reach Aurora
And by then the crud hits the fan. Well, youre here on Earth. We can scare up a robopsychologist, and surely anything that happens on Earth will never reach the ears of Aurora and there will be no scandal.
Except that neither Dr. Humboldt, nor Dr. Sabbat, will allow his servant to be investigated by a robopsychologist of Earth. The Earthman would have to He paused.
Lije Baley said stolidly, Hed have to touch the robot.
These are old servants, well thought of
And not to be sullied by the touch of Earthman. Then what do you want me to do, damn it? He paused, grimacing. Im sorry, R. Daneel, but I see no reason for your having involved me.

I was on the ship on a mission utterly irrelevant to the problem at hand. The captain turned to me because he had to turn to someone. I seemed human enough to talk to, and robot enough to be a safe recipient of confidences. He told me the whole story and asked what I would do. I realized the next Jump could take us as easily to Earth as to our target. I told the captain that, although I was at as much a loss to resolve the mirror-image as he was, there was on Earth one who might help.
Jehoshaphat! muttered Baley under his breath.
Consider, friend Elijah, that if you succeed in solving this puzzle, it would do your career good and Earth itself might benefit. The matter could not be publicized, of course, but the captain is a man of some influence on his home world and he would be grateful.
You just put a greater strain on me.
I have every confidence, said R. Daneel, stolidly, that you already have some idea as to what procedure ought to be followed.
Do you? I suppose that the obvious procedure is to interview the two mathematicians, one of whom would seem to be a thief.
Im afraid, friend Elijah, that neither one will come into the city. Nor would either one be willing to have you come to them.
And there is no way of forcing a Spacer to allow contact with an Earthman, no matter what the emergency. Yes, I understand that, Daneelbut I was thinking of an interview by closed-circuit television.
Nor that. They will not submit to interrogation by an Earthman.
Then what do they want of me? Could I speak to the robots?
They would not allow the robots to come here, either.
Jehoshaphat, Daneel. Youve come.
That was my own decision. I have permission, while on board ship, to make decisions of that sort without veto by any human being but the captain himselfand he was eager to establish the contact. I, having known you, decided that television contact was insufficient. I wished to shake your hand.
Lije Baley softened. I appreciate that, Daneel, but I still honestly wish you could have refrained from thinking of me at all in this case. Can I talk to the robots by television at least?
That. I think, can be arranged.
Something, at least. That means I would be doing the work of a robopsychologistin a crude sort of way.
But you are a detective, friend Elijah, not a robopsychologist.
Well, let it pass. Now before I see them, lets think a bit. Tell me: is it possible that both robots are telling the truth? Perhaps the conversation between the two mathematicians was equivocal. Perhaps it was of such a nature that each robot could honestly believe its own master was proprietor of the idea. Or perhaps one robot heard only one portion of the discussion and the other another portion, so that each could suppose its own master was proprietor of the idea.
That is quite impossible, friend Elijah. Both robots repeat the conversation in identical fashion. And the two repetitions are fundamentally inconsistent.
Then it is absolutely certain that one of the robots is lying?
Yes.
Will I be able to see the transcript of all evidence given so far in the presence of the captain, if I should want to?
I thought you would ask that and I have copies with me.
Another blessing. Have the robots been cross-examined at all, and is that cross-examination included in the transcript?
The robots have merely repeated their tales. Cross-examination would be conducted only by robopsychologists.
Or by myself?
You are a detective, friend Elijah, not a
All right, R. Daneel. Ill try to get the Spacer psychology straight. A detective can do it because he isnt a robopsychologist. Lets think further. Ordinarily a robot will not lie, but he will do so if necessary to maintain the Three Laws. He might lie to protect, in legitimate fashion, his own existence in accordance with the Third Law. He is more apt to lie if that is necessary to follow a legitimate order given him by a human being in accordance with the Second Law. He is most apt to lie if that is necessary to save a human life, or to prevent harm from coming to a human in accordance with the First Law.
Yes.
And in this case, each robot would be defending the professional reputation of his master, and would lie if it were necessary to do so. Under the circumstances, the professional reputation would be nearly equivalent to life and there might be a near-First-Law urgency to the lie.
Yet by the lie, each servant would be harming the professional reputation of the others master, friend Elijah.
So it would, but each robot might have a clearer conception of the value of its own masters reputation and honestly judge it to be greater than that of the others. The lesser harm would be done by his lie, he would suppose, than by the truth.
Having said that, Lije Baley remained quiet for a moment. Then he said, A1l right, then, can you arrange to have me talk to one of the robotsto R. Idda first, I think?
Dr. Sabbats robot?
Yes, said Baley, dryly, the young fellows robot.
It will take me but a few minutes, said R. Daneel. I have a micro-receiver outfitted with a projector. I will need merely a blank wall and I think this one will do if you will allow me to move some of these film cabinets.
Go ahead. Will I have to talk into a microphone of some sort?
No, you will be able to talk in an ordinary manner. Please pardon me, friend Elijah, for a moment of further delay. I will have to contact the ship and arrange for R. Idda to be interviewed.
If that will take some time, Daneel, how about giving me the transcripted material of the evidence so far.

Lije Baley lit his pipe while R. Daneel set up the equipment, and leafed through the flimsy sheets he had been handed.
The minutes passed and R. Daneel said, If you are ready, friend Elijah, R. Idda is. Or would you prefer a few more minutes with the transcript?
No, sighed Baley, Im not learning anything new. Put him on and arrange to have the interview recorded and transcribed.
R. Idda, unreal in two-dimensional projection against the wall, was basically metallic in structurenot at all the humanoid creature that R. Daneel was. His body was tall but blocky, and there was very little to distinguish him from the many robots Baley had seen, except for minor structural details.
Baley said, Greetings, R. Idda.
Greetings, sir, said R. Idda, in a muted voice that sounded surprisingly humanoid.
You are the personal servant of Gennao Sabbat, are you not?
I am sir.
For how long, boy?
For twenty-two years, sir.
And your masters reputation is valuable to you?
Yes, sir.
Would you consider it of importance to protect that reputation?
Yes, sir.
As important to protect his reputation as his physical life?
No, sir.
As important to protect his reputation as the reputation of another.
R. Idda hesitated. He said, Such cases must be decided on their individual merit, sir. There is no way of establishing a general rule.

Baley hesitated. These Spacer robots spoke more smoothly and intellectually than Earth-models did. He was not at all sure he could outthink one.
He said, If you decided that the reputation of your master were more important than that of another, say, that of Alfred Barr Humboldt, would you lie to protect your masters reputation?
I would, sir.
Did you lie in your testimony concerning your master in his controversy with Dr. Humboldt?
No, sir.
But if you were lying, you would deny you were lying in order to protect that lie, wouldnt you?
Yes, sir.
Well, then, said Baley, lets consider this. Your master, Gennao Sabbat, is a young man of great reputation in mathematics, but he is a young man. If, in this controversy with Dr. Humboldt, he had succumbed to temptation and had acted unethically, he would suffer a certain eclipse of reputation, but he is young and would have ample time to recover. He would have many intellectual triumphs ahead of him and men would eventually look upon this plagiaristic attempt as the mistake of a hot-blooded youth, deficient in judgment. It would be something that would be made up for in the future.
If, on the other hand, it were Dr. Humboldt who succumbed to temptation, the matter would be much more serious. He is an old man whose great deeds have spread over centuries. His reputation has been unblemished hitherto. All of that, however, would be forgotten in the light of this one crime of his later years, and he would have no opportunity to make up for it in the comparatively short time remaining to him. There would be little more that he could accomplish. There would be so many more years of work ruined in Humboldts case than in that of your master and so much less opportunity to win back his position. You see, dont you, that Humboldt faces the worse situation and deserves the greater consideration?
There was a long pause. Then R. Idda said, with unmoved voice, My evidence was a lie. It was Dr. Humboldt whose work it was, and my master has attempted, wrongfully, to appropriate the credit.
Baley said, Very well, boy. You are instructed to say nothing to anyone about this until given permission by the captain of the ship. You are excused.
The screen blanked out and Baley puffed at his pipe. Do you suppose the captain heard that, Daneel?
I am sure of it. He is the only witness, except for us.
Good. Now for the other.
But is there any point to that, friend Elijah, in view of what R. Idda has confessed?
Of course there is. R. Iddas confession means nothing.
Nothing?
Nothing at all. I pointed out that Dr. Humboldts position was the worse. Naturally, if he were lying to protect Sabbat, he would switch to the truth as, in fact, he claimed to have done. On the other hand, if he were telling the truth, he would switch to a lie to protect Humboldt. Its still mirror-image and we havent gained anything.
But then what will we gain by questioning R. Preston?
Nothing, if the minor-image were perfectbut it is not. After all, one of the robots is telling the truth to begin with, and one is lying to begin with, and that is a point of asymmetry. Let me see R. Preston. And if the transcription of R. Iddas examination is done, let me have it.

The projector came into use again. R. Preston stared out of it; identical with R. Idda in every respect, except for some trivial chest design.
Baley said, Greetings, R. Preston. He kept the record of R. Iddas examination before him as he spoke.
Greetings, sir, said R. Preston. His voice was identical with that of R. Idda.
You are the personal servant of Alfred Ban Humboldt are you not?
I am, sir.
For how long, boy?
For twenty-two years, sir.
And your masters reputation is valuable to you?
Yes, sir.
Would you consider it of importance to protect that reputation?
Yes, sir.
As important to protect his reputation as his physical life?
No, sir.
As important to protect his reputation as the reputation of another?
R. Preston hesitated. He said, Such cases must be decided on their individual merit, sir. There is no way of establishing a general rule.
Baley said, If you decided that the reputation of your master were more important than that of another, say, that of Gennao Sabbat, would you lie to protect your masters reputation?
I would, sir.
Did you lie in your testimony concerning your master in his controversy with Dr. Sabbat?
No, sir.
But if you were lying, you would deny you were lying, in order to protect that lie, wouldnt you?
Yes, sir.
Well, then, said Baley, lets consider this. Your master, Alfred Barr Humboldt, is an old man of great reputation in mathematics, but he is an old man. If, in this controversy with Dr. Sabbat, he had succumbed to temptation and had acted unethically, he would suffer a certain eclipse of reputation, but his great age and his centuries of accomplishments would stand against that and would win out. Men would look upon this plagiaristic attempt as the mistake of a perhaps-sick old man, no longer certain in judgment.
If, on the other hand, it were Dr. Sabbat who had succumbed to temptation, the matter would be much more serious. He is a young man, with a far less secure reputation. He would ordinarily have centuries ahead of him in which he might accumulate knowledge and achieve great things. This will be closed to him, now, obscured by one mistake of his youth. He has a much longer future to lose than your master has. You see, dont you, that Sabbat faces the worse situation and deserves the greater consideration?
There was a long pause. Then R. Preston said, with unmoved voice, My evidence was as I
At that point, he broke off and said nothing more. Baley said, Please continue, R. Preston.
There was no response.
R. Daneel said, I am afraid, friend Elijah, that R. Preston is in stasis. He is out of commission.
Well, then, said Baley, we have finally produced an asymmetry. From this, we can see who the guilty person is.
In what way, friend Elijah?
Think it out. Suppose you were a person who had committed no crime and that your personal robot were a witness to that. There would be nothing you need do. Your robot would tell the truth and bear you out. If, however, you were a person who had committed the crime, you would have to depend on your robot to lie. That would be a somewhat riskier position, for although the robot would lie, if necessary, the greater inclination would be to tell the truth, so that the lie would be less firm than the truth would be. To prevent that, the crime-committing person would very likely have to order the robot to lie. In this way, First Law would be strengthened by Second Law; perhaps very substantially strengthened.
That would seem reasonable, said R. Daneel.
Suppose we have one robot of each type. One robot would switch from truth, unreinforced, to the lie, and could do so after some hesitation, without serious trouble. The other robot would switch from the lie, strongly reinforced, to the truth, but could do so only at the risk of burning out various positronic-track-ways in his brain and falling into stasis.
And since R. Preston went into stasis
R. Prestons master, Dr. Humboldt, is the man guilty of plagiarism. If you transmit this to the captain and urge him to face Dr. Humboldt with the matter at once, he may force a confession. If so, I hope you will tell me immediately.
I will certainly do so. You will excuse me, friend Elijah? I must talk to the captain privately.
Certainly. Use the conference room. It is shielded.

Baley could do no work of any kind in R. Daneels absence. He sat in uneasy silence. A great deal would depend on the value of his analysis, and he was acutely aware of his lack of expertise in robotics.
R. Daneel was back in half an hourvery nearly the longest half hour of Baleys life.
There was no use, of course, in trying to determine what had happened from the expression of the humanoids impassive face. Baley tried to keep his face impassive.
Yes, R. Daneel? he asked.
Precisely as you said, friend Elijah. Dr. Humboldt has confessed. He was counting, he said, on Dr. Sabbat giving way and allowing Dr. Humboldt to have this one last triumph. The crisis is over and you will find the captain grateful. He has given me permission to tell you that he admires your subtlety greatly and I believe that I, myself, will achieve favor for having suggested you.
Good, said Baley, his knees weak and his forehead moist now that his decision had proven correct, but Jehoshaphat, R. Daneel, dont put me on the spot like that again, will you?
I will try not to, friend Elijah. All will depend, of course, on the importance of a crisis, on your nearness, and on certain other factors. Meanwhile, I have a question
Yes?
Was it not possible to suppose that passage from a lie to the truth was easy, while passage from the truth to a lie was difficult? And in that case, would not the robot in stasis have been going from a truth to a lie, and since R. Preston was in stasis, might one not have drawn the conclusion that it was Dr. Humboldt who was innocent and Dr. Sabbat who was guilty?
Yes, R. Daneel. It was possible to argue that way, but it was the other argument that proved right. Humboldt did confess, didnt he?
He did. But with arguments possible in both directions, how could you, friend Elijah, so quickly pick the correct one?
For a moment, Baleys lips twitched. Then he relaxed and they curved into a smile. Because, R. Daneel, I took into account human reactions, not robotic ones. I know more about human beings than about robots. In other words, I had an idea as to which mathematician was guilty before I ever interviewed the robots. Once I provoked an asymmetric response in them, I simply interpreted it in such a way as to place the guilt on the one I already believed to be guilty. The robotic response was dramatic enough to break down the guilty man; my own analysis of human behavior might not have been sufficient to do so.
I am curious to know what your analysis of human behavior was?
Jehoshaphat, R. Daneel; think, and you wont have to ask. There is another point of asymmetry in this tale of mirror-image besides the matter of true-and-false. There is the matter of the age of the two mathematicians; one is quite old and one is quite young.
Yes, of course, but what then?
Why, this. I can see a young man, flushed with a sudden, startling and revolutionary idea, consulting in the matter an old man whom he has, from his early student days, thought of as a demigod in the field. I can not see an old man, rich in honors and used to triumphs, coming up with a sudden, startling and revolutionary idea, consulting a man centuries his junior whom he is bound to think of as a young whippersnapperor whatever term a Spacer would use. Then, too, if a young man had the chance, would he try to steal the idea of a revered demigod? It would be unthinkable. On the other hand, an old man, conscious of declining powers, might well snatch at one last chance of fame and consider a baby in the field to have no rights he was bound to observe. In short, it was not conceivable that Sabbat steal Humboldts idea; and from both angles, Dr. Humboldt was guilty.
R. Daneel considered that for a long time. Then he held out his hand. I must leave now, friend Elijah. It was good to see you. May we meet again soon.
Baley gripped the robots hand, warmly, If you dont mind, R. Daneel, he said, not too soon.
Lenny

United States Robots and Mechanical Men Corporation had a problem. The problem was people.
Peter Bogert, Senior Mathematician, was on his way to Assembly when he encountered Alfred Lanning, Research Director. Lanning was bending his ferocious white eyebrows together and staring down across the railing into the computer room.
On the floor below the balcony, a trickle of humanity of both sexes and various ages was looking about curiously, while a guide intoned a set speech about robotic computing.
This computer you see before you, he said, is the largest of its type in the world. It contains five million three hundred thousand cryotrons and is capable of dealing simultaneously with over one hundred thousand variables. With its help, U. S. Robots is able to design with precision the positronic brains of new models. The requirements are fed in on tape which is perforated by the action of this keyboardsomething like a very complicated typewriter or linotype machine, except that it does not deal with letters but with concepts. Statements are broken down into the symbolic logic equivalents and those in turn converted to perforation patterns.
The computer can, in less than one hour, present our scientists with a design for a brain which will give all the necessary positronic paths to make a robot...
Alfred Lanning looked up at last and noticed the other. Ah, Peter, he said.
Bogert raised both hands to smooth down his already perfectly smooth and glossy head of black hair. He said, You dont look as though you think much of this, Alfred.
Lanning grunted. The idea of public guided tours of U. S. Robots was of fairly recent origin, and was supposed to serve a dual function. On the one hand, the theory went, it allowed people to see robots at close quarters and counter their almost instinctive fear of the mechanical objects through increased familiarity. And on the other hand, it was supposed to interest at least an occasional person in taking up robotics research as a life work.
You know I dont, Lanning said finally. Once a week, work is disrupted. Considering the man-hours lost, the return is insufficient.
Still no rise in job applications, then?
Oh, some, but only in the categories where the need isnt vital. Its research men that are needed. You know that. The trouble is that with robots forbidden on Earth itself, theres something unpopular about being a roboticist.
The damned Frankenstein complex, said Bogert, consciously imitating one of the others pet phrases.
Lanning missed the gentle jab. He said, I ought to be used to it, but I never will. Youd think that by now every human being on Earth would know that the Three Laws represented a perfect safeguard; that robots are simply not dangerous. Take this bunch. He glowered down. Look at them. Most of them go through the robot assembly room for the thrill of fear, like riding a roller coaster. Then when they enter the room with the MEC modeldamn it, Peter, a MEC model that will do nothing on Gods green Earth but take two steps forward, say Pleased to meet you, sir, shake hands, then take two steps backthey back away and mothers snatch up their kids. How do we expect to get brainwork out of such idiots?
Bogert had no answer. Together, they stared down once again at the line of sightseers, now passing out of the computer room and into the positronic brain assembly section. Then they left. They did not, as it turned out, observe Mortimer W. Jacobson, age 16who, to do him complete justice, meant no harm whatever.

In fact, it could not even be said to be Mortimers fault. The day of the week on which the tour took place was known to all workers.
All devices in its path ought to have been carefully neutralized or locked, since it was unreasonable to expect human beings to withstand the temptation to handle knobs, keys, handles and pushbuttons. In addition, the guide ought to have been very carefully on the watch for those who succumbed.
But, at the time, the guide had passed into the next room and Mortimer was tailing the line. He passed the keyboard on which instructions were fed into the computer. He had no way of suspecting that the plans for a new robot design were being fed into it at that moment, or, being a good kid, he would have avoided the keyboard. He had no way of knowing that, by what amounted to almost criminal negligence, a technician had not inactivated the keyboard.
So Mortimer touched the keys at random as though he were playing a musical instrument.
He did not notice that a section of perforated tape stretched itself out of the instrument in another part of the roomsoundlessly, unobtrusively.
Nor did the technician, when he returned, discover any signs of tampering. He felt a little uneasy at noticing that the keyboard was live, but did not think to check. After a few minutes, even his first trifling uneasiness was gone, and he continued feeding data into the computer.
As for Mortimer, neither then, nor ever afterward, did he know what he had done.

The new LNE model was designed for the mining of boron in the asteroid belt. The boron hydrides were increasing in value yearly as primers for the proton micropiles that carried the ultimate load of power production on spaceships, and Earths own meager supply was running thin.
Physically, that meant that the LNE robots would have to be equipped with eyes sensitive to those lines prominent in the spectroscopic analysis of boron ores and the type of limbs most useful for the working up of ore to finished product. As always, though, the mental equipment was the major problem.
The first LNE positronic brain had been completed now. It was the prototype and would join all other prototypes in U. S. Robots collection. When finally tested, others would then be manufactured for leasing (never selling) to mining corporations.
LNE-Prototype was complete now. Tall, straight, polished, it looked from outside like any of a number of not-too-specialized robot models.
The technician in charge, guided by the directions for testing in the Handbook of Robotics, said, How are you?
The indicated answer was to have been, I am well and ready to begin my functions. I trust you are well, too, or some trivial modification thereof.
This first exchange served no purpose but to show that the robot could hear, understand a routine question, and make a routine reply congruent with what one would expect of a robotic attitude. Beginning from there, one could pass on to more complicated matters that would test the different Laws and their interaction with the specialized knowledge of each particular model.
So the technician said, How are you? He was instantly jolted by the nature of LNE-Prototypes voice. It had a quality like no robotic voice he had ever heard (and he had heard many). It formed syllables like the chimes of a low-pitched celeste.
So surprising was this that it was only after several moments that the technician heard, in retrospect, the syllables that had been formed by those heavenly tones. They were, Da, da, da, goo. The robot still stood tall and straight but its right hand crept upward and a finger went into its mouth.
The technician stared in absolute horror and bolted. He locked the door behind him and, from another room, put in an emergency call to Dr. Susan Calvin.


Dr. Susan Calvin was U. S. Robots (and, virtually, mankinds) only robopsychologist. She did not have to go very far in her testing of LNE-Prototype before she called very peremptorily for a transcript of the computer-drawn plans of the positronic brain-paths and the taped instructions that had directed them. After some study, she, in turn, sent for Bogert.
Her iron-gray hair was drawn severely back; her cold face, with its strong vertical lines marked off by the horizontal gash of the pale, thin-lipped mouth, turned intensely upon him.
What is this, Peter? Bogert studied the passages she pointed out with increasing stupefaction and said, Good Lord, Susan, it makes no sense.
It most certainly doesnt. How did it get into the instructions? The technician in charge, called upon, swore in all sincerity that it was none of his doing, and that he could not account for it. The computer checked out negative for all attempts at flaw-finding.
The positronic brain, said Susan Calvin, thoughtfully, is past redemption. So many of the higher functions have been cancelled out by these meaningless directions that the result is very like a human baby.
Bogert looked surprised, and Susan Calvin took on a frozen attitude at once, as she always did at the least expressed or implied doubt of her word. She said, We make every effort to make a robot as mentally like a man as possible. Eliminate what we call the adult functions and what is naturally left is a human infant, mentally speaking. Why do you look so surprised, Peter?
LNE-Prototype, who showed no signs of understanding any of the things that were going on around it, suddenly slipped into a sitting position and began a minute examination of its feet.
Bogert stared at it. Its a shame to have to dismantle the creature. Its a handsome job.
Dismantle it? said the robopsychologist forcefully. Of course, Susan. Whats the use of this thing? Good Lord, if theres one object completely and abysmally useless its a robot without a job it can perform. You dont pretend theres a job this thing can do, do you?
No, of course not.
Well, then?
Susan Calvin said, stubbornly, I want to conduct more tests. Bogert looked at her with a moments impatience, then shrugged. If there was one person at U. S. Robots with whom it was useless to dispute, surely that was Susan Calvin. Robots were all she loved, and long association with them, it seemed to Bogert, had deprived her of any appearance of humanity. She was no more to be argued out of a decision than was a triggered micropile to be argued out of operating.
Whats the use? he breathed; then aloud, hastily: Will you let us know when your tests are complete?
I will, she said. Come, Lenny.
(LNE, thought Bogert. That becomes Lenny. Inevitable.)
Susan Calvin held out her hand but the robot only stared at it. Gently, the robopsychologist reached for the robots hand and took it. Lenny rose smoothly to its feet (its mechanical coordination, at least, worked well). Together they walked out, robot topping woman by two feet. Many eyes followed them curiously down the long corridors.

One wall of Susan Calvins laboratory, the one opening directly off her private office, was covered with a highly magnified reproduction of a positronic-path chart. Susan Calvin had studied it with absorption for the better part of a month.
She was considering it now, carefully, tracing the blunted paths through their contortions. Behind her, Lenny sat on the floor, moving its legs apart and together, crooning meaningless syllables to itself in a voice so beautiful that one could listen to the nonsense and be ravished.
Susan Calvin turned to the robot, LennyLenny
She repeated this patiently until finally Lenny looked up and made an inquiring sound. The robopsychologist allowed a glimmer of pleasure to cross her face fleetingly. The robots attention was being gained in progressively shorter intervals.
She said, Raise your hand, Lenny. Hand-up. Hand-up. She raised her own hand as she said it, over and over.
Lenny followed the movement with its eyes. Up, down, up, down. Then it made an abortive gesture with its own hand and chimed, Eh-uh.
Very good, Lenny, said Susan Calvin, gravely. Try it again. Hand-up.
Very gently, she reached out her own hand, took the robots, and raised it, lowered it. Hand-up. Hand-up.
A voice from her office called and interrupted. Susan?
Calvin halted with a tightening of her lips. What is it, Alfred? The research director walked in, and looked at the chart on the wall and at the robot. Still at it?
Im at my work, yes.
Well, you know, Susan... He took out a cigar, staring at it hard, and made as though to bite off the end. In doing so, his eyes met the womans stern look of disapproval; and he put the cigar away and began over. Well, you know, Susan, the LNE model is in production now.
So Ive heard. Is there something in connection with it you wish of me?
No-o. Still, the mere fact that it is in production and is doing well means that working with this messed-up specimen is useless. Shouldnt it be scrapped?
In short, Alfred, you are annoyed that I am wasting my so-valuable time. Feel relieved. My time is not being wasted. I am working with this robot.
But the work has no meaning.
Ill be the judge of that, Alfred. Her voice was ominously quiet, and Lanning thought it wiser to shift his ground.
Will you tell me what meaning it has? What are you doing with it right now, for instance?
Im trying to get it to raise its hand on the word of command. Im trying to get it to imitate the sound of the word.
As though on cue, Lenny said, Eh-uhAnd raised its hand waveringly.
Lanning shook his head. That voice is amazing. How does it happen?
Susan Calvin said, I dont quite know. Its transmitter is a normal one. It could speak normally, Im sure. It doesnt, however; it speaks like this as a consequence of something in the positronic paths that I have not yet pinpointed.
Well, pinpoint it, for Heavens sake. Speech like that might be useful.
Oh, then there is some possible use in my studies on Lenny? Lanning shrugged in embarrassment. Oh, well, its a minor point.
Im sorry you dont see the major points, then, said Susan Calvin with asperity, which are much more important, but thats not my fault. Would you leave now, Alfred, and let me go on with my work?

Lanning got to his cigar, eventually, in Bogerts office. He said, sourly, That woman is growing more peculiar daily.
Bogert understood perfectly. In the U. S. Robots and Mechanical Men Corporation, there was only one that woman. He said, Is she still scuffing about with that pseudo-robotthat Lenny of hers?
Trying to get it to talk, so help me. Bogert shrugged. Points up the company problem. I mean, about getting qualified personnel for research. If we had other robopsychologists, we could retire Susan. Incidentally, I presume the directors meeting scheduled for tomorrow is for the purpose of dealing with the procurement problem?
Lanning nodded and looked at his cigar as though it didnt taste good. Yes. Quality, though, not quantity. Weve raised wages until theres a steady stream of applicantsthose who are interested primarily in money. The trick is to get those who are interested primarily in roboticsa few more like Susan Calvin.
Hell, no. Not like her.
Well, not like her personally. But youll have to admit, Peter, that shes single-minded about robots. She has no other interest in life.
I know. And thats exactly what makes her so unbearable. Lanning nodded. He had lost count of the many times it would have done his soul good to have fired Susan Calvin. He had also lost count of the number of millions of dollars she had at one time or another saved the company. She was a truly indispensable woman and would remain one until she diedor until they could lick the problem of finding men and women of her own high caliber who were interested in robotics research.
He said, I think well cut down on the tour business. Peter shrugged. If you say so. But meanwhile, seriously, what do we do about Susan? She can easily tie herself up with Lenny indefinitely. You know how she is when she gets what she considers an interesting problem.
What can we do? said Lanning. If we become too anxious to pull her off, shell stay on out of feminine contrariness. In the last analysis, we cant force her to do anything.
The dark-haired mathematician smiled. I wouldnt ever apply the adjective feminine to any part of her.
Oh, well, said Lanning, grumpily. At least, it wont do anyone any actual harm.
In that, if in nothing else, he was wrong. The emergency signal is always a tension-making thing in any large industrial establishment. Such signals had sounded in the history of U. S. Robots a dozen timesfor fire, flood, riot and insurrection.
But one thing had never occurred in all that time. Never had the particular signal indicating Robot out of control sounded. No one ever expected it to sound. It was only installed at government insistence. (Damn the Frankenstein complex, Lanning would mutter on those rare occasions when he thought of it.)
Now, finally, the shrill siren rose and fell at ten-second intervals, and practically no worker from the President of the Board of Directors down to the newest janitors assistant recognized the significance of the strange sound for a few moments. After those moments passed, there was a massive convergence of armed guards and medical men to the indicated area of danger and U. S. Robots was struck with paralysis.
Charles Randow, computing technician, was taken off to hospital level with a broken arm. There was no other damage. No other physical damage.
But the moral damage, roared Lanning, is beyond estimation.
Susan Calvin faced him, murderously calm. You will do nothing to Lenny. Nothing. Do you understand?
Do you understand, Susan? That thing has hurt a human being. It has broken First Law. Dont you know what First Law is?
You will do nothing to Lenny.
For Gods sake, Susan, do I have to tell you First Law? A robot may not harm a human being or, through inaction, allow a human being to come to harm. Our entire position depends on the fact that First Law is rigidly observed by all robots of all types. If the public should hear, and they will hear, that there was an exception, even one exception, we might be forced to close down altogether. Our only chance of survival would be to announce at once that the robot involved had been destroyed, explain the circumstances, and hope that the public can be convinced that it will never happen again.
I would like to find out exactly what happened, said Susan Calvin. I was not present at the time and I would like to know exactly what the Randow boy was doing in my laboratories without my permission.
The important thing that happened, said Lanning, is obvious. Your robot struck Randow and the damn fool flashed the Robot out of control button and made a case of it. But your robot struck him and inflicted damage to the extent of a broken arm. The truth is your Lenny is so distorted it lacks First Law and it must be destroyed.
It does not lack First Law. I have studied its brainpaths and know it does not lack it.
Then how could it strike a man? Desperation turned him to sarcasm. Ask Lenny. Surely you have taught it to speak by now.
Susan Calvins cheeks Bushed a painful pink. She said, I prefer to interview the victim. And in my absence, Alfred, I want my offices sealed tight, with Lenny inside. I want no one to approach him. If any harm comes to him while I am gone, this company will not see me again under any circumstances.
Will you agree to its destruction, if it has broken First Law?
Yes, said Susan Calvin, because I know it hasnt.

Charles Randow lay in bed with his arm set and in a cast. His major suffering was still from the shock of those few moments in which he thought a robot was advancing on him with murder in its positronic mind. No other human had ever had such reason to fear direct robotic harm as he had had just then. He had had a unique experience.
Susan Calvin and Alfred Lanning stood beside his bed now; Peter Bogert, who had met them on the way, was with them. Doctors and nurses had been shooed out.
Susan Calvin said, Nowwhat happened? Randow was daunted. He muttered, The thing hit me in the arm. It was coming at me.
Calvin said, Move further back in the story. What were you doing in my laboratory without authorization?
The young computer swallowed, and the Adams apple in his thin neck bobbed noticeably. He was high-cheekboned and abnormally pale. He said, We all knew about your robot. The word is you were trying to teach it to talk like a musical instrument. There were bets going as to whether it talked or not. Some saiduhyou could teach a gatepost to talk.
I suppose, said Susan Calvin, freezingly, that is meant as a compliment. What did that have to do with you?
I was supposed to go in there and settle matterssee if it would talk, you know. We swiped a key to your place and I waited till you were gone and went in. We had a lottery on who was to do it. I lost.
Then?
I tried to get it to talk and it hit me.
What do you mean, you tried to get it to talk? How did you try?
II asked it questions, but it wouldnt say anything, and I had to give the thing a fair shake, so I kind ofyelled at it, and
And?
There was a long pause. Under Susan Calvins unwavering stare, Randow finally said, I tried to scare it into saying something. He added defensively, I had to give the thing a fair shake.
How did you try to scare it?
I pretended to take a punch at it.
And it brushed your arm aside?
It hit my arm.
Very well. Thats all. To Lanning and Bogert, she said, Come, gentlemen.
At the doorway, she turned back to Randow. I can settle the bets going around, if you are still interested. Lenny can speak a few words quite well.

They said nothing until they were in Susan Calvins office. Its walls were lined with her books, some of which she had written herself. It retained the patina of her own frigid, carefully ordered personality. It had only one chair in it and she sat down. Lanning and Bogert remained standing.
She said, Lenny only defended itself. That is the Third Law: A robot must protect its own existence.
Except, said Lanning forcefully, when this conflicts with the First or Second Laws. Complete the statement! Lenny had no right to defend itself in any way at the cost of harm, however minor, to a human being.
Nor did it, shot back Calvin, knowingly. Lenny has an aborted brain. It had no way of knowing its own strength or the weakness of humans. In brushing aside the threatening arm of a human being it could not know the bone would break. In human terms, no moral blame can be attached to an individual who honestly cannot differentiate good and evil.
Bogert interrupted, soothingly, Now, Susan, we dont blame. We understand that Lenny is the equivalent of a baby, humanly speaking, and we dont blame it. But the public will. U. S. Robots will be closed down.
Quite the opposite. If you had the brains of a flea, Peter, you would see that this is the opportunity U. S. Robots is waiting for. That this will solve its problems.
Lanning hunched his white eyebrows low. He said, softly, What problems, Susan?
Isnt the corporation concerned about maintaining our research personnel at the presentHeaven help ushigh level?
We certainly are.
Well, what are you offering prospective researchers? Excitement? Novelty? The thrill of piercing the unknown? No! You offer them salaries and the assurance of no problems.
Bogert said, How do you mean, no problems?
Are there problems? shot back Susan Calvin. What kind of robots do we turn out? Fully developed robots, fit for their tasks. An industry tells us what it needs; a computer designs the brain; machinery forms the robot; and there it is, complete and done. Peter, some time ago, you asked me with reference to Lenny what its use was. Whats the use, you said, of a robot that was not designed for any job? Now I ask youwhats the use of a robot designed for only one job? It begins and ends in the same place. The LNE models mine boron. If beryllium is needed, they are useless. If boron technology enters a new phase, they become useless. A human being so designed would be sub-human. A robot so designed is sub-robotic.
Do you want a versatile robot? asked Lanning, incredulously. Why not? demanded the robopsychologist. Why not? Ive been handed a robot with a brain almost completely stultified. Ive been teaching it, and you, Alfred, asked me what was the use of that. Perhaps very little as far as Lenny itself is concerned, since it will never progress beyond the five-year-old level on a human scale. But whats the use in general? A very great deal, if you consider it as a study in the abstract problem of learning how to teach robots. I have learned ways to short-circuit neighboring pathways in order to create new ones. More study will yield better, more subtle and more efficient techniques of doing so.
Well?
Suppose you started with a positronic brain that had all the basic pathways carefully outlined but none of the secondaries. Suppose you then started creating secondaries. You could sell basic robots designed for instruction; robots that could be modeled to a job, and then modeled to another, if necessary. Robots would become as versatile as human beings. Robots could learn!
They stared at her. She said, impatiently, You still dont understand, do you?
I understand what you are saying, said Lanning.
Dont you understand that with a completely new field of research and completely new techniques to be developed, with a completely new area of the unknown to be penetrated, youngsters will feel a new urge to enter robotics? Try it and see.
May I point out, said Bogert, smoothly, that this is dangerous. Beginning with ignorant robots such as Lenny will mean that one could never trust First Lawexactly as turned out in Lennys case.
Exactly. Advertise the fact.
Advertise it!
Of course. Broadcast the danger. Explain that you will set up a new research institute on the moon, if Earths population chooses not to allow this sort of thing to go on upon Earth, but stress the danger to the possible applicants by all means.
Lanning said, For Gods sake, why?
Because the spice of danger will add to the lure. Do you think nuclear technology involves no danger and spationautics no peril? Has your lure of absolute security been doing the trick for you? Has it helped you to cater to the Frankenstein complex you all despise so? Try something else then, something that has worked in other fields.
There was a sound from beyond the door that led to Calvins personal laboratories. It was the chiming sound of Lenny.
The robopsychologist broke off instantly, listening. She said, Excuse me. I think Lenny is calling me.
Can it call you? said Lanning.
I said Ive managed to teach it a few words. She stepped toward the door, a little flustered. If you will wait for me
They watched her leave and were silent for a moment. Then Lanning said, Do you think theres anything to what she says, Peter?
Just possibly, Alfred, said Bogert. Just possibly. Enough for US to bring the matter up at the directors meeting and see what they say. After all, the fat is in the fire. A robot has harmed a human being and knowledge of it is public. As Susan says, we might as well try to turn the matter to our advantage. Of course, I distrust her motives in all this.
How do you mean?
Even if all she has said is perfectly true, it is only rationalization as far as she is concerned. Her motive in all this is her desire to hold on to this robot. If we pressed her (and the mathematician smiled at the incongruous literal meaning of the phrase) she would say it was to continue learning techniques of teaching robots, but I think she has found another use for Lenny. A rather unique one that would fit only Susan of all women.
I dont get your drift. Bogert said, Did you hear what the robot was calling?
Well, no, I didnt quite began Lanning, when the door opened suddenly, and both men stopped talking at once.
Susan Calvin stepped in again, looking about uncertainly. Have either of you seenIm positive I had it somewhere aboutOh, there it is.
She ran to a corner of one bookcase and picked up an object of intricate metal webbery, dumbbell shaped and hollow, with variously shaped metal pieces inside each hollow, just too large to be able to fallout of the webbing.
As she picked it up, the metal pieces within moved and struck together, clicking pleasantly. It struck Lanning that the object was a kind of robotic version of a baby rattle.
As Susan Calvin opened the door again to pass through, Lennys voice chimed again from within. This time, Lanning heard it clearly as it spoke the words Susan Calvin had taught it.
In heavenly celeste-like sounds, it called out, Mommie, I want you. I want you, Mommie.
And the footsteps of Susan Calvin could be heard hurrying eagerly across the laboratory floor toward the only kind of baby she could ever have or love.
Galley Slave

The United States Robots and Mechanical Men Corporation, as defendants in the case, had influence enough to force a closed-doors trial without a jury.
Nor did Northeastern University try hard to prevent it. The trustees knew perfectly well how the public might react to any issue involving misbehavior of a robot, however rarefied that misbehavior might be. They also had a clearly visualized notion of how an antirobot riot might become an antiscience riot without warning.
The government, as represented in this case by Justice Harlow Shane, was equally anxious for a quiet end to this mess. Both U. S. Robots and the academic world were bad people to antagonize.
Justice Shane said, Since neither press, public nor jury is present, gentlemen, let us stand on as little ceremony as we can and get to the facts.
He smiled stiffly as he said this, perhaps without much hope that his request would be effective, and hitched at his robe so that he might sit more comfortably. His face was pleasantly rubicund, his chin round and soft, his nose broad and his eyes light in color and wide-set. All in all, it was not a face with much judicial majesty and the judge knew it.
Barnabas H. Goodfellow, Professor of Physics at Northeastern U., was sworn in first, taking the usual vow with an expression that made mincemeat of his name.
After the usual opening-gambit questions, Prosecution shoved his hands deep into his pockets and said, When was it, Professor, that the matter of the possible employ of Robot EZ-27 was first brought to your attention, and how?
Professor Goodfellows small and angular face set itself into an uneasy expression, scarcely more benevolent than the one it replaced. He said, I have had professional contact and some social acquaintance with Dr. Alfred Lanning, Director of Research at U. S. Robots. I was inclined to listen with some tolerance then when I received a rather strange suggestion from him on the third of March of last year
Of 2033?
Thats right.
Excuse me for interrupting. Please proceed.
The professor nodded frostily, scowled to fix the facts in his mind, and began to speak.

Professor Goodfellow looked at the robot with a certain uneasiness. It had been carried into the basement supply room in a crate, in accordance with the regulations governing the shipment of robots from place to place on the Earths surface.
He knew it was coming; it wasnt that he was unprepared. From the moment of Dr. Lannings first phone call on March 3, he had felt himself giving way to the others persuasiveness, and now, as an inevitable result, he found himself face to face with a robot.
It looked uncommonly large as it stood within arms reach. Alfred Lanning cast a hard glance of his own at the robot, as though making certain it had not been damaged in transit. Then he turned his ferocious eyebrows and his mane of white hair in the professors direction.
This is Robot EZ-27, first of its model to be available for public use. He turned to the robot. This is Professor Goodfellow, Easy.
Easy spoke impassively, but with such suddenness that the professor shied. Good afternoon, Professor.
Easy stood seven feet tall and had the general proportions of a manalways the prime selling point of U. S. Robots. That and the possession of the basic patents on the positronic brain had given them an actual monopoly on robots and a near-monopoly on computing machines in general.
The two men who had uncrated the robot had left now and the professor looked from Lanning to the robot and back to Lanning. It is harmless, Im sure. He didnt sound sure.
More harmless than I am, said Lanning. I could be goaded into striking you. Easy could not be. You know the Three Laws of Robotics, I presume.
Yes, of course, said Goodfellow.
They are built into the positronic patterns of the brain and must be observed. The First Law, the prime rule of robotic existence, safeguards the life and well-being of all humans. He paused, rubbed at his cheek, then added, Its something of which we would like to persuade all Earth if we could.
Its just that he seems formidable.
Granted. But whatever he seems, youll find that he is useful.
Im not sure in what way. Our conversations were not very helpful in that respect. Still, I agreed to look at the object and Im doing it.
Well do more than look, Professor. Have you brought a book?
I have.
May I see it?
Professor Goodfellow reached down without actually taking his eyes off the metal-in-human-shape that confronted him. From the briefcase at his feet, he withdrew a book.
Lanning held out his hand for it and looked at the backstrip. Physical Chemistry of Electrolytes in Solution. Fair enough, sir. You selected this yourself, at random. It was no suggestion of mine, this particular text. Am I right?
Yes.
Lanning passed the book to Robot EZ-27.


The professor jumped a little. No! Thats a valuable book! Lanning raised his eyebrows and they looked like shaggy coconut icing. He said, Easy has no intention of tearing the book in two as a feat of strength, I assure you. It can handle a book as carefully as you or I. Go ahead, Easy.
Thank you, sir, said Easy. Then, turning its metal bulk slightly, it added, With your permission, Professor Goodfellow.
The professor stared, then said, Yesyes, of course.
With a slow and steady manipulation of metal fingers, Easy turned the pages of the book, glancing at the left page, then the right; turning the page, glancing left, then right; turning the page and so on for minute after minute.
The sense of its power seemed to dwarf even the large cement-walled room in which they stood and to reduce the two human watchers to something considerably less than life-size.
Goodfellow muttered, The light isnt very good.
It will do.
Then, rather more sharply, But what is he doing?
Patience, sir.
The last page was turned eventually. Lanning asked, Well, Easy?
The robot said, It is a most accurate book and there is little to which I can point. On line 22 of page 27, the word positive is spelled p-o-i-s-t-i-v-e. The comma in line 6 of page 32 is superfluous, whereas one should have been used on line 13 of page 54. The plus sign in equation XIV-2 on page 337 should be a minus sign if it is to be consistent with the previous equations
Wait! Wait! cried the professor. What is he doing?
Doing? echoed Lanning in sudden irascibility. Why, man, he has already done it! He has proofread that book.
Proofread it?
Yes. In the short time it took him to turn those pages, he caught every mistake in spelling, grammar and punctuation. He has noted errors in word order and detected inconsistencies. And he will retain the information, letter-perfect, indefinitely.
The professors mouth was open. He walked rapidly away from Lanning and Easy and as rapidly back. He folded his arms across his chest and stared at them. Finally he said, You mean this is a proofreading robot?
Lanning nodded. Among other things.
But why do you show it to me?
So that you might help me persuade the university to obtain it for use.
To read proof?
Among other things, Lanning repeated patiently.
The professor drew his pinched face together in a kind of sour disbelief. But this is ridiculous!
Why?
The university could never afford to buy this half-tonit must weigh that at leastthis half-ton proofreader.
Proofreading is not all it will do. It will prepare reports from outlines, fill out forms, serve as an accurate memory-file, grade papers
All picayune!

Lanning said, Not at all, as I can show you in a moment. But I think we can discuss this more comfortably in your office, if you have no objection.
No, of course not, began the professor mechanically and took a half-step as though to turn. Then he snapped out, But the robotwe cant take the robot. Really, Doctor, youll have to crate it up again.
Time enough. We can leave Easy here.
Unattended?
Why not? He knows he is to stay. Professor Goodfellow, it is necessary to understand that a robot is far more reliable than a human being.
I would be responsible for any damage
There will be no damage. I guarantee that. Look, its after hours. You expect no one here, I imagine, before tomorrow morning. The truck and my two men are outside. U. S. Robots will take any responsibility that may arise. None will. Call it a demonstration of the reliability of the robot.
The professor allowed himself to be led out of the storeroom. Nor did he look entirely comfortable in his own office, five stories up.
He dabbed at the line of droplets along the upper half of his forehead with a white handkerchief.
As you know very well, Dr. Lanning, there are laws against the use of robots on Earths surface, he pointed out.
The laws, Professor Goodfellow, are not simple ones. Robots may not be used on public thoroughfares or within public edifices. They may not be used on private grounds or within private structures except under certain restrictions that usually turn out to be prohibitive. The university, however, is a large and privately owned institution that usually receives preferential treatment. If the robot is used only in a specific room for only academic purposes, if certain other restrictions are observed and if the men and women having occasion to enter the room cooperate fully, we may remain within the law.
But all that trouble just to read proof?
The uses would be infinite. Professor. Robotic labor has so far been used only to relieve physical drudgery. Isnt there such a thing as mental drudgery? When a professor capable of the most useful creative thought is forced to spend two weeks painfully checking the spelling of lines of print and I offer you a machine that can do it in thirty minutes, is that picayune?
But the price
The price need not bother you. You cannot buy EZ-27. U. S. Robots does not sell its products. But the university can lease EZ-27 for a thousand dollars a yearconsiderably less than the cost of a single microwave spectograph continuous-recording attachment.
Goodfellow looked stunned. Lanning followed up his advantage by saying, I only ask that you put it up to whatever group makes the decisions here. I would be glad to speak to them if they want more information.
Well, Goodfellow said doubtfully, I can bring it up at next weeks Senate meeting. I cant promise that will do any good, though.
Naturally, said Lanning.

The Defense Attorney was short and stubby and carried himself rather portentously, a stance that had the effect of accentuating his double chin. He stared at Professor Goodfellow, once that witness had been handed over, and said, You agreed rather readily, did you not?
The Professor said briskly, I suppose I was anxious to be rid of Dr. Lanning. I would have agreed to anything.
With the intention of forgetting about it after he left?
Well
Nevertheless, you did present the matter to a meeting of the Executive Board of the University Senate.
Yes, I did.
So that you agreed in good faith with Dr. Lannings suggestions. You werent just going along with a gag. You actually agreed enthusiastically, did you not?
I merely followed ordinary procedures.
As a matter of fact, you werent as upset about the robot as you now claim you were. You know the Three Laws of Robotics and you knew them at the time of your interview with Dr. Lanning.
Well, yes.
And you were perfectly willing to leave a robot at large and unattended.
Dr. Lanning assured me
Surely you would never have accepted his assurance if you had had the slightest doubt that the robot might be in the least dangerous.
The professor began frigidly, I had every faith in the word
That is all, said Defense abruptly.
As Professor Goodfellow, more than a bit ruffled, stood down, Justice Shane leaned forward and said, Since I am not a robotics man myself, I would appreciate knowing precisely what the Three Laws of Robotics are. Would Dr. Lanning quote them for the benefit of the court?
Dr. Lanning looked startled. He had been virtually bumping heads with the gray-haired woman at his side. He rose to his feet now and the woman looked up, tooexpressionlessly.
Dr. Lanning said, Very well, Your Honor. He paused as though about to launch into an oration and said, with laborious clarity, First Law: a robot may not injure a human being, or, through inaction, allow a human being to come to harm. Second Law: a robot must obey the orders given it by human beings, except where such orders would conflict with the First Law. Third Law: a robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.
I see, said the judge, taking rapid notes. These Laws are built into every robot, are they?
Into every one. That will be borne out by any roboticist.
And into Robot EZ-27 specifically?
Yes, Your Honor.
You will probably be required to repeat those statements under oath.
I am ready to do so, Your Honor. He sat down again.
Dr. Susan Calvin, robopsychologist-in-chief for U. S. Robots, who was the gray-haired woman sitting next to Lanning, looked at her titular superior without favor, but then she showed favor to no human being. She said, Was Goodfellows testimony accurate,
Alfred?
Essentially, muttered Lanning. He wasnt as nervous as all that about the robot and he was anxious enough to talk business with me when he heard the price. But there doesnt seem to be any drastic distortion.
Dr. Calvin said thoughtfully, It might have been wise to put the price higher than a thousand.
We were anxious to place Easy.
I know. Too anxious, perhaps. Theyll try to make it look as
though we had an ulterior motive.
Lanning looked exasperated. We did. I admitted that at the University Senate meeting.
They can make it look as if we had one beyond the one we admitted.
Scott Robertson, son of the founder of U. S. Robots and still owner of a majority of the stock, leaned over from Dr. Calvins other side and said in a kind of explosive whisper, Why cant you get Easy to talk so well know where were at?
You know he cant talk about it, Mr. Robertson.
Make him. Youre the psychologist, Dr. Calvin. Make him.
If Im the psychologist, Mr. Robertson, said Susan Calvin coldly, let me make the decisions. My robot will not be made to do anything at the price of his well-being.
Robertson frowned and might have answered, but Justice Shane was tapping his gavel in a polite sort of way and they grudgingly fell silent.
Francis J. Hart, head of the Department of English and Dean of Graduate Studies, was on the stand. He was a plump man, meticulously dressed in dark clothing of a conservative cut, and possessing several strands of hair traversing the pink top of his cranium. He sat well back in the witness chair with his hands folded neatly in his lap and displaying, from time to time, a tight-lipped smile.
He said, My first connection with the matter of the Robot EZ-27 was on the occasion of the session of the University Senate Executive Committee at which the subject was introduced by Professor Goodfellow. Thereafter, on the tenth of April of last year, we held a special meeting on the subject, during which I was in the chair.
Were minutes kept of the meeting of the Executive Committee? Of the special meeting, that is?
Well, no. It was a rather unusual meeting. The dean smiled briefly. We thought it might remain confidential.
What transpired at the meeting?

Dean Hart was not entirely comfortable as chairman of that meeting. Nor did the other members assembled seem completely calm. Only Dr. Lanning appeared at peace with himself. His tall, gaunt figure and the shock of white hair that crowned him reminded Hart of portraits he had seen of Andrew Jackson.
Samples of the robots work lay scattered along the central regions of the table and the reproduction of a graph drawn by the robot was now in the hands of Professor Minott of Physical Chemistry. The chemists lips were pursed in obvious approval.
Hart cleared his throat and said, There seems no doubt that the robot can perform certain routine tasks with adequate competence. I have gone over these, for instance, just before coming in and there is very little to find fault with.
He picked up a long sheet of printing, some three times as long as the average book page. It was a sheet of galley proof, designed to be corrected by authors before the type was set up in page form. Along both of the wide margins of the galley were proofmarks, neat and superbly legible. Occasionally, a word of print was crossed out and a new word substituted in the margin in characters so fine and regular it might easily have been print itself. Some of the corrections were blue to indicate the original mistake had been the authors, a few in red, where the printer had been wrong.
Actually, said Lanning, there is less than very little to find fault with. I should say there is nothing at all to find fault with, Dr. Hart. Im sure the corrections are perfect, insofar as the original manuscript was. If the manuscript against which this galley was corrected was at fault in a matter of fact rather than of English, the robot is not competent to correct it.
We accept that. However, the robot corrected word order on occasion and I dont think the rules of English are sufficiently hidebound for US to be sure that in each case the robots choice was the correct one.
Easys positronic brain, said Lanning, showing large teeth as he smiled, has been molded by the contents of all the standard works on the subject. Im sure you cannot point to a case where the robots choice was definitely the incorrect one.
Professor Minott looked up from the graph he still held. The question in my mind, Dr. Lanning, is why we need a robot at all, with all the difficulties in public relations that would entail. The science of automation has surely reached the point where your company could design a machine, an ordinary computer of a type known and accepted by the public, that would correct galleys.
I am sure we could, said Lanning stiffly, but such a machine would require that the galleys be translated into special symbols or, at the least, transcribed on tapes. Any corrections would emerge in symbols. You would need to keep men employed translating words to symbols, symbols to words. Furthermore, such a computer could do no other job. It couldnt prepare the graph you hold in your hand, for instance.
Minott grunted.

Lanning went on. The hallmark of the positronic robot is its flexibility. It can do a number of jobs. It is designed like a man so that it can use all the tools and machines that have, after all, been designed to be used by a man. It can talk to you and you can talk to it. You can actually reason with it up to a point. Compared to even a simple robot, an ordinary computer with a non-positronic brain is only a heavy adding machine.
Goodfellow looked up and said, If we all talk and reason with the robot, what are the chances of our confusing it? I suppose it doesnt have the capability of absorbing an infinite amount of data.
No, it hasnt. But it should last five years with ordinary use. It will know when it will require clearing, and the company will do the job without charge.
The company will?
Yes. The company reserves the right to service the robot outside the ordinary course of its duties. It is one reason we retain control of our positronic robots and lease rather than sell them. In the pursuit of its ordinary functions, any robot can be directed by any man. Outside its ordinary functions, a robot requires expert handling, and that we can give it. For instance, any of you might clear an EZ robot to an extent by telling it to forget this item or that. But you would be almost certain to phrase the order in such a way as to cause it to forget too much or too little. We would detect such tampering, because we have built-in safeguards. However, since there is no need for clearing the robot in its ordinary work, or for doing other useless things, this raises no problem.

Dean Hart touched his head as though to make sure his carefully cultivated strands lay evenly distributed and said, You are anxious to have us take the machine. Yet surely it is a losing proposition for U. S. Robots. One thousand a year is a ridiculously low price. Is it that you hope through this to rent other such machines to other universities at a more reasonable price?
Certainly thats a fair hope, said Lanning.
But even so, the number of machines you could rent would be limited. I doubt if you could make it a paying proposition.
Lanning put his elbows on the table and earnestly leaned forward. Let me put it bluntly, gentlemen. Robots cannot be used on Earth, except in certain special cases, because of prejudice against them on the part of the public. U. S. Robots is a highly successful corporation with our extraterrestrial and spaceflight markets alone, to say nothing of our computer subsidiaries. However, we are concerned with more than profits alone. It is our firm belief that the use of robots on Earth itself would mean a better life for all eventually, even if a certain amount of economic dislocation resulted at first.
The labor unions are naturally against us, but surely we may expect cooperation from the large universities. The robot, Easy, will help you by relieving you of scholastic drudgeryby assuming, if you permit it, the role of galley slave for you. Other universities and research institutions will follow your lead, and if it works out, then perhaps other robots of other types may be placed and the publics objections to them broken down by stages.
Minott murmured, Today Northeastern University, tomorrow the world.
Angrily, Lanning whispered to Susan Calvin, I wasnt nearly that eloquent and they werent nearly that reluctant. At a thousand a year, they were jumping to get Easy. Professor Minott told me hed never seen as beautiful a job as that graph he was holding and there was no mistake on the galley or anywhere else. Hart admitted it freely.
The severe vertical lines on Dr. Calvins face did not soften. You should have demanded more money than they could pay, Alfred, and let them beat you down.
Maybe, he grumbled.
Prosecution was not quite done with Professor Hart. After Dr. Lanning left, did you vote on whether to accept Robot EZ-27?
Yes, we did.
With what result?
In favor of acceptance, by majority vote.
What would you say influenced the vote? Defense objected immediately.
Prosecution rephrased the question. What influenced you, personally, in your individual vote? You did vote in favor, I think.
I voted in favor, yes. I did so largely because I was impressed by Dr. Lannings feeling that it was our duty as members of the worlds intellectual leadership to allow robotics to help Man in the solution of his problems.
In other words, Dr. Lanning talked you into it.
Thats his job. He did it very well.
Your witness.
Defense strode up to the witness chair and surveyed Professor Hart for a long moment. He said, In reality, you were all pretty eager to have Robot EZ-27 in your employ, werent you?
We thought that if it could do the work, it might be useful.
If it could do the work? I understand you examined the samples of Robot EZ-27s original work with particular care on the day of the meeting which you have just described.
Yes, I did. Since the machines work dealt primarily with the handling of the English language, and since that is my field of competence, it seemed logical that I be the one chosen to examine the work.
Very good. Was there anything on display on the table at the time of the meeting which was less than satisfactory? I have all the material here as exhibits. Can you point to a single unsatisfactory item?
Well
Its a simple question. Was there one single solitary unsatisfactory item? You inspected it. Was there?
The English professor frowned. There wasnt.
I also have some samples of work done by Robot EZ-27 during the course of his fourteen-month employ at Northeastern. Would you examine these and tell me if there is anything wrong with them in even one particular?
Hart snapped, When he did make a mistake, it was a beauty.
Answer my question, thundered Defense, and only the question I am putting to you! Is there anything wrong with the material?
Dean Hart looked cautiously at each item. Well, nothing.
Barring the matter concerning which we are here engaged. do you know of any mistake on the part of EZ-27?
Barring the matter for which this trial is being held, no.

Defense cleared his throat as though to signal end of paragraph. He said. Now about the vote concerning whether Robot EZ-27 was to be employed or not. You said there was a majority in favor. What was the actual vote?
Thirteen to one, as I remember.
Thirteen to one! More than just a majority, wouldnt you say?
No, sir!All the pedant in Dean Hart was aroused. In the English language, the word majority means more than half. Thirteen out of fourteen is a majority, nothing more.
But an almost unanimous one.
A majority all the same!
Defense switched ground. And who was the lone holdout?
Dean Hart looked acutely uncomfortable. Professor Simon Ninheimer.
Defense pretended astonishment. Professor Ninheimer? The head of the Department of Sociology?
Yes, Sir.
The plaintiff?
Yes, sir.
Defense pursed his lips. In other words, it turns out that the man bringing the action for payment of $750,000 damages against my client. United States Robots and Mechanical Men Corporation was the one who from the beginning opposed the use of the robotalthough everyone else on the Executive Committee of the University Senate was persuaded that it was a good idea.
He voted against the motion, as was his right.
You didnt mention in your description of the meeting any remarks made by Professor Ninheimer. Did he make any?
I think he spoke.
You think?
Well, he did speak.
Against using the robot?
Yes.
Was he violent about it?
Dean Hart paused. He was vehement.
Defense grew confidential. How long have you known Professor Ninheimer, Dean Hart?
About twelve years.
Reasonably well?
I should say so, yes.
Knowing him, then, would you say he was the kind of man who might continue to bear resentment against a robot, all the more so because an adverse vote had
Prosecution drowned out the remainder of the question with an indignant and vehement objection of his own. Defense motioned the witness down and Justice Shane called luncheon recess.

Robertson mangled his sandwich. The corporation would not founder for loss of three-quarters of a million, but the loss would do it no particular good. He was conscious, moreover, that there would be a much more costly long-term setback in public relations.
He said sourly, Why all this business about how Easy got into the university? What do they hope to gain?
The Attorney for Defense said quietly, A court action is like a chess game, MI. Robertson. The winner is usually the one who can see more moves ahead, and my friend at the prosecutors table is no beginner. They can show damage; thats no problem. Their main effort lies in anticipating our defense. They must be counting on us to try to show that Easy couldnt possibly have committed the offensebecause of the Laws of Robotics.
All right, said Robertson, that is our defense. An absolutely airtight one.
To a robotics engineer. Not necessarily to a judge. Theyre setting themselves up a position from which they can demonstrate that EZ-27 was no ordinary robot. It was the first of its type to be offered to the public. It was an experimental model that needed field-testing and the university was the only decent way to provide such testing. That would look plausible in the light of Dr. Lannings strong efforts to place the robot and the willingness of U. S. Robots to lease it for so little. The prosecution would then argue that the field-test proved Easy to have been a failure. Now do you see the purpose of whats been going on?
But EZ-27 was a perfectly good model, Argued Robertson. It was the twenty-seventh in production.
Which is really a bad point, said Defense somberly. What was wrong with the first twenty-six? Obviously something. Why shouldnt there be something wrong with the twenty-seventh, too?
There was nothing wrong with the first twenty-six except that they werent complex enough for the task. These were the first positronic brains of the sort to be constructed and it was rather hit-and-miss to begin with. But the Three Laws held in all of them! No robot is so imperfect that the Three Laws dont hold.
Dr. Lanning has explained this to me, Mr. Robertson, and I am willing to take his word for it. The judge, however, may not be. We are expecting a decision from an honest and intelligent man who knows no robotics and thus may be led astray. For instance, if you or Dr. Lanning or Dr. Calvin were to say on the stand that any positronic brains were constructed hit-and-miss, as you just did, prosecution would tear you apart in cross-examination. Nothing would salvage our case. So thats something to avoid.
Robertson growled, If only Easy would talk.
Defense shrugged. A robot is incompetent as a witness, so that would do us no good.
At least wed know some of the facts. Wed know how it came to do such a thing.
Susan Calvin fired up. A dullish red touched her cheeks and her voice had a trace of warmth in it. We know how Easy came to do it. It was ordered to! Ive explained this to counsel and Ill explain it to you now.
Ordered to by whom? asked Robertson in honest astonishment. (No one ever told him anything, he thought resentfully. These research people considered themselves the owners of U. S. Robots, by God!)
By the plaintiff, said Dr. Calvin. In heavens name, why?
I dont know why yet. Perhaps just that we might be sued, that he might gain some cash. There were blue glints in her eyes as she said that.
Then why doesnt Easy say so?
Isnt that obvious? Its been ordered to keep quiet about the matter.
Why should that be obvious? demanded Robertson truculently. Well, its obvious to me. Robot psychology is my profession. If
Easy will not answer questions about the matter directly, he will answer questions on the fringe of the matter. By measuring increased hesitation in his answers as the central question is approached, by measuring the area of blankness and the intensity of counterpotentials set up, it is possible to tell with scientific precision that his troubles are the result of an order not to talk, with its strength based on First Law. In other words, hes been told that if he talks, harm will be done a human being. Presumably harm to the unspeakable Professor Ninheimer, the plaintiff, who, to the robot, would seem a human being.
Well, then, said Robertson, cant you explain that if he keeps quiet, harm will be done to U. S. Robots?
U. S. Robots is not a human being and the First Law of Robotics does not recognize a corporation as a person the way ordinary laws do. Besides, it would be dangerous to try to lift this particular sort of inhibition. The person who laid it on could lift it off least dangerously, because the robots motivations in that respect are centered on that person. Any other course She shook her head and grew almost impassioned. I wont let the robot be damaged!
Lanning interrupted with the air of bringing sanity to the problem. It seems to me that we have only to prove a robot incapable of the act of which Easy is accused. We can do that.
Exactly, said Defense, in annoyance. You can do that. The only witnesses capable of testifying to Easys condition and to the nature of Easys state of mind are employees of U. S. Robots. The judge cant possibly accept their testimony as unprejudiced.
How can he deny expert testimony?
By refusing to be convinced by it. Thats his right as the judge. Against the alternative that a man like Professor Ninheimer deliberately set about ruining his own reputation, even for a sizable sum of money, the judge isnt going to accept the technicalities of your engineers. The judge is a man, after all. If he has to choose between a man doing an impossible thing and a robot doing an impossible thing, hes quite likely to decide in favor of the man.
A man can do an impossible thing, said Lanning, because we dont know all the complexities of the human mind and we dont know what, in a given human mind, is impossible and what is not. We do know what is really impossible to a robot.
Well, well see if we cant convince the judge of that, Defense replied wearily.
If all you say is so, rumbled Robertson, I dont see how you can.
Well see. Its good to know and be aware of the difficulties involved, but lets not be too downhearted. Ive tried to look ahead a few moves in the chess game, too. With a stately nod in the direction of the robopsychologist, he added, With the help of the good lady here.
Lanning looked from one to the other and said, What the devil is this?
But the bailiff thrust his head into the room and announced somewhat breathlessly that the trial was about to resume.
They took their seats, examining the man who had started all the trouble.
Simon Ninheimer owned a fluffy head of sandy hair, a face that narrowed past a beaked nose toward a pointed chin, and a habit of sometimes hesitating before key words in his conversation that gave him an air of a seeker after an almost unbearable precision. When he said, The Sun rises in theuheast, 11 one was certain he had given due consideration to the possibility that it might at some time rise in the west.
Prosecution said, Did you oppose employment of Robot EZ-27 by the university?
I did, sir.
Why was that?
I did not feel that we understood theuhmotives of U. S. Robots thoroughly. I mistrusted their anxiety to place the robot with us.
Did you feel that it was capable of doing the work that it was allegedly designed to do?
I know for a fact that it was not.
Would you state your reasons?

Simon Ninheimers book, entitled Social Tensions Involved in Space-Flight and Their Resolution, had been eight years in the making. Ninheimers search for precision was not confined to his habits of speech, and in a subject like sociology, almost inherently imprecise, it left him breathless.
Even with the material in galley proofs, he felt no sense of completion. Rather the reverse, in fact. Staring at the long strips of print, he felt only the itch to tear the lines of type apart and rearrange them differently.
Jim Baker, Instructor and soon to be Assistant Professor of Sociology, found Ninheimer, three days after the first batch of galleys had arrived from the printer, staring at the handful of paper in abstraction. The galleys came in three copies: one for Ninheimer to proofread, one for Baker to proofread independently, and a third, marked Original, which was to receive the final corrections, a combination of those made by Ninheimer and by Baker, after a conference at which possible conflicts and disagreements were ironed out. This had been their policy on the several papers on which they had collaborated in the past three years and it worked well.
Baker, young and ingratiatingly soft-voiced, had his own copies of the galleys in his hand. He said eagerly, Ive done the first chapter and it contains some typographical beauts.
The first chapter always has them, said Ninheimer distantly. Do you want to go over it now?
Ninheimer brought his eyes to grave focus on Baker. I havent done anything on the galleys, Jim. I dont think Ill bother.
Baker looked confused. Not bother?
Ninheimer pursed his lips. Ive asked about theuhworkload of the machine. After all, he was originallyuhpromoted as a proofreader. Theyve set a schedule.
The machine? You mean Easy?
I believe that is the foolish name they gave it.
But, Dr. Ninheimer, I thought you were staying clear of it
I seem to be the only one doing so. Perhaps I ought to take my share of theuhadvantage.
Oh. Well, I seem to have wasted time on this first chapter, then, said the younger man ruefully.
Not wasted. We can compare the machines result with yours as a check.
If you want to, but
Yes?
I doubt that well find anything wrong with Easys work. Its supposed never to have made a mistake.
I dare say, said Ninheimer dryly.

The first chapter was brought in again by Baker four days later. This time it was Ninheimers copy, fresh from the special annex that had been built to house Easy and the equipment it used.
Baker was jubilant. Dr. Ninheimer, it not only caught everything I caughtit found a dozen errors I missed! The whole thing took it twelve minutes!
Ninheimer looked over the sheaf, with the neatly printed marks and symbols in the margins. He said, It is not as complete as you and I would have made it. We would have entered an insert on Suzukis work on the neurological effects of low gravity.
You mean his paper in Sociological Reviews?
Of course.
Well, you cant expect impossibilities of Easy. It cant read the literature for us.
I realize that. As a matter of fact, I have prepared the insert. I will see the machine and make certain it knows how touhhandle inserts.
It will know.
I prefer to make certain.
Ninheimer had to make an appointment to see Easy, and then could get nothing better than fifteen minutes in the late evening.
But the fifteen minutes turned out to be ample. Robot EZ-27 understood the matter of inserts at once.
Ninheimer found himself uncomfortable at close quarters with the robot for the first time. Almost automatically, as though it were human, he found himself asking, Are you happy with your work?
Most happy, Professor Ninheimer, said Easy solemnly, the photocells that were its eyes gleaming their normal deep red.
You know me?
From the fact that you present me with additional material to include in the galleys, it follows that you are the author. The authors name, of course, is at the head of each sheet of galley proof.
I see. You makeuhdeductions, then. Tell mehe couldnt resist the questionwhat do you think of the book so far?
Easy said, I find it very pleasant to work with.
Pleasant? That is an odd word for auha mechanism without emotion. Ive been told you have no emotion.
The words of your book go in accordance with my circuits, Easy explained. They set up little or no counterpotentials. It is in my brain paths to translate this mechanical fact into a word such as pleasant. The emotional context is fortuitous.
I see. Why do you find the book pleasant?
It deals with human beings, Professor, and not with inorganic materials or mathematical symbols. Your book attempts to understand human beings and to help increase human happiness.
And this is what you try to do and so my book goes in accordance with your circuits? Is that it?
That is it, Professor.
The fifteen minutes were up. Ninheimer left and went to the university library, which was on the point of closing. He kept them open long enough to find an elementary text on robotics. He took it home with him.
Except for occasional insertion of late material, the galleys went to Easy and from him to the publishers with little intervention from Ninheimer at firstand none at all later.
Baker said, a little uneasily, It almost gives me a feeling of uselessness.
It should give you a feeling of having time to begin a new project, said Ninheimer, without looking up from the notations he was making in the current issue of Social Science Abstracts.
Im just not used to it. I keep worrying about the galleys. Its silly, I know.
It is.
The other day I got a couple of sheets before Easy sent them off to
What! Ninheimer looked up, scowling. The copy of Abstracts slid shut. Did you disturb the machine at its work?
Only for a minute. Everything was all right. Oh, it changed one word. You referred to something as criminal; it changed the word to reckless. It thought the second adjective fit in better with the context.
Ninheimer grew thoughtful. What did you think?
You know, I agreed with it. I let it stand.
Ninheimer turned in his swivel-chair to face his young associate. See here, I wish you wouldnt do this again. If I am to use the machine, I wish theuhfull advantage of it. If I am to use it and lose youruhservices anyway because you supervise it when the whole point is that it requires no supervision, I gain nothing. Do you see?
Yes, Dr. Ninheimer, said Baker, subdued. The advance copies of Social Tensions arrived in Dr. Ninheimers office on the eighth of May. He looked through it briefly, flipping pages and pausing to read a paragraph here and there. Then he put his copies away.
As he explained later, he forgot about it. For eight years, he had worked at it, but now, and for months in the past, other interests had engaged him while Easy had taken the load of the book off his shoulders. He did not even think to donate the usual complimentary copy to the university library. Even Baker, who had thrown himself into work and had steered clear of the department head since receiving his rebuke at their last meeting, received no copy.
On the sixteenth of June that stage ended. Ninheimer received a phone call and stared at the image in the plate with surprise.
Speidell! Are you in town?
No, sir. Im in Cleveland. Speidells voice trembled with emotion.
Then why the call?
Because Ive just been looking through your new book! Ninheimer, are you mad? Have you gone insane?

Ninheimer stiffened. Is somethinguhwrong? he asked in alarm.
Wrong? I refer you to page 562. What in blazes do you mean by interpreting my work as you do? Where in the paper cited do I make the claim that the criminal personality is nonexistent and that it is the law-enforcement agencies that are the true criminals? Here, let me quote
Wait! Wait! cried Ninheimer, trying to find the page. Let me see. Let me see...Good God!
Well?
Speidell, I dont see how this could have happened. I never wrote this.
But thats whats printed! And that distortion isnt the worst. You look at page 690 and imagine what Ipatiev is going to do to you when he sees the hash youve made of his findings! Look, Ninheimer, the book is riddled with this sort of thing. I dont know what you were thinking ofbut theres nothing to do but get the book off the market. And youd better be prepared for extensive apologies at the next Association meeting!
Speidell, listen to me But Speidell had flashed off with a force that had the plate glowing with after-images for fifteen seconds.
It was then that Ninheimer went through the book and began marking off passages with red ink.
He kept his temper remarkably well when he faced Easy again, but his lips were pale. He passed the book to Easy and said, Will you read the marked passages on pages 562, 631, 664 and 690?
Easy did so in four glances. Yes, Professor Ninheimer.
This is not as I had it in the original galleys.
No, sir. It is not.
Did you change it to read as it now does?
Yes, sir.
Why?
Sir, the passages as they read in your version were most uncomplimentary to certain groups of human beings. I felt it advisable to change the wording to avoid doing them harm.
How dared you do such a thing?
The First Law, Professor, does not let me, through any inaction, allow harm to come to human beings. Certainly, considering your reputation in the world of sociology and the wide circulation your book would receive among scholars, considerable harm would come to a number of the human beings you speak of.
But do you realize the harm that will come to me now?
It was necessary to choose the alternative with less harm. Professor Ninheimer, shaking with fury, staggered away. It was clear to him that U. S. Robots would have to account to him for this.

There was some excitement at the defendants table, which increased as Prosecution drove the point home.
Then Robot EZ-27 informed you that the reason for its action was based on the First Law of Robotics?
That is correct, sir.
That, in effect, it had no choice?
Yes, sir.
It follows then that U. S. Robots designed a robot that would of necessity rewrite books to accord with its own conceptions of what was right. And yet they palmed it off as simple proofreader. Would you say that?
Defense objected firmly at once, pointing out that the witness was being asked for a decision on a matter in which he had no competence. The judged admonished Prosecution in the usual terms, but there was no doubt that the exchange had sunk homenot least upon the attorney for the Defense.
Defense asked for a short recess before beginning cross-examination, using a legal technicality for the purpose that got him five minutes.
He leaned over toward Susan Calvin. Is it possible, Dr. Calvin, that Professor Ninheimer is telling the truth and that Easy was motivated by the First Law?
Calvin pressed her lips together, then said, No. It isnt possible. The last part of Ninheimers testimony is deliberate perjury. Easy is not designed to be able to judge matters at the stage of abstraction represented by an advanced textbook on sociology. It would never be able to tell that certain groups of humans would be harmed by a phrase in such a book. Its mind is simply not built for that.
I suppose, though, that we cant prove this to a layman, said Defense pessimistically.
No, Admitted Calvin. The proof would be highly complex. Our way out is still what it was. We must prove Ninheimer is lying, and nothing he has said need change our plan of attack.
Very well, Dr. Calvin, said Defense, I must accept your word in this. Well go on as planned.
In the courtroom, the judges gavel rose and fell and Dr. Ninheimer took the stand once more. He smiled a little as one who feels his position to be impregnable and rather enjoys the prospect of countering a useless attack.
Defense approached warily and began softly. Dr. Ninheimer, do you mean to say that you were completely unaware of these alleged changes in your manuscript until such time as Dr. Speidell called you on the sixteenth of June?
That is correct, sir.
Did you never look at the galleys after Robot EZ-27 had proofread them?
At first I did, but it seemed to me a useless task. I relied on the claims of U. S. Robots. The absurduhchanges were made only in the last quarter of the book after the robot, I presume, had learned enough about sociology
Never mind your presumptions! said Defense. I understood your colleague, Dr. Baker, saw the later galleys on at least one occasion. Do you remember testifying to that effect?
Yes, sir. As I said, he told me about seeing one page, and even there, the robot had changed a word.

Again Defense broke in. Dont you find it strange, sir, that after over a year of implacable hostility to the robot, after having voted against it in the first place and having refused to put it to any use whatever, you suddenly decided to put your book, your magnum opus, into its hands?
I dont find that strange. I simply decided that I might as well use the machine.
And you were so confident of Robot EZ-27all of a suddenthat you didnt even bother to check your galleys?
I told you I wasuhpersuaded by U. S. Robots propaganda.
So persuaded that when your colleague, Dr. Baker, attempted to check on the robot, you berated him soundly?
I didnt berate him. I merely did not wish to have himuhwaste his time. At least, I thought then it was a waste of time. I did not see the significance of that change in a word at the
Defense said with heavy sarcasm, I have no doubt you were instructed to bring up that point in order that the word-change be entered in the record He altered his line to forestall objection and said, The point is that you were extremely angry with Dr. Baker.
No, sir. Not angry.
You didnt give him a copy of your book when you received it.
Simple forgetfulness. I didnt give the library its copy, either.
Ninheimer smiled cautiously. Professors are notoriously absentminded.
Defense said, Do you find it strange that, after more than a year of perfect work, Robot EZ-27 should go wrong on your book? On a book, that is, which was written by you, who was, of all people, the most implacably hostile to the robot?
My book was the only sizable work dealing with mankind that it had to face. The Three Laws of Robotics took hold then.
Several times, Dr. Ninheimer, said Defense, you have tried to sound like an expert on robotics. Apparently you suddenly grew interested in robotics and took out books on the subject from the library. You testified to that effect, did you not?
One book, sir. That was the result of what seems to me to have beenuhnatural curiosity.
And it enabled you to explain why the robot should, as you allege, have distorted your book?
Yes, sir.
Very convenient. But are you sure your interest in robotics was not intended to enable you to manipulate the robot for your own purposes?
Ninheimer flushed. Certainly not, sir! Defenses voice rose. In fact, are you sure the alleged altered passages were not as you had them in the first place?
The sociologist half-rose. Thatsuhuhridiculous! I have the galleys
He had difficulty speaking and Prosecution rose to insert smoothly, With your permission, Your Honor, I intend to introduce as evidence the set of galleys given by Dr. Ninheimer to Robot EZ-27 and the set of galleys mailed by Robot EZ-27 to the publishers. I will do so now if my esteemed colleague so desires, and will be willing to allow a recess in order that the two sets of galleys may be compared.

Defense waved his hand impatiently. That is not necessary. My honored opponent can introduce those galleys whenever he chooses. Im sure they will show whatever discrepancies are claimed by the plaintiff to exist. What I would like to know of the witness, however, is whether he also has in his possession Dr. Bakers galleys.
Dr. Bakers galleys? Ninheimer frowned. He was not yet quite master of himself.
Yes, Professor! I mean Dr. Bakers galleys. You testified to the effect that Dr. Baker had received a separate copy of the galleys. I will have the clerk read your testimony if you are suddenly a selective type of amnesiac. Or is it just that professors are, as you say, notoriously absent-minded?
Ninheimer said, I remember Dr. Bakers galleys. They werent necessary once the job was placed in the care of the proofreading machine
So you burned them?
No. I put them in the waste basket.
Burned them, dumped themwhats the difference? The point is you got rid of them.
Theres nothing wrong began Ninheimer weakly.
Nothing wrong? thundered Defense. Nothing wrong except that there is now no way we can check to see if, on certain crucial galley sheets, you might not have substituted a harmless blank one from Dr. Bakers copy for a sheet in your own copy which you had deliberately mangled in such a way as to force the robot to
Prosecution shouted a furious objection. Justice Shane leaned forward, his round face doing its best to assume an expression of anger equivalent to the intensity of the emotion felt by the man.
The judge said, Do you have any evidence, Counselor, for the extraordinary statement you have just made?
Defense said quietly, No direct evidence, Your Honor. But I would like to point out that, viewed properly, the sudden conversion of the plaintiff from anti-roboticism, his sudden interest in robotics, his refusal to check the galleys or to allow anyone else to check them, his careful neglect to allow anyone to see the book immediately after publication, all very clearly point
Counselor, interrupted the judge impatiently, this is not the place for esoteric deductions. The plaintiff is not on trial. Neither are you prosecuting him. I forbid this line of attack and I can only point out that the desperation that must have induced you to do this cannot help but weaken your case. If you have legitimate questions to ask, Counselor, you may continue with your cross-examination. But I warn you against another such exhibition in this courtroom.
I have no further questions, Your Honor.
Robertson whispered heatedly as counsel for the Defense returned to his table, What good did that do, for Gods sake? The judge is dead-set against you now.
Defense replied calmly, But Ninheimer is good and rattled. And weve set him up for tomorrows move. Hell be ripe.
Susan Calvin nodded gravely.

The rest of Prosecutions case was mild in comparison. Dr. Baker was called and bore out most of Ninheimers testimony. Drs. Speidell and Ipatiev were called, and they expounded most movingly on their shock and dismay at certain quoted passages in Dr. Ninheimers book. Both gave their professional opinion that Dr. Ninheimers professional reputation had been seriously impaired.
The galleys were introduced in evidence, as were copies of the finished book.
Defense cross-examined no more that day. Prosecution rested and the trial was recessed till the next morning.
Defense made his first motion at the beginning of the proceedings on the second day. He requested that Robot EZ-27 be admitted as a spectator to the proceedings.
Prosecution objected at once and Justice Shane called both to the bench.
Prosecution said hotly, This is obviously illegal. A robot may not be in any edifice used by the general public.
This courtroom, pointed out Defense, is closed to all but those having an immediate connection with the case.
A large machine of known erratic behavior would disturb my clients and my witnesses by its very presence! It would make hash out of the proceedings.
The judge seemed inclined to agree. He turned to Defense and said rather unsympathetically, What are the reasons for your request?
Defense said, It will be our contention that Robot EZ-27 could not possibly, by the nature of its construction, have behaved as it has been described as behaving. It will be necessary to present a few demonstrations.
Prosecution said, I dont see the point, Your Honor. Demonstrations conducted by men employed at U. S. Robots are worth little as evidence when U. S. Robots is the defendant.
Your Honor, said Defense, the validity of any evidence is for you to decide, not for the Prosecuting Attorney. At least, that is my understanding.
Justice Shane, his prerogatives encroached upon, said, Your understanding is correct. Nevertheless, the presence of a robot here does raise important legal questions.
Surely, Your Honor, nothing that should be allowed to override the requirements of justice. If the robot is not present, we are prevented from presenting our only defense.
The judge considered. There would be the question of transporting the robot here.
That is a problem with which U. S. Robots has frequently been faced. We have a truck parked outside the courtroom, constructed according to the laws governing the transportation of robots. Robot EZ-27 is in a packing case inside with two men guarding it. The doors to the truck are properly secured and all other necessary precautions have been taken.
You seem certain, said Justice Shane, in renewed ill-temper, that judgment on this point will be in your favor.
Not at all, Your Honor. If it is not, we simply turn the truck about. I have made no presumptions concerning your decision.
The judge nodded. The request on the part of the Defense is granted.
The crate was carried in on a large dolly and the two men who handled it opened it. The courtroom was immersed in a dead silence.

Susan Calvin waited as the thick slabs of celluform went down, then held out one hand. Come, Easy.
The robot looked in her direction and held out its large metal arm. It towered over her by two feet but followed meekly, like a child in the clasp of its mother. Someone giggled nervously and choked it off at a hard glare from Dr. Calvin.
Easy seated itself carefully in a large chair brought by the bailiff, which creaked but held.
Defense said, When it becomes necessary, Your Honor, we will prove that this is actually Robot EZ-27, the specific robot in the employ of Northeastern University during the period of time with which we are concerned.
Good, His Honor said. That will be necessary. I, for one, have no idea how you can tell one robot from another.
And now, said Defense, I would like to call my first witness to the stand. Professor Simon Ninheimer, please.
The clerk hesitated, looked at the judge. Justice Shane asked, with visible surprise, You are calling the plaintiff as your witness?
Yes, Your Honor.
I hope that youre aware that as long as hes your witness, you will be allowed none of the latitude you might exercise if you were cross-examining an opposing witness.
Defense said smoothly, My only purpose in all this is to arrive at the truth. It will not be necessary to do more than ask a few polite questions.
Well, said the judge dubiously, youre the one handling the case. Call the witness.
Ninheimer took the stand and was informed that he was still under oath. He looked more nervous than he had the day before, almost apprehensive.
But Defense looked at him benignly.
Now, Professor Ninheimer, you are suing my clients in the amount of $750,000.
That is theuhsum. Yes.
That is a great deal of money.
I have suffered a great deal of harm.
Surely not that much. The material in question involves only a few passages in a book. Perhaps these were unfortunate passages, but after all, books sometimes appear with curious mistakes in them.
Ninheimers nostrils flared. Sir, this book was to have been the climax of my professional career! Instead, it makes me look like an incompetent scholar, a perverter of the views held by my honored friends and associates, and a believer of ridiculous anduhoutmoded viewpoints. My reputation is irretrievably shattered! I can never hold up my head in anyuhassemblage of scholars, regardless of the outcome of this trial. I certainly cannot continue in my career, which has been the whole of my life. The very purpose of my life has beenuhaborted and destroyed.
Defense made no attempt to interrupt the speech, but stared abstractedly at his fingernails as it went on.
He said very soothingly, But surely, Professor Ninheimer, at your present age, you could not hope to earn more thanlet us be generous$l5O,OOO during the remainder of your life. Yet you are asking the court to award you five times as much.

Ninheimer said, with an even greater burst of emotion, It is not in my lifetime alone that I am ruined. I do not know for how many generations I shall be pointed at by sociologists as auha fool or maniac. My real achievements will be buried and ignored. I am ruined not only until the day of my death, but for all time to come, because there will always be people who will not believe that a robot made those insertions
It was at this point that Robot EZ-27 rose to his feet. Susan Calvin made no move to stop him. She sat motionless, staring straight ahead. Defense sighed softly.
Easys melodious voice carried clearly. It said, I would like to explain to everyone that I did insert certain passages in the galley proofs that seemed directly opposed to what had been there at first
Even the Prosecuting Attorney was too startled at the spectacle of a seven-foot robot rising to address the court to be able to demand the stopping of what was obviously a most irregular procedure.
When he could collect his wits, it was too late. For Ninheimer rose in the witness chair, his face working.
He shouted wildly, Damn you, you were instructed to keep your mouth shut about
He ground to a choking halt, and Easy was silent, too. Prosecution was on his feet now, demanding that a mistrial be declared.
Justice Shane banged his gavel desperately. Silence! Silence! Certainly there is every reason here to declare a mistrial, except that in the interests of justice I would like to have Professor Ninheimer complete his statement. I distinctly heard him say to the robot that the robot had been instructed to keep its mouth shut about something. There was no mention in your testimony, Professor Ninheimer, as to any instructions to the robot to keep silent about anything!
Ninheimer stared wordlessly at the judge. Justice Shane said, Did you instruct Robot EZ-27 to keep silent about something? And if so, about what?
Your Honor began Ninheimer hoarsely, and couldnt continue.
The judges voice grew sharp. Did you, in fact, order the inserts in question to be made in the galleys and then order the robot to keep quiet about your part in this?
Prosecution objected vigorously, but Ninheimer shouted, Oh, whats the use? Yes! Yes!And he ran from the witness stand. He was stopped at the door by the bailiff and sank hopelessly into one of the last rows of seats, head buried in both hands.
Justice Shane said, It is evident to me that Robot EZ-27 was brought here as a trick. Except for the fact that the trick served to prevent a serious miscarriage of justice, I would certainly hold attorney for the Defense in contempt. It is clear now, beyond any doubt, that the plaintiff has committed what is to me a completely inexplicable fraud since, apparently, he was knowingly ruining his career in the process
Judgment, of course, was for the defendant.

Dr. Susan Calvin had herself announced at Dr. Ninheimers bachelor quarters in University Hall. The young engineer who had driven the car offered to go up with her, but she looked at him scornfully.
Do you think hell assault me? Wait down here.
Ninheimer was in no mood to assault anyone. He was packing, wasting no time, anxious to be away before the adverse conclusion of the trial became general knowledge.
He looked at Calvin with a queerly defiant air and said, Are you coming to warn me of a countersuit? If so, it will get you nothing. I have no money, no job, no future. I cant even meet the costs of the trial.
If youre looking for sympathy, said Calvin coldly, dont look for it here. This was your doing. However, there will be no countersuit, neither of you nor of the university. We will even do what we can to keep you from going to prison for perjury. We arent vindictive.
Oh, is that why Im not already in custody for forswearing myself? I had wondered. But then, he added bitterly, why should you be vindictive? You have what you want now.
Some of what we want, yes, said Calvin. The university will keep Easy in its employ at a considerably higher rental fee. Furthermore, certain underground publicity concerning the trial will make it possible to place a few more of the EZ models in other institutions without danger of a repetition of this trouble.
Then why have you come to see me?
Because I dont have all of what I want yet. I want to know why you hate robots as you do. Even if you had won the case, your reputation would have been ruined. The money you might have obtained could not have compensated for that. Would the satisfaction of your hatred for robots have done so?
Are you interested in human minds, Dr. Calvin? asked Ninheimer, with acid mockery.
Insofar as their reactions concern the welfare of robots, yes. For that reason, I have learned a little of human psychology.
Enough of it to be able to trick met
That wasnt hard, said Calvin, without pomposity. The difficult thing was doing it in such a way as not to damage Easy.
It is like you to be more concerned for a machine than for a man. He looked at her with savage contempt.
It left her unmoved. It merely seems so, Professor Ninheimer. It is only by being concerned for robots that one can truly be concerned for twenty-first-century man. You would understand this if you were a roboticist.
I have read enough robotics to know I dont want to be a roboticist!
Pardon me, you have read a book on robotics. It has taught you nothing. You learned enough to know that you could order a robot to do many things, even to falsify a book, if you went about it properly. You learned enough to know that you could not order him to forget something entirely without risking detection, but you thought you could order him into simple silence more safely. You were wrong.
You guessed the truth from his silencer It wasnt guessing. You were an amateur and didnt know enough to cover your tracks completely. My only problem was to prove the matter to the judge and you were kind enough to help us there, in your ignorance of the robotics you claim to despise.

Is there any purpose in this discussion? asked Ninheimer wearily.
For me, yes, said Susan Calvin, because I want you to understand how completely you have misjudged robots. You silenced Easy by telling him that if he told anyone about your own distortion of the book, you would lose your job. That set up a certain potential within Easy toward silence, one that was strong enough to resist our efforts to break it down. We would have damaged the brain if we had persisted.
On the witness stand, however, you yourself put up a higher counterpotential. You said that because people would think that you, not a robot, had written the disputed passages in the book, you would lose far more than just your job. You would lose your reputation, your standing, your respect, your reason for living. You would lose the memory of you after death. A new and higher potential was set up by youand Easy talked.
Oh, God, said Ninheimer, turning his head away. Calvin was inexorable. She said, Do you understand why he talked? It was not to accuse you, but to defend you! It can be mathematically shown that he was about to assume full blame for your crime, to deny that you had anything to do with it. The First Law required that. He was going to lieto damage himselfto bring monetary harm to a corporation. All that meant less to him than did the saving of you. If you really understood robots and robotics, you would have let him talk. But you did not understand, as I was sure you wouldnt, as I guaranteed to the defense attorney that you wouldnt. You were certain, in your hatred of robots, that Easy would act as a human being would act and defend itself at your expense. So you flared out at him in panicand destroyed yourself.
Ninheimer said with feeling, I hope some day your robots turn on you and kill you!
Dont be foolish, said Calvin. Now I want you to explain why youve done all this.
Ninheimer grinned a distorted, humorless grin. I am to dissect my mind, am I, for your intellectual curiosity, in return for immunity from a charge of perjury?
Put it that way if you like, said Calvin emotionlessly. But explain.
So that you can counter future anti-robot attempts more efficiently? With greater understanding?
I accept that.
You know, said Ninheimer, Ill tell youjust to watch it do you no good at all. You cant understand human motivation. You can only understand your damned machines because youre a machine yourself, with skin on.

He was breathing hard and there was no hesitation in his speech, no searching for precision. It was as though he had no further use for precision.
He said, For two hundred and fifty years, the machine has been replacing Man and destroying the handcraftsman. Pottery is spewed out of molds and presses. Works of art have been replaced by identical gimcracks stamped out on a die. Call it progress, if you wish! The artist is restricted to abstractions, confined to the world of ideas. He must design something in mindand then the machine does the rest.
Do you suppose the potter is content with mental creation? Do you suppose the idea is enough? That there is nothing in the feel of the clay itself, in watching the thing grow as hand and mind work together? Do you suppose the actual growth doesnt act as a feedback to modify and improve the idea?
You are not a potter, said Dr. Calvin. I am a creative artist! I design and build articles and books. There is more to it than the mere thinking of words and of putting them in the right order. If that were all, there would be no pleasure in it, no return.
A book should take shape in the hands of the writer. One must actually see the chapters grow and develop. One must work and rework and watch the changes take place beyond the original concept even. There is taking the galleys in hand and seeing how the sentences look in print and molding them again. There are a hundred contacts between a man and his work at every stage of the game and the contact itself is pleasurable and repays a man for the work he puts into his creation more than anything else could. Your robot would take all that away.
So does a typewriter. So does a printing press. Do you propose to return to the hand illumination of manuscripts?
Typewriters and printing presses take away some, but your robot would deprive us of all. Your robot takes over the galleys. Soon it, or other robots, would take over the original writing, the searching of the sources, the checking and cross-checking of passages, perhaps even the deduction of conclusions. What would that leave the scholar? One thing onlythe barren decisions concerning what orders to give the robot next! I want to save the future generations of the world of scholarship from such a final hell. That meant more to me than even my own reputation and so I set out to destroy U. S. Robots by whatever means.
You were bound to fail, said Susan Calvin. I was bound to try, said Simon Ninheimer. Calvin turned and left. She did her best to feel no pang of sympathy for the broken man.
She did not entirely succeed.


Christmas Without Rodney

It all started with Gracie (my wife of nearly forty years) wanting to give Rodney time off for the holiday season and it ended with me in an absolutely impossible situation. Ill tell you about it if you dont mind because Ive got to tell somebody. Naturally, Im changing names and details for our own protection.
It was just a couple of months ago, mid-December, and Gracie said to me, Why dont we give Rodney time off for the holiday season? Why shouldnt he celebrate Christmas, too?
I remember I had my optics unfocused at the time (theres a certain amount of relief in letting things go hazy when you want to rest or just listen to music) but I focused them quickly to see if Gracie were smiling or had a twinkle in her eye. Not that she has much of a sense of humor, you understand.
She wasnt smiling. No twinkle. I said, Why on Earth should we give him time off?
Why not?
Do you want to give the freezer a vacation, the sterilizer, the holoviewer? Shall we just turn off the power supply?
Come, Howard, she said. Rodney isnt a freezer or a sterilizer. Hes a person.
Hes not a person. Hes a robot. He wouldnt want a vacation.
How do you know? And hes a person. He deserves a chance to rest and just revel in the holiday atmosphere.
I wasnt going to argue that person thing with her. I know youve all read those polls which show that women are three times as likely to resent and fear robots as men are. Perhaps thats because robots tend to do what was once called, in the bad old days, womens work and women fear being made useless, though I should think theyd be delighted. In any case, Gracie is delighted and she simply adores Rodney. (Thats her word for it. Every other day she says, I just adore Rodney.)
Youve got to understand that Rodney is an old-fashioned robot whom weve had about seven years. Hes been adjusted to fit in with our old-fashioned house and our old-fashioned ways and Im rather pleased with him myself. Sometimes I wonder about getting one of those slick, modern jobs, which are automated to death, like the one our son, DeLancey, has, but Gracie would never stand for it.
But then I thought of DeLancey and I said, How are we going to give Rodney time off, Gracie? DeLancey is coming in with that gorgeous wife of his (I was using gorgeous in a sarcastic sense, but Gracie didnt noticeits amazing how she insists on seeing a good side even when it doesnt exist) and how are we going to have the house in good shape and meals made and all the rest of it without Rodney?
But thats just it, she said, earnestly. DeLancey and Hortense could bring their robot and he could do it all. You know they dont think much of Rodney, and theyd love to show what theirs can do and Rodney can have a rest.
I grunted and said, If it will make you happy, I suppose we can do it. Itll only be for three days. But I dont want Rodney thinking hell get every holiday off.
It was another joke, of course, but Gracie just said, very earnestly, No, Howard, I will talk to him and explain its only just once in a while.
She cant quite understand that Rodney is controlled by the three laws of robotics and that nothing has to be explained to him.
So I had to wait for DeLancey and Hortense, and my heart was heavy. DeLancey is my son, of course, but hes one of your upwardly mobile, bottom-line individuals. He married Hortense because she has excellent connections in business and can help him in that upward shove. At least, I hope so, because if she has another virtue I have never discovered it.
They showed up with their robot two days before Christmas. The robot was as glitzy as Hortense and looked almost as hard. He was polished to a high gloss and there was none of Rodneys clumping. Hortenses robot (Im sure she dictated the design) moved absolutely silently. He kept showing up behind me for no reason and giving me heart-failure every time I turned around and bumped into him.
Worse, DeLancey brought eight-year-old LeRoy. Now hes my grandson, and I would swear to Hortenses fidelity because Im sure no one would voluntarily touch her, but Ive got to admit that putting him through a concrete mixer would improve him no end.
He came in demanding to know if we had sent Rodney to the metal-reclamation unit yet. (He called it the bust-up place.) Hortense sniffed and said, Since we have a modern robot with us, I hope you keep Rodney out of sight.
I said nothing, but Gracie said, Certainly, dear. In fact, weve given Rodney time off.
DeLancey made a face but didnt say anything. He knew his mother.
I said, pacifically, Suppose we start off by having Rambo make something good to drink, eh? Coffee, tea, hot chocolate, a bit of brandy
Rambo was their robots name. I dont know why except that it starts with R. Theres no law about it, but youve probably noticed for yourself that almost every robot has a name beginning with R. R for robot, I suppose. The usual name is Robert. There must be a million robot Roberts in the northeast corridor alone.
And frankly, its my opinion thats the reason human names just dont start with R any more. You get Bob and Dick but not Robert or Richard. You get Posy and Trudy, but not Rose or Ruth. Sometimes you get unusual Rs. I know of three robots called Rutabaga, and two that are Rameses. But Hortense is the only one I know who named a robot Rambo, a syllable-combination Ive never encountered, and Ive never liked to ask why. I was sure the explanation would prove to be unpleasant.
Rambo turned out to be useless at once. He was, of course, programmed for the DeLancey/Hortense menage and that was utterly modern and utterly automated. To prepare drinks in his own home, all Rambo had to do was to press appropriate buttons. (Why anyone would need a robot to press buttons, I would like to have explained to me!)
He said so. He turned to Hortense and said in a voice like honey (it wasnt Rodneys city-boy voice with its trace of Brooklyn), The equipment is lacking, madam.
And Hortense drew a sharp breath. You mean you still dont have a robotized kitchen, grandfather? (She called me nothing at all, until LeRoy was born, howling of course, and then she promptly called me grandfather. Naturally, she never called me Howard. That would tend to show me to be human, or, more unlikely, show her to be human.)
I said, Well, its robotized when Rodney is in it.
I dare say, she said. But were not living in the twentieth century, grandfather.
I thought: How I wish we werebut I just said, Well, why not program Rambo how to operate our controls. Im sure he can pour and mix and heat and do whatever else is necessary.
Im sure he can, said Hortense, but thank Fate he doesnt have to. Im not going to interfere with his programming. It will make him less efficient.
Gracie said, worried, but amiable, But if we dont interfere with his programming, then Ill just have to instruct him, step by step, but I dont know how its done. Ive never done it.
I said, Rodney can tell him.
Gracie said, Oh, Howard, weve given Rodney a vacation.
I know, but were not going to ask him to do anything; just tell Rambo here what to do and then Rambo can do it.
Whereupon Rambo said stiffly, Madam, there is nothing in my programming or in my instructions that would make it mandatory for me to accept orders given me by another robot, especially one that is an earlier model.
Hortense said, soothingly, Of course, Rambo. Im sure that grandfather and grandmother understand that. (I noticed that DeLancey never said a word. I wonder if he ever said a word when his dear wife was present.)
I said, All right, I tell you what. Ill have Rodney tell me, and then I will tell Rambo.
Rambo said nothing to that. Even Rambo is subject to the second law of robotics which makes it mandatory for him to obey human orders.
Hortenses eyes narrowed and I knew that she would like to tell me that Rambo was far too fine a robot to be ordered about by the likes of me, but some distant and rudimentary near-human waft of feeling kept her from doing so.
Little LeRoy was hampered by no such quasi-human restraints. He said, I dont want to have to look at Rodneys ugly puss. I bet he dont know how to do anything and if he does, ol Grampa would get it all wrong anyway.
It would have been nice, I thought, if I could be alone with little LeRoy for five minutes and reason calmly with him, with a brick, but a mothers instinct told Hortense never to leave LeRoy alone with any human being whatever.
There was nothing to do, really, but get Rodney out of his niche in the closet where he had been enjoying his own thoughts (I wonder if a robot has his own thoughts when he is alone) and put him to work. It was hard. He would say a phrase, then I would say the same phrase, then Rambo would do something, then Rodney would say another phrase and so on.
It all took twice as long as if Rodney were doing it himself and it wore me out, I can tell you, because everything had to be like that, using the dishwasher/sterilizer, cooking the Christmas feast, cleaning up messes on the table or on the floor, everything.
Gracie kept moaning that Rodneys vacation was being ruined, but she never seemed to notice that mine was, too, though I did admire Hortense for her manner of saying something unpleasant at every moment that some statement seemed called for. I noticed, particularly, that she never repeated herself once. Anyone can be nasty, but to be unfailingly creative in ones nastiness filled me with a perverse desire to applaud now and then.
But, really, the worst thing of all came on Christmas Eve. The tree had been put up and I was exhausted. We didnt have the kind of situation in which an automated box of ornaments was plugged into an electronic tree, and at the touch of one button there would result an instantaneous and perfect distribution of ornaments. On our tree (of ordinary, old-fashioned plastic) the ornaments had to be placed, one by one, by hand.
Hortense looked revolted, but I said,  Actually, Hortense, this means you can be creative and make your own arrangement.
Hortense sniffed, rather like the scrape of claws on a rough plaster wall, and left the room with an obvious expression of nausea on her face. I bowed in the direction of her retreating back, glad to see her go, and then began the tedious task of listening to Rodneys instructions and passing them on to Rambo.


When it was over, I decided to rest my aching feet and mind by sitting in a chair in a far and rather dim corner of the room. I had hardly folded my aching body into the chair when little LeRoy entered. He didnt see me, I suppose, or, then again, he might simply have ignored me as being part of the less important and interesting pieces of furniture in the room.
He cast a disdainful look on the tree and said, to Rambo, Listen, where are the Christmas presents? Ill bet old Gramps and Gram got me lousy ones, but I aint going to wait for no tomorrow morning.
Rambo said, I do not know where they are, Little Master.
Huh! said LeRoy, turning to Rodney. How about you, Stink-face. Do you know where the presents are?
Rodney would have been within the bounds of his programming to have refused to answer on the grounds that he did not know he was being addressed, since his name was Rodney and not Stink-face. Im quite certain that that would have been Rambos attitude. Rodney, however, was of different stuff. He answered politely, Yes, I do, Little Master.
So where is it, you old puke?
Rodney said, I dont think it would be wise to tell you, Little Master. That would disappoint Gracie and Howard who would like to give the presents to you tomorrow morning.
Listen, said little LeRoy, who you think youre talking to, you dumb robot? Now I gave you an order. You bring those presents to me. And in an attempt to show Rodney who was master, he kicked the robot in the shin.
It was a mistake. I saw it would be that a second before and that was a joyous second. Little LeRoy, after all, was ready for bed (though I doubted that he ever went to bed before he was good and ready). Therefore, he was wearing slippers. Whats more, the slipper sailed off the foot with which he kicked, so that he ended by slamming his bare toes hard against the solid chrome-steel of the robotic shin.
He fell to the floor howling and in rushed his mother. What is it, LeRoy? What is it?
Whereupon little LeRoy had the immortal gall to say, He hit me. That old monster-robot hit me.
Hortense screamed. She saw me and shouted, That robot of yours must be destroyed.
I said, Come, Hortense. A robot cant hit a boy. First law of robotics prevents it.
Its an old robot, a broken robot. LeRoy says
LeRoy lies. There is no robot, no matter how old or how broken, who could hit a boy.
Then he did it. Grampa did it, howled LeRoy.
I wish I did, I said, quietly, but no robot would have allowed me to. Ask your own. Ask Rambo if he would have remained motionless while either Rodney or I had hit your boy. Rambo!
I put it in the imperative, and Rambo said, I would not have allowed any harm to come to the Little Master, Madam, but I did not know what he purposed. He kicked Rodneys shin with his bare foot, Madam.
Hortense gasped and her eyes bulged in fury. Then he had a good reason to do so. Ill still have your robot destroyed.
Go ahead, Hortense. Unless youre wining to ruin your robots efficiency by trying to reprogram him to lie, he win bear witness to just what preceded the kick and so, of course, with pleasure, win I.
Hortense left the next morning, carrying the pale-faced LeRoy with her (it turned out he had broken a toenothing he didnt deserve) and an endlessly wordless DeLancey.
Gracie wrung her hands and implored them to stay, but I watched them leave without emotion. No, thats a lie. I watched them leave with lots of emotion, an pleasant.
Later, I said to Rodney, when Gracie was not present, Im sorry, Rodney. That was a horrible Christmas, an because we tried to have it without you. Well never do that again, I promise.
Thank you, Sir, said Rodney. I must admit that there were times these two days when I earnestly wished the laws of robotics did not exist.
I grinned and nodded my head, but that night I woke up out of a sound sleep and began to worry. Ive been worrying ever since.
I admit that Rodney was greatly tried, but a robot cant wish the laws of robotics did not exist. He cant, no matter what the circumstances.
If I report this, Rodney will undoubtedly be scrapped, and if were issued a new robot as recompense, Gracie will simply never forgive me. Never! No robot, however new, however talented, can possibly replace Rodney in her affection.
In fact, Ill never forgive myself. Quite apart from my own liking for Rodney, I couldnt bear to give Hortense the satisfaction.
But if I do nothing, I live with a robot capable of wishing the laws of robotics did not exist. From wishing they did not exist to acting as if they did not exist is just a step. At what moment will he take that step and in what form will he show that he has done so?
What do I do? What do I do?


Robots I Have Known

Mechanical men, or, to use Capeks now universally-accepted term, robots, are a subject to which the modern science-fiction writer has turned again and again. There is no uninvented invention, with the possible exception of the spaceship, that is so clearly pictured in the minds of so many: a sinister form, large, metallic, vaguely human, moving like a machine and speaking with no emotion.
The key word in the description is sinister and therein lies a tragedy, for no science-fiction theme wore out its welcome as quickly as did the robot. Only one robot-plot seemed available to the average author: the mechanical man that proved a menace, the creature that turned against its creator, the robot that became a threat to humanity. And almost all stories of this sort were heavily surcharged, either explicitly or implicitly, with the weary moral that there are some things mankind must never seek to learn.
This sad situation has, since 1940, been largely ameliorated. Stories about robots abound; a newer viewpoint, more mechanistic and less moralistic, has developed. For this development, some people (notably Mr. Groff Conklin in the introduction to his science-fiction anthology entitled Science-Fiction Thinking Machines, published in 1954) have seen fit to attach at least partial credit to a series of robot stories I wrote beginning in 1940. Since there is probably no one on Earth less given to false modesty than myself, I accept said partial credit with equanimity and ease, modifying it only to include Mr. John w. Campbell, Jr., editor of  Astounding Science-Fiction, with whom I had many fruitful discussions on robot stories.
My own viewpoint was that robots were story material, not as blasphemous imitations of life, but merely as advanced machines. A machine does not turn against its creator if it is properly designed. When a machine, such as a power-saw, seems to do so by occasionally lopping off a limb, this regrettable tendency towards evil is combated by the installation of safety devices. Analogous safety devices would, it seemed obvious, be developed in the case of robots. And the most logical place for such safety devices would seem to be in the circuit-patterns of the robotic brain.
Let me pause to explain that in science-fiction, we do not quarrel intensively concerning the actual engineering of the robotic brain. Some mechanical device is assumed which in a volume that approximates that of the human brain must contain all the circuits necessary to allow the robot a range of perception-and-response reasonably equivalent to that of a human being. How that can be done without the use of mechanical units the size of a protein molecule or, at the very least, the size of a brain cell, is not explained. Some authors may talk about transistors and printed circuits. Most say nothing at all. My own pet trick is to refer, somewhat mystically, to positronic brains, leaving it to the ingenuity of the reader to decide what positrons have to do with it and to his good-will to continue reading after having failed to reach a decision.
In any case, as I wrote my series of robot stories, the safety devices gradually crystallized in my mind as The Three Laws of Robotics.  These three laws were first explicitly stated in Runaround.  As finally perfected, the Three Laws read as follows.

First LawA robot may not injure a human being, or, through inaction, allow a human being to come to harm.
Second LawA robot must obey the orders given it by human beings except where such orders would conflict with the First Law.
Third LawA robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

These laws are firmly built into the robotic brain, or at least the circuit equivalents are. Naturally, I dont describe the circuit equivalents. In fact, I never discuss the engineering of the robots for the very good reason that I am colossally ignorant of the practical aspects of robotics.
The First Law, as you can readily see, immediately eliminates that old, tired plot which I will not offend you by referring to any further.
Although, at first flush, it may appear that to set up such restrictive rules must hamper the creative imagination, it has turned out that the Laws of Robotics have served as a rich source of plot material. They have proved anything but a mental road-block.
An example would be the story Runaround to which I have already referred. The robot in that story, an expensive and experimental model, is designed for operation on the sunside of the planet Mercury. The Third Law has been built into him more strongly than usual for obvious economic reasons. He has been sent out by his human employers, as the story begins, to obtain some liquid selenium for some vital and necessary repairs. (Liquid selenium lies about in puddles in the heat of Mercurys sunward side, I will ask you to believe.)
Unfortunately, the robot was given his order casually so that the Second Law circuit set up was weaker than usual. Still more unfortunately, the selenium pool to which the robot was sent was near a site of volcanic activity, as a result of which there were sizable concentrations of carbon monoxide in the area. At the temperature of Mercurys sunside, I surmised that carbon monoxide would react fairly quickly with iron to form volatile iron carbonyls so that the robots more delicate joints might be badly damaged. The further the robot penetrates into this area, the greater the danger to his existence and the more intensive is the Third Law effect driving him away. The Second Law, however, ordinarily the superior, drives him onward. At a certain point, the unusually weak Second Law potential and the unusually strong Third Law potential reach a balance and the robot can neither advance nor retreat. He can only circle the selenium pool on the equipotential locus that makes a rough circle about the site.
Meanwhile, our heroes must have the selenium. They chase after the robot in special suits, discover the problem and wonder how to correct it. After several failures, the correct answer is hit upon. One of the men deliberately exposes himself to Mercurys sun in such a way that unless the robot rescues him, he will surely die. That brings the First Law into operation, which being superior to both Second and Third, pulls the robot out of his useless orbit and brings on the necessary happy ending.
It is in the story Runaround, by the way, that I believe I first made use of the term robotics (implicitly defined as the science of robot design, construction, maintenance, etc). Years later, I was told that I had invented the term and that it had never seen publication before. I do not know whether this is true. If it is true, I am happy, because I think it is a logical and useful word, and I hereby donate it to real workers in the field with all good will.
None of my other robot stories spring so immediately out of the Three Laws as does Runaround but all are born of the Laws in some way. There is the story, for instance, of the mind-reading robot who was forced to lie because he was unable to tell any human being anything other than that which the human in question wished to hear. The truth, you see, would almost invariably cause harm to the human being in the form of disappointment, disillusion, embarrassment, chagrin and other similar emotions, all of which were but too plainly visible to the robot.
Then there was the puzzle of the man who was suspected of being a robot, that is, of having a quasi-protoplasmic body and a robots positronic brain. One way of proving his humanity would be for him to break the First Law in public, so he obliges by deliberately striking a man. But the story ends in doubt because there is still the suspicion that the other man might also be a robot and there is nothing in the Three Laws that would prevent a robot from hitting another robot.
And then we have the ultimate robots, models so advanced that they are used to precalculate such things as weather, crop harvests, industrial production figures, political developments and so on. This is done in order that world economy may be less subject to the whims of those factors which are now beyond mans control. But these ultimate robots, it seems, are still subject to the First Law. They cannot through inaction allow human beings to come to harm, so they deliberately give answers which are not necessarily truthful and which cause localized economic upsets so designed as to maneuver mankind along the road that leads to peace and prosperity. So the robots finally win the mastery after all, but only for the good of man.
The interrelationship of man and robot is not to be neglected. Mankind may know of the existence of the Three Laws on an intellectual level and yet have an ineradicable fear and distrust for robots on an emotional level. If you wanted to invent a term, you might call it a Frankenstein complex. There is also the more practical matter of the opposition of labor unions, for instance, to the possible replacement of human labor by robot labor.
This, too, can give rise to stories. My first robot story concerned a robot nursemaid and a child. The child adored its robot as might be expected, but the mother feared it, as might also be expected. The nub of the story lay in the mothers attempt to get rid of it and in the childs reaction to that.
My first full-length robot novel, The Caves of Steel (1954), peers further into the future, and is laid in a time when other planets, populated by emigrating Earthmen, have adopted a thoroughly robotized economy, but where Earth itself, for economic and emotional reasons, still objects to the introduction of the metal creatures. A murder is committed, with robot-hatred as the motive. It is solved by a pair of detectives, one a man, one a robot, with a great portion of the deductive reasoning (to which detective stories are prone) revolving about the Three Laws and their implications.
I have managed to convince myself that the Three Laws are both necessary and sufficient for human safety in regard to robots. It is my sincere belief that some day when advanced human-like robots are indeed built, something very like the Three Laws will be built into them. I would enjoy being a prophet in this respect, and I regret only the fact that the matter probably cannot be arranged in my lifetime. *

*This essay was written in 1956. In the years since, robotics has indeed entered the English language and is universally used, and I have lived to see roboticists taking the Three Laws very seriously.

The New Teachers

The percentage of older people in the world is increasing and that of younger people decreasing, and this trend will continue if the birthrate should drop and medicine continue to extend the average life span.
In order to keep older people imaginative and creative and to prevent them from becoming an ever-growing drag on a shrinking pool of creative young, I have recommended frequently that our educational system be remodeled and that education be considered a lifelong activity.
But how can this be done? Where will an the teachers come from?
Who says, however, that an teachers must be human beings or even animate?
Suppose that over the next century communications satellites become numerous and more sophisticated than those weve placed in space so far. Suppose that in place of radio waves the more capacious laser beam of visible light becomes the chief communications medium.
Under these circumstances, there would be room for many minions of separate channels for voice and picture, and it is easy to imagine every human being on Earth having a particular television wavelength assigned to her or him.
Each person (child, adult, or elderly) can have his own private outlet to which could be attached, at certain desirable periods of time, his or her personal teaching machine. It would be a far more versatile and interactive teaching machine than anything we could put together now, for computer technology will also have advanced in the interval.
We can reasonably hope that the teaching machine will be sufficiently intricate and flexible to be capable of modifying its own program (that is, learning) as a result of the students input.
In other words, the student will ask questions, answer questions, make statements, offer opinions, and from all of this, the machine will be able to gauge the student well enough to adjust the speed and intensity of its course of instruction and, whats more, shift it in the direction of the student interest displayed.
We cant imagine a personal teaching machine to be very big, however. It might resemble a television set in size and appearance. Can so small an object contain enough information to teach the students as much as they want to know, in any direction intellectual curiosity may lead them? No, not if the teaching machine is self-containedbut need it be?
In any civilization with computer science so advanced as to make teaching machines possible, there will surely be thoroughly computerized central libraries. Such libraries may even be interconnected into a single planetary library.
All teaching machines would be plugged into this planetary library and each could then have at its disposal any book, periodical, document, recording, or video cassette encoded there. If the machine has it, the student would have it too, either placed directly on a viewing screen, or reproduced in print-on-paper for more leisurely study.
Of course, human teachers will not be totally eliminated. In some subjects, human interaction is essentialathletics, drama, public speaking, and so on. There is also value, and interest, in groups of students working in a particular fieldgetting together to discuss and speculate with each other and with human experts, sparking each other to new insights.
After this human interchange they may return, with some relief, to the endlessly knowledgeable, endlessly flexible, and, most of all, endlessly patient machines.
But who will teach the teaching machines?
Surely the students who learn will also teach. Students who learn freely in those fields and activities that interest them are bound to think, speculate, observe, experiment, and, now and then, come up with something of their own that may not have been previously known.
They would transmit that knowledge back to the machines, which will in turn record it (with due credit, presumably) in the planetary librarythus making it available to other teaching machines. All will be put back into the central hopper to serve as a new and higher starting point for those who come after: The teaching machines will thus make it possible for the human species to race forward to heights and in directions now impossible to foresee.
But I am describing only the mechanics of learning. What of the content? What subjects will people study in the age of the teaching machine? Ill speculate on that in the next essay.

Whatever You Wish

The difficulty in deciding on what the professions of the future would be is that it all depends on the kind of future we choose to have. If we allow our civilization to be destroyed, the only profession of the future will be scrounging for survival, and few will succeed at it.
Suppose, though, that we keep our civilization alive and flourishing and, therefore, that technology continues to advance. It seems logical that the professions of such a future would include computer programming, lunar mining, fusion engineering, space construction, laser communications, neurophysiology, and so on.
I cant help but think, however, that the advance of computerization and automation is going to wipe out the subwork of humanitythe dull pushing and shoving and punching and clicking and filing and all the other simple and repetitive motions, both physical and mental, that can be done perfectly easilyand betterby machines no more complicated than those we can already build.
In short, the world could be so well run that only a relative handful of human foremen would be needed to engage in the various professions and supervisory work necessary to keep the worlds population fed, housed, and cared for.
What about the majority of the human species in this automated future? What about those who dont have the ability or the desire to work at the professions of the future or for whom there is no room in those professions? It may be that most people will have nothing to do of what we think of as work nowadays.
This could be a frightening thought. What will people do without work? Wont they sit around and be bored; or worse, become unstable or even vicious? The saying is that Satan finds mischief still for idle hands to do.
But we judge from the situation that has existed till now, a situation in which people are left to themselves to rot.
Consider that there have been times in history when an aristocracy lived in idleness off the backs of flesh-and-blood machines called slaves or serfs or peasants. When such a situation was combined with a high culture, however, aristocrats used their leisure to become educated in literature, the arts, and philosophy. Such studies were not useful for work, but they occupied the mind, made for interesting conversation and an enjoyable life.
These were the liberal arts, arts for free men who didnt have to work with their hands. And these were considered higher and more satisfying than the mechanical arts, which were rarely materially useful.
Perhaps, then, the future will see a world aristocracy supported by the only slaves that can humanely serve in such a post-sophisticated machines. And there will be an infinitely newer and broader liberal arts program, taught by the teaching machines, from which each person could choose.
Some might choose computer technology or fusion engineering or lunar mining or any of the professions that would seem vital to the proper functioning of the world. Why not? Such professions, placing demands on human imagination and skill, would be very attractive to many, and there will surely be enough who will be voluntarily drawn to these occupations to fill them adequately.
But to most people the field of choice might be far less cosmic. It might be stamp collecting, pottery, ornamental painting, cooking, dramatics, or whatever. Every field will be an elective, and the only guide will be whatever you wish.
Each person, guided by teaching machines sophisticated enough to offer a wide sampling of human activities, can then choose what he or she can best and most willingly do.
Is the individual person wise enough to know what he or she can best do? Why not? Who else can know? And what can a person do best except that which he or she wants to do most?
Wont people choose to do nothing? Sleep their lives away?
If thats what they want, why not?Except that I have a feeling they wont. Doing nothing is hard work, and, it seems to me, would be indulged in only by those who have never had the opportunity to evolve out of themselves something more interesting and, therefore, easier to do.
In a properly automated and educated world, then, machines may prove to be the true humanizing influence. It may be that machines will do the work that makes life possible and that human beings will do all the other things that make life pleasant and worthwhile.

The Friends We Make

The term robot dates back only sixty years. It was invented by the Czech playwright, Karel Capek, in his play, R. U. R., and is a Czech word meaning worker.
The idea, however, is far older. It is as old as mans longing for a servant as smart as a human being, but far stronger, and incapable of growing weary, bored, or dissatisfied. In the Greek myths, the god of the forge, Hephaistos, had two golden girlsas bright and alive as flesh-and-blood girlsto help him. And the island of Crete was guarded, in the myths, by a bronze giant named Talos, who circled its shores perpetually and tirelessly, watching for intruders.
Are robots possible, though? And if they are, are they desirable?
Mechanical devices with gears and springs and ratchets could certainly make manlike devices perform manlike actions, but the essence of a successful robot is to have it thinkand think well enough to perform useful functions without being continually supervised.
But thinking takes a brain. The human being is made up of microscopic neurons, each of which has an extraordinarily complex substructure. There are 10 billion neurons in the brain and 90 billion supporting cells, all hooked together in a very intricate pattern. How can anything like that be duplicated by some man-made device in a robot?
It wasnt until the invention of the electronic computer thirty-five years ago that such a thing became conceivable. Since its birth, the electronic computer has grown ever more compact, and each year it becomes possible to pack more and more information into less and less volume.
In a few decades, might not enough versatility to direct a robot be packed into a volume the size of the human brain? Such a computer would not have to be as advanced as the human brain, but only advanced enough to guide the actions of a robot designed, let us say, to vacuum rugs, to run a hydraulic press, to survey the lunar surface.
A robot would, of course, have to include a self-contained energy source; we couldnt expect it to be forever plugged into a wall socket. This, however, can be handled. A battery that needs periodic charging is not so different from a living body that needs periodic feeding.
But why bother with a humanoid shape? Would it not be more sensible to devise a specialized machine to perform a particular task without asking it to take on all the inefficiencies involved in arms, legs, and torso? Suppose you design a robot that can hold a finger in a furnace to test its temperature and turn the heating unit on and off to maintain that temperature nearly constant. Surely a simple thermostat made of a bimetallic strip will do the job as well.
Consider, though, that over the thousands of years of mans civilization, we have built a technology geared to the human shape. Products for humans use are designed in size and form to accommodate the human bodyhow it bends and how long, wide, and heavy the various bending parts are. Machines are designed to fit the human reach and the width and position of human fingers.
We have only to consider the problems of human beings who happen to be a little taller or shorter than the normor even just left-handedto see how important it is to have a good fit into our technology.
If we want a directing device then, one that can make use of human tools and machines, and that can fit into the technology, we would find it useful to make that device in the human shape, with all the bends and turns of which the human body is capable. Nor would we want it to be too heavy or too abnormally proportioned. Average in all respects would be best.
Then too, we relate to all nonhuman things by finding, or inventing, something human about them. We attribute human characteristics to our pets, and even to our automobiles. We personify nature and all the products of nature and, in earlier times, made human-shaped gods and goddesses out of them.
Surely, if we are to take on thinking partnersor, at the least, thinking servantsin the form of machines, we will be more comfortable with them, and we will relate to them more easily, if they are shaped like humans.
It will be easier to be friends with human-shaped robots than with specialized machines of unrecognizable shape. And I sometimes think that, in the desperate straits of humanity today, we would be grateful to have nonhuman friends, even if they are only friends we build ourselves.

Our Intelligent Tools

Robots dont have to be very intelligent to be intelligent enough. If a robot can follow simple orders and do the housework, or run simple machines in a cut-and-dried, repetitive way, we would be perfectly satisfied.
Constructing a robot is hard because you must fit a very compact computer inside its skull, if it is to have a vaguely human shape. Making a sufficiently complex computer as compact as the human brain is also hard.
But robots aside, why bother making a computer that compact? The units that make up a computer have been getting smaller and smaller, to be surefrom vacuum tubes to transistors to tiny integrated circuits and silicon chips. Suppose that, in addition to making the units smaller, we also make the whole structure bigger.
A brain that gets too large would eventually begin to lose efficiency because nerve impulses dont travel very quickly. Even the speediest nerve impulses travel at only about 3.75 miles a minute. A nerve impulse can flash from one end of the brain to the other in one four-hundred-fortieth of a second, but a brain 9 miles long, if we could imagine one, would require 2.4 minutes for a nerve impulse to travel its length. The added complexity made possible by the enormous size would fall apart simply because of the long wait for information to be moved and processed within it.
Computers, however, use electric impulses that travel at more than 11 million miles per minute. A computer 400 miles wide would still flash electric impulses from end to end in about one four-hundred-fortieth of a second. In that respect, at least, a computer of that asteroidal size could still process information as quickly as the human brain could.
If, therefore, we imagine computers being manufactured with finer and finer components, more and more intricately interrelated, and also imagine those same computers becoming larger and larger, might it not be that the computers would eventually become capable of doing all the things a human brain can do?
Is there a theoretical limit to how intelligent a computer can become?
Ive never heard of any. It seems to me that each time we learn to pack more complexity into a given volume, the computer can do more. Each time we make a computer larger, while keeping each portion as densely complex as before, the computer can do more.
Eventually, if we learn how to make a computer sufficiently complex and sufficiently large, why should it not achieve a human intelligence?
Some people are sure to be disbelieving and say, But how can a computer possibly produce a great symphony, a great work of art, a great new scientific theory?
The retort I am usually tempted to make to this question is, Can you? But, of course, even if the questioner is ordinary, there are extraordinary people who are geniuses. They attain genius, however, only because atoms and molecules within their brains are arranged in some complex order. Theres nothing in their brains but atoms and molecules. If we arrange atoms and molecules in some complex order in a computer, the products of genius should be possible to it; and if the individual parts are not as tiny and delicate as those of the brain, we compensate by making the computer larger.
Some people may say, But computers can only do what theyre programmed to do.
The answer to that is, True. But brains can do only what theyre programmed to doby their genes. Part of the brains programming is the ability to learn, and that will be part of a complex computers programming.
In fact, if a computer can be built to be as intelligent as a human being, why cant it be made more intelligent as well?
Why not, indeed? Maybe thats what evolution is all about. Over the space of three billion years, hit-and-miss development of atoms and molecules has finally produced, through glacially slow improvement, a species intelligent enough to take the next step in a matter of centuries, or even decades. Then things will really move.
But if computers become more intelligent than human beings, might they not replace us? Well, shouldnt they? They may be as kind as they are intelligent and just let us dwindle by attrition. They might keep some of us as pets, or on reservations.
Then too, consider what were doing to ourselves right nowto all living things and to the very planet we live on. Maybe it is time we were replaced. Maybe the real danger is that computers wont be developed to the point of replacing us fast enough.
Think about it!*

*I present this view only as something to think about. I consider a quite different view in Intelligences Together later in this collection.

The Laws Of Robotics

It isnt easy to think about computers without wondering if they will ever take over.
Will they replace us, make us obsolete, and get rid of us the way we got rid of spears and tinderboxes?
If we imagine computerlike brains inside the metal imitations of human beings that we call robots, the fear is even more direct. Robots look so much like human beings that their very appearance may give them rebellious ideas.
This problem faced the world of science fiction in the 19208 and 19308, and many were the cautionary tales written of robots that were built and then turned on their creators and destroyed them.
When I was a young man I grew tired of that caution, for it seemed to me that a robot was a machine and that human beings were constantly building machines. Since all machines are dangerous, one way or another, human beings built safeguards into them.
In 1939, therefore, I began to write a series of stories in which robots were presented sympathetically, as machines that were carefully designed to perform given tasks, with ample safeguards built into them to make them benign.
In a story I wrote in October 1941, I finally presented the safeguards in the specific form of The Three Laws of Robotics.  (I invented the word robotics, which had never been used before.)
Here they are:
1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.
2. A robot must obey the orders given it by human beings except where those orders would conflict with the First Law.
3. A robot must protect its own existence except where such protection would conflict with the First and Second Law.
These laws were programmed into the computerized brain of the robot, and the numerous stories I wrote about robots took them into account. Indeed, these laws proved so popular with the readers and made so much sense that other science fiction writers began to use them (without ever quoting them directlyonly I may do that), and all the old stories of robots destroying their creators died out.
Ah, but thats science fiction. What about the work really being done now on computers and on artificial intelligence? When machines are built that begin to have an intelligence of their own, will something like the Three Laws of Robotics be built into them?
Of course they will, assuming the computer designers have the least bit of intelligence. Whats more, the safeguards will not merely be like the Three Laws of Robotics; they will be the Three Laws of Robotics.
I did not realize, at the time I constructed those laws, that humanity has been using them since the dawn of time. Just think of them as The Three Laws of Tools, and this is the way they would read:
1. A tool must be safe to use.
(Obviously! Knives have handles and swords have hilts. Any tool that is sure to harm the user, provided the user is aware, will never be used routinely whatever its other qualifications.)
2. A tool must perform its function, provided it does so safely.
3. A tool must remain intact during use unless its destruction is required for safety or unless its destruction is part of its function.
No one ever cites these Three Laws of Tools because they are taken for granted by everyone. Each law, were it quoted, would be sure to be greeted by a chorus of Well, of course!
Compare the Three Laws of Tools, then, with the Three Laws of Robotics, law by law, and you will see that they correspond exactly. And why not, since the robot or, if you will, the computer, is a human tool?
But are safeguards sufficient? Consider the effort that is put into making the automobile safeyet automobiles still kill 50,000 Americans a year. Consider the effort that is put into making banks secureyet there are still bank robberies in a steady drumroll. Consider the effort that is put into making computer programs secureyet there is the growing danger of computer fraud.
Computers, however, if they get intelligent enough to take over, may also be intelligent enough no longer to require the Three Laws. They may, of their own benevolence, take care of us and guard us from harm.
Some of you may argue, though, that were not children and that it would destroy the very essence of our humanity to be guarded.
Really? Look at the world today and the world in the past and ask yourself if were not childrenand destructive children at thatand if we dont need to be guarded in our own interest.
If we demand to be treated as adults, shouldnt we act like adults? And when do we intend to start?

Future Fantastic

In the past, three fundamental advances in human communication evolved that altered every facet of our world enormously and permanently. The first advance was speech, the second writing, and the third printing.
Now we face a fourth advance in communication every bit as important as the first threethe computer. This fourth revolution will enable most human beings to be more creative than theyve ever been before. And provided we do not destroy the world by nuclear warfare, overpopulation or pollution, we will have a world of the technochilda world as different from our present one as todays is from the world of the caveman. How will the lives of the next generation be different from their parents and grandparents?
One immediate response is to view the computer merely as another form of amusement, rather like a super-TV. It can be used for complex games, for making contact with friends, or for various trivial pursuits. Still, such things can change the world. For one thing, communication by computer networks can wipe out the feeling of distance. It can make the globe seem like a neighborhood, and this can have important consequencesthe development of the concept of humanity as a single society, not as a collection of endlessly and inevitably warring social segments. The world might develop a global lingua franca, a language (no doubt something quite close to todays English) that everyone can understand, even though people would retain their individual languages for local use.
Then, too, since communication will be so easy and since mechanical and electronic devices can be controlled remotely (telemetering, for example, makes it possible even now for engineers to send instructions toand obtain obedience fromdevices sailing past planets billions of miles away), computers will reduce the necessity of using physical transportation to gain or gather information.
There will, of course, be no bar to travel. You can still be a tourist or visit friends or family in person rather than by closed-circuit television. But you will not have to battle hordes of people merely to carry or receive information that can be transferred by computer.
This means that the technochildren of tomorrow will be accustomed to living in a decentralized world, to reaching out in a variety of ways from their homesor wherever they areto do what needs doing. At one and the same time, they will feel both entirely isolated and in total contact.

The children of the next generationand the society they will createwill see the greatest impact from computers in the area of education. Currently our society is intent on educating as many children as possible. The limit in the number of teachers means that students learn in mass. Every student in a school district or state or nation is taught the same thing at the same time in more or less the same way. But because each child has individual interests and methods of learning, the experience of mass education turns out to be unpleasant. The result is that most adults resist the learning process in postschool life; theyve had enough of it.
Learning could be pleasant, even all-absorbingly fascinating, if children studied something that specifically interested them individually, on their own time and in their own way. Such study is currently possible through public libraries. But the library is a clumsy tool. One must go there, borrowing is limited to a few volumes, and books must be returned after a short time.
Clearly the solution is to move libraries into the home. Just as record players brought home the concert hall and television brought home the movie theater, the computer can bring home the public library. Tomorrows technochildren will have a ready means of sating their curiosity. They will know at an early age how to command their computers to give listings of materials. As their interests are aroused (and guided, it is to be hoped, by their teachers at school), they will learn more in less time and find new byways to follow.
Education will have a strong component of self-motivation added to it. The ability to follow a personal path will encourage the technochild to associate learning with pleasure and grow into a lively technoadulteager, curious, and ready to expand the mental environment for as long as his or her brain remains physically undulled by the ravages of old age.
This new approach to education can also influence another area of life: work. Until now, most human beings have worked at jobs that seriously underutilized the brain. In the ages when work consisted largely of brutish physical labor, few ever had the chance to lift their eyes to the stars or ponder abstractions. Even when the Industrial Revolution brought machinery that could lift the load of physical labor from the backs of humanity, meaningless skilled work took its place. Today employees on the assembly line and in offices still perform jobs that require little thought.
For the first time in history, skilled machines, or robots, will be able to do those mindless jobs. Any job that is so simple and repetitive that a robot can do it as well as, if not better than, a person is beneath the dignity of the human brain. As technochildren turn into adults and move into the work world, they will have time to exercise more creativity, to work in the fields of drama, science, literature, government, and entertainment. And they will be ready for this kind of work as a result of the computerized revolution in education.
Some might believe that its simply impossible to expect people to be creative in large numbers. But that thinking comes from a world in which only a few escape the mental destruction of jobs that dont use the brain. Weve been through this before: It was always assumed that literacy, for example, was the province of the few who had minds peculiarly adapted to the complicated task of reading and writing. Of course, with the advent of printing and mass education, it turned out that most human beings could be literate.

What does all this mean? That we will be dealing with a world of leisure. Once computers and robots are doing the dull, mechanical work, the world will start running itself to a far greater extent than ever before. Will there be more Renaissance people as a result? Yes. Currently leisure is a small segment of life that is used narrowly because of lack of time, or is wasted on doing nothing in a desperate attempt to get far away from the hated workaday world. With leisure filling most of ones time, there will be no sensation of racing the clock, no compulsion to enter into a wild spree against the slavery of hateful work. People will sample a variety of interests without haste, become skillful or knowledgeable in a number of areas, and cultivate different talents at various times.
This is not just guesswork. There have been eras in history when people had slavesthe brutalized, human version of the computerto do the work for them. Others have had patrons to support them. When even a few people have had ample leisure time to pursue their interests, the result has been an explosion of variegated culture. The Golden Age of Athens in the late fifth century B.C. and the Italian Renaissance in the 14th to 16th centuries are the most famous examples.
Not only will people have the freedom to pursue hobbies and interests and dreams, but a great number of them will also want to share their talents. So many of us have a bit of the ham in us. We sing in the shower, take part in amateur theatrical productions, or love to swing along in parades. It is my guess that the 21st century may see a society in which one-third of the population will be engaged in entertaining the other two-thirds.
And there are bound to be new forms of entertainment that one can now foresee only dimly. Three-dimensional TV is easy to forecast. And space may become a new arena for activity. In near-zero gravity, for example, the manipulation of balls may produce far more complicated forms of tennis or soccer. Ballet and even social dancing may become incredibly startling and require a new kind of coordination thats delightful to watch, as it will be as easy to move up and down as it is to move forward and backward or left and right.

What about those people who choose not to share their bents and interests and instead retire into worlds of their own? Someone who is interested, for example, in learning about the history of costumes and who is capable of exploring the libraries of the world from an isolated comer might simply stay there. Might we, then, find ourselves in a society in which an unprecedented number of people are intellectual hermits? Might we breed a race of introverts?
I think the chances are slim. People who grow ferociously interested in one aspect of knowledge or expertise are quite likely to be filled with missionary zeal. They will want to share their knowledge with others. Even today, someone who has an obscure field of interest is far more likely to want to explain it to everyone he or she meets than to sit silently in a comer. If theres any danger, its that an arcane interest will nurture a loquacious bore rather than a hermit.
We must not forget the tendency of those who share interests to wish to get together, to form a temporary subuniverse that is a haven of concentrated special fascination. In the 19708, for example, someone had the idea of organizing a convention for Star Trek fans, expecting a few hundred at most to attend. Instead, fans poured in by the thousands (and television was supposed to be an isolating medium!). On-line gatherings, in which the computer is the medium and people are actively involved, will experience similarly high levels of participation.
And in between the formal get-togethers, there will be a kaleidoscope of people linked into global communities by computerized communication. Perpetual conventions will take place, in which individuals continually drop in and out, bringing in findings or ideas and leaving stimulated. There will be a constant melange of teaching and learning.
What I foresee is a society in intense creative ferment, people reaching out to others, new thoughts arising and spreading at a speed never before imagined, change and variety filling the planet (to say nothing of the smaller, artificial worlds that will be constructed in space). It will be a new world that will look back at earlier centuries as having been only half alive.

The Machine And The Robot

To a physicist, a machine is any device that transfers a force from the point where it is applied to another point where it is used and, in the process, changes its intensity or direction.
In this sense it is difficult for a human being to make use of anything that is not part of his body without, in the process, using a machine. A couple of million years ago, when one could scarcely decide whether the most advanced hominids were more humanlike than apelike, pebbles were already being chipped and their sharp edges used to cut or scrape.
And even a chipped pebble is a machine, for the force applied to the blunt edge by the hand is transmitted to the sharp end and, in the process, intensified. The force spread over the large area of the blunt end is equal to the force spread over the small area of the sharp end. The pressure (force per area) is therefore increased, and without ever increasing the total force, that force is intensified in action. The sharp-edge pebble could, by the greater pressure it exerts, force its way through an object, as a rounded pebble (or a mans hand) could not.
In actual practice, however, few people, other than physicists at their most rigid, would call a chipped pebble a machine. In actual practice, we think of machines as relatively complicated devices, and are more likely to use the name if the device is somewhat removed from direct human guidance and manipulation.
The further a device is removed from human control, the more authentically mechanical it seems, and the whole trend in technology has been to devise machines that are less and less under direct human control and more and more seem to have the beginning of a will of their own. A chipped pebble is almost part of the hand it never leaves. A thrown spear declares a sort of independence the moment is its released.
The clear progression away from direct and immediate control made it possible for human beings, even in primitive times, to slide forward into extrapolation, and to picture devices still less controllable, still more independent than anything of which they had direct experience. Immediately we have a form of fantasywhich some, defining the term more broadly than I would, might even call science fiction.
Man can move on his feet by direct and intimate control; or on horseback, controlling the more powerful animal muscles by rein and heel; or on ship, making use of the invisible power of the wind. Why not progress into further etherealization by way of seven-league boots, flying carpets, self-propelled boats. The power used in these cases was magic, the tapping of the superhuman and transcendental energies of gods or demons.
Nor did these imaginings concern only the increased physical power of inanimate objects, but even increased mental power of objects which were still viewed as essentially inanimate. Artificial intelligence is not really a modern concept.
Hephaistos, the Greek god of the forge, is pictured in the Iliad as having golden mechanical women, which were as mobile and as intelligent as flesh-and-blood women, and which helped him in his palace.
Why not? After all, if a human smith makes inanimate metal objects of the base metal iron, why should not a god-smith make far more clever inanimate metal objects of the noble metal gold? It is an easy extrapolation, of the sort that comes as second nature to science fiction writers (who, in primitive times, had to be myth-makers, in default of science).
But human artisans, if clever enough, could also make mechanical human beings. Consider Talos, a bronze warrior made by that Thomas Edison of the Greek myths, Oaedalus. Talos guarded the shores of Crete, circling the island once each day and keeping off all intruders. The fluid that kept him alive was kept within his body by a plug at his heel. When the Argonauts landed on Crete, Medea used her magic to pull out the plug and Talos lost all his pseudoanimation.
(It is easy to ascribe a symbolic meaning to this myth. Crete, starting in the fourth millennium B.C., before the Greeks had yet entered Greece, had a navy, the first working navy in human history. The Cretan navy made it possible for the islanders to establish an empire over what became the nearby islands and mainland. The Greek barbarians, invading the land, were more or less under Cretan dominion to begin with. The bronze-armored warriors carried by the ships guarded the Cretan mainland for two thousand yearsand then failed. The plug was pulled, so to speak, when the island of Thera exploded in a vast volcanic eruption in 1500 B.C. and a tsunami greatly weakened the Cretan civilizationand the Greeks took over. Still, the fact that a myth is a sort of vague and distorted recall of something actual does not alter its function of indicating a way of human thinking.)
From the start, then, the machine has faced mankind with a double aspect. As long as it is completely under human control, it is useful and good and makes a better life for people. However, it is the experience of mankind (and was already his experience in quite early times) that technology is a cumulative thing, that machines are invariably improved, and that the improvement is always in the direction of etherealization, always in the direction of less human control and more auto-controland at an accelerating rate.
As the human control decreases, the machine becomes frightening in exact proportion. Even when the human control is not visibly decreasing, or is doing so at an excessively low rate, it is a simple task for human ingenuity to look forward to a time when the machine may go out of control altogether, and the fear of that can be felt in advance.

What is the fear?
The simplest and most obvious fear is that of the possible harm that comes from machinery out of control. In fact, any technological advance, however fundamental, has the double aspect of good/harm and, in response, is viewed with a double aspect of love/fear.
Fire warms you, gives you light, cooks your food, smelts your oreand, out of control, burns and kills. Your knives and spears kill your animal enemies and your human foes and, out of your control, are used by your foes to kill you. You can run down the list and build examples indefinitely and there has never been any human activity which, on getting out of control and doing harm, has raised the sigh among many of, Oh, if we had only stuck to the simple and virtuous lives of our ancestors who were not cursed with this new-fangled misery.
Yet is this fear of piecemeal harm from this advance or that the kind of deep-seated terror so difficult to express that it finds its way into the myths?
I think not. Fear of machinery for the discomfort and occasional harm it brings has (at least until very recently) not moved humanity to more than that occasional sigh. The love of the uses of machinery has always far overbalanced such fears, as we might judge if we consider that very rarely in the history of mankind has any culture voluntarily given up significant technological advance because of the inconvenience or harm of its side effects. There have been involuntary retreats from technology as a result of warfare, civil strife, epidemics, or natural disasters, but the results of that are precisely what we call a dark age and the population suffering from one does its best over the generations to get back on the track and restore the technology.
Mankind has always chosen to counter the evils of technology, not by abandonment of technology, but by additional technology. The smoke of an indoor fire was countered by the chimney. The danger of the spear was countered by the shield. The danger of the mass army was countered by the city wall.
This attitude, despite the steady drizzle of backwardist outcries, has continued to the present. Thus the characteristic technological product of our present life is the automobile. It pollutes the air, assaults our eardrums, kills fifty thousand Americans a year and inflicts survivable injuries on hundreds of thousands.
Does anyone seriously expect Americans to give up their murderous little pets voluntarily? Even those who attend rallies to denounce the mechanization of modern life are quite likely to reach those rallies by automobile.
The first moment when the magnitude of possible evil was seen by many people as uncounterable by any conceivable good came with the fission bomb in 1945. Never before had any technological advance set off demands for abandonment by so large a percentage of the population.
In fact, the reaction to the fission bomb set a new fashion. People were readier to oppose other advances they saw as unacceptably harmful in their side effects-biological warfare, the SST, certain genetic experiments on micro-organisms, breeder reactors, spray cans.
And even so, not one of these items has yet been given up.

But were on the right track. The fear of the machine is not at the deepest level of the soul if the harm it does is accompanied by good, too; or if the harm is merely to some peoplethe few who happen to be on the spot in a vehicular collision, for instance.
The majority, after all, escape, and reap the good of the machine.
No, it is when the machine threatens all mankind in any way so that each individual human being begins to feel that he, himself, will not escape, that fear overwhelms love.
But since technology has begun to threaten the human race as a whole only in the last thirty years, were we immune to fear before thator has the human race always been threatened?
After all, is physical destruction by brute energy of a type only now in our fist, the only way in which human beings can be destroyed? Might not the machine destroy the essence of humanity, our minds and souls, even while leaving our bodies intact and secure and comfortable?
It is a common fear, for instance, that television makes people unable to read and pocket computers will make them unable to add. Or think of the Spartan king who, on observing a catapult in action, mourned that that would put an end to human valor.
Certainly such subtle threats to humanity have existed and been recognized through all the long ages when mans feeble control over nature made it impossible for him to do himself very much physical harm.
The fear that machinery might make men effete is not yet, in my opinion, the basic and greatest fear. The one (it seems to me) that hits closest to the core is the general fear of irreversible change. Consider:
There are two kinds of change that we can gather from the universe about us. One is cyclic and benign.
Day both follows and is followed by night. Summer both follows and is followed by winter. Rain both follows and is followed by clear weather, and the net result is, therefore, no change. That may be boring, but it is comfortable and induces a feeling of security.
In fact, so comfortable is the notion of short-term cyclic change implying long-term changelessness, that human beings labor to find it everywhere. In human affairs, there is the notion that one generation both follows and is followed by another, that one dynasty both follows and is followed by another, that one empire both follows and is followed by another. It is not a good analogy to the cycles of nature since the repetitions are not exact, but it is good enough to be comforting.
So strongly do human beings want the comfort of cycles that they will seize upon one even when the evidence is insufficientor even when it actually points the other way.
With respect to the universe, what evidence we have points to a hyperbolic evolution; a universe that expands forever out of the initial big bang and ends as formless gas and black holes. Yet our emotions drag us, against the evidence, to notions of oscillating, cyclic, repeating universes, in which even the black holes are merely gateways to new big bangs.
But then there is the other change, to be avoided at all coststhe irreversible, malignant change; the one-way change; the permanent change; the change-never-to-return.
What is so fearful about it? The fact is that there is one such change that lies so close to ourselves that it distorts the entire universe for us.
We are, after all, old, and though we were once young we shall never be young again. Irreversible! Our friends are dead, and though they were once alive, they shall never be alive again. Irreversible! The fact is that life ends in death and that is not a cyclic change and we fear that end and know it is useless to fight it.
What is worse is that the universe doesnt die with us. Callously and immortally it continues onward in its cyclic changes, adding to the injury of death the insult of indifference.
And what is still worse is that other human beings dont die with us. There are younger human beings, born later, who were helpless and dependent on us to start with, but who grow into supplanting nemeses and take our places as we age and die. To the injury of death is added the insult of supplantation.
Did I say it is useless to fight this honor of death accompanied by indifference and supplantation? Not quite. The uselessness is apparent only if we cling to the rational, but there is no law that says we must cling to it, and human beings do not, in fact, do so.
Death can be avoided by simply denying it exists. We can suppose that life on Earth is an illusion, a short testing period prior to entry into some afterlife where all is eternal and there is no question of irreversible change. Or we can suppose that it is only the body that is subject to death and that there is an immortal component of ourselves, not subject to irreversible change, which might, after the death of one body, enter another, in indefinite, cyclic repetitions of life.
These mythic inventions of afterlife and transmigration may make life tolerable for many human beings and enable them to face death with reasonable equanimitybut the fear of death and supplantation is only masked and overlaid; it is not removed.
In fact, the Greek myths involve the successive supplantation of one set of immortals by anotherin what seems to be a despairing admission that not even eternal life and superhuman power can remove the danger of irreversible change and the humiliation of being supplanted.
To the Greeks it was disorder (Chaos) that first ruled the universe, and it was supplanted by Ouranos (the sky), whose intricate powdering of stars and complexly moving planets symbolized order (Kosmos).
But Ouranos was castrated by Kronos, his son. Kronos, his brothers, his sisters, and their progeny then ruled the universe.
Kronos feared that he would be served by his children as he had served his father (a kind of cycle of irreversible changes) and devoured his children as they were born. He was duped by his wife, however, who managed to save her last-born, Zeus, and to spirit him away to safety. Zeus grew to adult godhood, rescued his siblings from his fathers stomach, warred against Kronos and those who followed him, defeated him, and replaced him as ruler.
(There are supplantation myths among other cultures, too, even in our ownas the one in which Satan tried to supplant God and failed; a myth that reached its greatest literary expression in John Miltons Paradise Lost.)
And was Zeus safe? He was attracted to the sea nymph Thetis and would have married her had he not been informed by the Fates that Thetis was destined to bear a son mightier than his father. That meant it was not safe for Zeus, or for any other god, either, to marry her. She was therefore forced (much against her will) to marry Peleus, a mortal, and bear a mortal son, the only child the myths describe her as having. That son was Achilles, who was certainly far mightier than his father (and, like Talos, had only his heel as his weak point through which he might be killed).
Now, then, translate this fear of irreversible change and of being supplanted into the relationship of man and machine and what do we have? Surely the great fear is not that machinery will harm usbut that it will supplant us. It is not that it will render us ineffectivebut that it will make us obsolete.

The ultimate machine is an intelligent machine and there is, only one basic plot to the intelligent-machine storythat it is created to serve man, but that it ends by dominating man. It cannot exist without threatening to supplant us, and it must therefore be destroyed or we will be.
There is the danger of the broom of the sorcerers apprentice, the golem of Rabbi Loew, the monster created by Dr. Frankenstein. As the child born of our body eventually supplants us, so does the machine born of our mind.
Mary Shelleys Frankenstein, which appeared in 1818, represents a peak of fear, however, for, as it happened, circumstances conspired to reduce that fear, at least temporarily.
Between the year 1815, which saw the end of a series of general European wars, and 1914, which saw the beginning of another, there was a brief period in which humanity could afford the luxury of optimism concerning its relationship to the machine. The Industrial Revolution seemed suddenly to uplift human power and to bring on dreams of a technological utopia on Earth in place of the mythic one in Heaven. The good of machines seemed to far outbalance the evil and the response of love far outbalance the response of fear.
It was in that interval that modern science fiction beganand by modern science fiction I refer to a form of literature that deals with societies differing from our own specifically in the level of science and technology, and into which we might conceivably pass from our own society by appropriate changes in that level. (This differentiates science fiction from fantasy or from speculative fiction, in which the fictional society cannot be connected with our own by any rational set of changes.)
Modern science fiction, because of the time of its beginning, took on an optimistic note. Mans relationship to the machine was one of use and control. Mans power grew and mans machines were his faithful tools, bringing him wealth and security and carrying him to the farthest reaches of the universe.
This optimistic note continues to this day, particularly among those writers who were molded in the years before the coming of the fission bombnotably, Robert Heinlein, Arthur C. Clarke, and myself.
Nevertheless, with World War I, disillusionment set in. Science and technology, which promised an Eden, turned out to be capable of delivering Hell was well. The beautiful airplane that fulfilled the age-old dream of flight could deliver bombs. The chemical techniques that produced anesthetics, dyes, and medicines produced poison gas as well.
The fear of supplantation rose again. In 1921, not long after the end of World War I, Karel Capeks drama R.U.R. appeared and it was the tale of Frankenstein again, escalated to the planetary level. Not a single monster was created but millions of robots (Capeks word, meaning worker, a mechanical one, that is). And it was not a single monster turning upon his single creator, but robots turning on humanity, wiping them out and supplanting them.
From the beginning of the science fiction magazine in 1926 to 1959 (a third of a century or a generation) optimism and pessimism battled each other in science fiction, with optimismthanks chiefly to the influence of John W. Campbell, Jr.having the better of it.
Beginning in 1939, I wrote a series of influential robot stories that self-consciously combated the Frankenstein complex and made of the robots the servants, friends, and allies of humanity.
It was pessimism, however, that won in the end, and for two reasons:
First, machinery grew more frightening. The fission bomb threatened physical destruction, of course, but worse still was the rapidly advancing electronic computer. Those computers seemed to steal the human soul. Deftly they solved our routine problems and more and more we found ourselves placing our questions in the hands of these machines with increasing faith, and accepting their answers with increasing humility.
All that fission and fusion bombs can do is destroy us, the computer might supplant us.
The second reason is more subtle, for it involved a change in the nature of the science fiction writer.
Until 1959, there were many branches of fiction, with science fiction perhaps the least among them. It brought its writers less in prestige and money than almost any other branch, so that no one wrote science fiction who wasnt so fascinated by it that he was willing to give up any chance at fame and fortune for its sake. Often that fascination stemmed from an absorption in the romance of science so that science fiction writers would naturally picture men as winning the universe by learning to bend it to their will.
In the 19508, however, competition with TV gradually killed the magazines that supported fiction, and by the time the 1960s arrived the only form of fiction that was flourishing, and even expanding, was science fiction. Its magazines continued and an incredible paperback boom was initiated. To a lesser extent it invaded movies and television, with its greatest triumphs undoubtedly yet to come.
This meant that in the 1960s and 19708, young writers began to write science fiction not because they wanted to, but because it was thereand because very little else was there. It meant that many of the new generation of science fiction writers had no knowledge of science, no sympathy for itand were in fact rather hostile to it. Such writers were far more ready to accept the fear half of the love/fear relationship of man to machine.
As a result, contemporary science fiction, far more often than not, is presenting us, over and over, with the myth of the child supplanting the parent, Zeus supplanting Kronos, Satan supplanting God, the machine supplanting humanity.
Nightmares they are, and they are to be read as such.
But allow me my own cynical commentary at the end. Remember that although Kronos foresaw the danger of being supplanted, and though he destroyed his children to prevent ithe was supplanted anyway, and rightly so, for Zeus was the better ruler.
So it may be that although we will hate and fight the machines, we will be supplanted anyway, and rightly so, for the intelligent machines to which we will give birth may, better than we, carry on the striving toward the goal of understanding and using the universe, climbing to heights we ourselves could never aspire to.

The New Profession

Back in 1940, I wrote a story in which the leading character was named Susan Calvin. (Good heavens, thats nearly half a century ago.) She was a robopsychologist by profession and knew everything there was to know about what made robots tick. It was a science fiction story, of course. I wrote other stories about Susan Calvin over the next few years, and as I described matters, she was born in 1982, went to Columbia, majored in robotics, and graduated in 2003. She went on to do graduate work and by 2010 was working at a firm called U.S. Robots and Mechanical Men, Inc. I didnt really take any of this seriously at the time I wrote it. What I was writing was just science fiction.
Oddly enough, however, its working out. Robots are in use on the assembly lines and are increasing in importance each year. The automobile companies are installing them in their factories by the tens of thousands. Increasingly, they will appear elsewhere as well, while ever more complex and intelligent robots will be appearing on the drawing boards. Naturally, these robots are going to wipe out many jobs, but they are going to create jobs, too. The robots will have to be designed, in the first place. They will have to be constructed and installed. Then, since nothing is perfect, they will occasionally go wrong and have to be repaired. To keep the necessity for repair to a minimum, they will have to be intelligently maintained. They may even have to be modified to do their work differently on occasion.
To do all this, we will need a group of people whom we can call, in general, robot technicians. There are some estimates that by the time my fictional Susan Calvin gets out of college, there will be over 2 million robot technicians in the United States alone, and perhaps 6 million in the world generally. Susan wont be alone. To these technicians, suppose we add all the other people that will be employed by those rapidly growing industries that are directly or indirectly related to robotics. It may well turn out that the robots will create more jobs than they will wipe outbut, of course, the two sets of jobs will be different, which means there will be a difficult transition period in which those whose jobs have vanished are retrained so that they can fill new jobs that have appeared.
This may not be possible in every case, and there will have to be innovative social initiatives to take care of those who, because of age or temperament, cannot fit in to the rapidly changing economic scene.
In the past, advances in technology have always necessitated the upgrading of education. Agricultural laborers didnt have to be literate, but factory workers did, so once the Industrial Revolution came to pass, industrialized nations had to establish public schools for the mass education of their populations. There must now be a further advance in education to go along with the new high-tech economy. Education in science and technology will have to be taken more seriously and made lifelong, for advances will occur too rapidly for people to be able to rely solely on what they learned as youngsters.
Wait! I have mentioned robot technicians, but that is a general term. Susan Calvin was not a robot technician; she was, specifically, a robopsychologist. She dealt with robotic intelligence, with robots ways of thinking. I have not yet heard anyone use that term in real life, but I think the time will come when it will be used, just as robotics was used after I had invented that term. After all, robot theoreticians are trying to develop robots that can see, that can understand verbal instructions, that can speak in reply. As robots are expected to do more and more tasks, more and more efficiently, and in a more and more versatile way, they will naturally seem more intelligent. In fact, even now, there are scientists at MIT and elsewhere who are working very seriously on the question of artificial intelligence.
Still, even if we design and construct robots that can do their jobs in such a way as to seem intelligent, it is scarcely likely that they will be intelligent in the same way that human beings are. For one thing, their brains will be constructed of materials different from the ones in our brains. For another, their brains will be made up of different components hooked together and organized in different ways, and will approach problems (very likely) in a totally different manner.
Robotic intelligence may be so different from human intelligence that it will take a new discipline-robopsychologyto deal with it. That is where Susan Calvin will come in. It is she and others like her who will deal with robots, where ordinary psychologists could not begin to do so. And this might turn out to be the most important aspect of robotics, for if we study in detail two entirely different kinds of intelligence, we may learn to understand intelligence in a much more general and fundamental way than is now possible. Specifically, we will learn more about human intelligence than may be possible to learn from human intelligence alone.

The Robot As Enemy?

It was back in 1942 that I invented the Three Laws of Robotics, and of these, the First Law is, of course, the most important. It goes as follows:  A robot may not injure a human being, or, through inaction, allow a human being to come to harm. In my stories, I always make it clear that the Laws, especially the First Law, are an inalienable part of all robots and that robots cannot and do not disobey them.
I also make it clear, though perhaps not as forcefully, that these Laws arent inherent in robots. The ores and raw chemicals of which robots are formed do not already contain the Laws. The Laws are there only because they are deliberately added to the design of the robotic brain, that is, to the computers that control and direct robotic action. Robots can fail to possess the Laws, either because they are too simple and crude to be given behavior patterns sufficiently complex to obey them or because the people designing the robots deliberately choose not to include the Laws in their computerized makeup.
So farand perhaps it will be so for a considerable time to comeit is the first of these alternatives that holds sway. Robots are simply too crude and primitive to be able to foresee that an act of theirs will harm a human being and to adjust their behavior to avoid that act. They are, so far, only computerized levers capable of a few types of rote behavior, and they are unable to step beyond the very narrow limits of their instructions. As a result, robots have already killed human beings, just as enormous numbers of noncomputerized machines have. It is deplorable but understandable, and we can suppose that as robots are developed with more elaborate sense perceptions and with the capability of more flexible responses, there will be an increasing likelihood of building safety factors into them that will be the equivalent of the Three Laws.
But what about the second alternative? Will human beings deliberately build robots without the Laws? Im afraid that is a distinct possibility. People are already talking about security Robots. There could be robot guards patrolling the grounds of a building or even its hallways. The function of these robots could be to challenge any person entering the grounds or the building. Presumably, persons who belonged there, or who were invited there, would be carrying (or would be given) some card or other form of identification that would be recognized by the robot, who would then let them pass. In our security-conscious times, this might even seem a good thing. It would cut down on vandalism and terrorism and it would, after all, only be fulfilling the function of a trained guard dog.
But security breeds the desire for more security. Once a robot became capable of stopping an intruder, it might not be enough for it merely to sound an alarm. It would be tempting to endow the robot with the capability of ejecting the intruder, even if it would do injury in the processjust as a dog might injure you in going for your leg or throat. What would happen, though, when the chairman of the board found he had left his identifying card in his other pants and was too upset to leave the building fast enough to suit the robot? Or what if a child wandered into the building without the proper clearance? I suspect that if the robot roughed up the wrong person, there would be an immediate clamor to prevent a repetition of the error.
To go to a further extreme, there is talk of robot weapons: computerized planes, tanks, artillery, and so on, that would stalk the enemy relentlessly, with superhuman senses and stamina. It might be argued that this would be a way of sparing human beings. We could stay comfortably at home and let our intelligent machines do the fighting for us. If some of them were destroyedwell, they are only machines. This approach to warfare would be particularly useful if we had such machines and the enemy didnt.
But even so, could we be sure that our machines could always tell an enemy from a friend? Even when all our weapons are controlled by human hands and human brains, there is the problem of friendly fire.  American weapons can accidentally kill American soldiers or civilians and have actually done so in the past. This is human error, but nevertheless its hard to take. But what if our robot weapons were to accidentally engage in friendly fire and wipe out American people, or even just American property? That would be far harder to take (especially if the enemy had worked out stratagems to confuse our robots and encourage them to hit our own side). No, I feel confident that attempts to use robots without safeguards wont work and that, in the end, we will come round to the Three Laws.

Intelligences Together

In Our Intelligent Tools I mentioned the possibility that robots might become so intelligent that they would eventually replace us. I suggested, with a touch of cynicism, that in view of the human record, such a replacement might be a good thing. Since then, robots have rapidly become more and more important in industry, and, although they are as yet quite idiotic on the intelligence scale, they are advancing quickly.
Perhaps, then, we ought to take another look at the matter of robots (or computerswhich are the actual driving mechanism of robots) replacing us. The outcome, of course, depends on how intelligent computers become and whether they will become so much more intelligent than we are that they will regard us as no more than pets, at best, or vermin, at worst. This implies that intelligence is a simple thing that can be measured with something like a ruler or a thermometer (or an IQ test) and then expressed in a single number. If the average human being is measured as 100 on an overall intelligence scale, then as soon as the average computer passes 100, we will be in trouble.
Is that the way it works, though? Surely there must be considerable variety in such a subtle quality as intelligence; different species of it, so to speak. I presume it takes intelligence to write a coherent essay, to choose the right words, and to place them in the right order. I also presume it takes intelligence to study some intricate technical device, to see how it works and how it might be improvedor how it might be repaired if it had stopped working. As far as writing is concerned, my intelligence is extremely high; as far as tinkering is concerned, my intelligence is extremely low. Well, then, am I a genius or an imbecile? The answer is: neither. Im just good at some things and not good at othersand thats true of everyone of us.
Suppose, then, we think about the origins of both human intelligence and computer intelligence. The human brain is built up essentially of proteins and nucleic acids; it is the product of over 3 billion years of hit-or-miss evolution; and the driving forces of its development have been adaptation and survival. Computers, on the other hand, are built up essentially of metal and electron surges; they are the product of some forty years of deliberate human design and development; and the driving force of their development has been the human desire to meet perceived human needs. If there are many aspects and varieties of intelligence among human beings themselves, isnt it certain that human and computer intelligences are going to differ widely since they have originated and developed under such different circumstances, out of such different materials, and under the impulse of such different drives?
It would seem that computers, even comparatively simple and primitive specimens, are extraordinarily good in some ways. They possess capacious memories, have virtually instant and unfailing recall, and demonstrate the ability to carry through vast numbers of repetitive arithmetical operations without weariness or error. If that sort of thing is the measure of intelligence, then already computers are far more intelligent than we are. It is because they surpass us so greatly that we use them in a million different ways and know that our economy would fall apart if they all stopped working at once.
But such computer ability is not the only measure of intelligence. In fact, we consider that ability of so little value that no matter how quick a computer is and how impressive its solutions, we see it only as an overgrown slide rule with no true intelligence at all. What the human specialty seems to be, as far as intelligence is concerned, is the ability to see problems as a whole, to grasp solutions through intuition or insight; to see new combinations; to be able to make extraordinarily perceptive and creative guesses. Cant we program a computer to do the same thing? Not likely, for we dont know how we do it.
It would seem, then, that computers should get better and better in their variety of point-by-point, short-focus intelligence, and that human beings (thanks to increasing knowledge and understanding of the brain and the growing technology of genetic engineering) may improve in their own variety of whole-problem, long-focus intelligence. Each variety of intelligence has its advantages and, in combination, human intelligence and computer intelligenceeach filling in the gaps and compensating for the weaknesses of the othercan advance far more rapidly than either one could alone. It will not be a case of competing and replacing at all, but of intelligences together, working more efficiently than either alone within the laws of nature.

My Robots

I wrote my first robot story, Robbie, in May of 1939, when I was only nineteen years old.
What made it different from robot stories that had been written earlier was that I was determined not to make my robots symbols. They were not to be symbols of humanitys overweening arrogance. They were not to be examples of human ambitions trespassing on the domain of the Almighty. They were not to be a new Tower of Babel requiring punishment.
Nor were the robots to be symbols of minority groups. They were not to be pathetic creatures that were unfairly persecuted so that I could make Aesopic statements about Jews, Blacks or any other mistreated members of society. Naturally, I was bitterly opposed to such mistreatment and I made that plain in numerous stories and essaysbut not in my robot stories.
In that case, what did I make my robots?I made them engineering devices. I made them tools. I made them machines to serve human ends. And I made them objects with built-in safety features. In other words, I set it up so that a robot could not kill his creator, and having outlawed that heavily overused plot, I was free to consider other, more rational consequences.
Since I began writing my robot stories in 1939, I did not mention computerization in their connection. The electronic computer had not yet been invented and I did not foresee it. I did foresee, however, that the brain had to be electronic in some fashion. However, electronic didnt seem futuristic enough. The positrona subatomic particle exactly like the electron but of opposite electric chargehad been discovered only four years before I wrote my first robot story. It sounded very science fictional indeed, so I gave my robots positronic brains and imagined their thoughts to consist of flashing streams of positrons, coming into existence, then going out of existence almost immediately. These stories that I wrote were therefore called the positronic robot series, but there was no greater significance than what I have just described to the use of positrons rather than electrons.
At first, I did not bother actually systematizing, or putting into words, just what the safeguards were that I imagined to be built into my robots. From the very start, though, since I wasnt going to have it possible for a robot to kill its creator, I had to stress that robots could not harm human beings; that this was an ingrained part of the makeup of their positronic brains.
Thus, in the very first printed version of Robbie, I had a character refer to a robot as follows: He just cant help being faithful and loving and kind. Hes a machine, made so.
After writing Robbie, which John Campbell, of Astounding Science Fiction, rejected, I went on to other robot stories which Campbell accepted. On December 23, 1940, I came to him with an idea for a mind-reading robot (which later became Liar!) and John was dissatisfied with my explanations of why the robot behaved as it did. He wanted the safeguard specified precisely so that we could understand the robot. Together, then, we worked out what came to be known as the Three Laws of Robotics.  The concept was mine, for it was obtained out of the stories I had already written, but the actual wording (if I remember correctly) was beaten out then and there by the two of us.
The Three Laws were logical and made sense. To begin with, there was the question of safety, which had been foremost in my mind when I began to write stories about my robots. Whats more I was aware of the fact that even without actively attempting to do harm, one could quietly, by doing nothing, allow harm to come. What was in my mind was Arthur Hugh Cloughs cynical The Latest Decalog, in which the Ten Commandments ate rewritten in deeply satirical Machiavellian fashion. The one item most frequently quoted is: Thou shalt not kill, but needst not strive / Officiously to keep alive.
For that reason I insisted that the First Law (safety) had to be in two parts and it came out this way:
1. A robot may not injure a human being, or, through inaction, allow a human being to come to harm.
Having got that out of the way, we had to pass on to the second law (service). Naturally, in giving the robot the built-in necessity to follow orders, you couldnt forfeit the overall concern of safety. The Second Law had to read as follows, then:
2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.
And finally, we had to have a third law (prudence). A robot was bound to be an expensive machine and it must not needlessly be damaged or destroyed. Naturally, this must not be used as a way of compromising either safety or service. The Third Law, therefore, had to read as follows:
3. A robot must protect its own existence, as long as such protection does not conflict with the First or Second Laws.
Of course, these laws are expressed in words, which is an imperfection. In the positronic brain, they are competing positronic potentials that are best expressed in terms of advanced mathematics (which is well beyond my ken, I assure you). However, even so, there are clear ambiguities. What constitutes harm to a human being? Must a robot obey orders given it by a child, by a madman, by a malevolent human being? Must a robot give up its own expensive and useful existence to prevent a trivial harm to an unimportant human being? What is trivial and what is unimportant?
These ambiguities are not shortcomings as far as a writer is concerned. If the Three Laws were perfect and unambiguous there would be no room for stories. It is in the nooks and crannies of the ambiguities that all ones plots can lodge, and which provide a foundation, if youll excuse the pun, for Robot City.
I did not specifically state the Three Laws in words in Liar! which appeared in the May 1941 Astounding. I did do so, however, in my next robot story, Runaround, which appeared in the March 1942 Astounding. In that issue on line seven of page one hundred, I have a character say, Now, look, lets start with the three fundamental Rules of Robotics, and I then quote them. That incidentally, as far as I or anyone else has been able to tell, represents the first appearance in print of the word roboticswhich, apparently, I invented.
Since then, I have never had occasion, over a period of over forty years during which I wrote many stories and novels dealing with robots, to be forced to modify the Three Laws. However, as time passed, and as my robots advanced in complexity and versatility, I did feel that they would have to reach for something still higher. Thus, in Robots and Empire, a novel published by Doubleday in 1985, I talked about the possibility that a sufficiently advanced robot might feel it necessary to consider the prevention of harm to humanity generally as taking precedence over the prevention of harm to an individual. This I called the Zeroth Law of Robotics, but Im still working on that.
My invention of the Three Laws of Robotics is probably my most important contribution to science fiction. They are widely quoted outside the field, and no history of robotics could possibly be complete without mention of the Three Laws. In 1985, John Wiley and Sons published a huge tome, Handbook of Industrial Robotics, edited by Shimon Y. Nof, and, at the editors request, I wrote an introduction concerning the Three Laws.
Now it is understood that science fiction writers generally have created a pool of ideas that form a common stock into which all writers can dip. For that reason, I have never objected to other writers who have used robots that obey the Three Laws. I have, rather, been flattered and, honestly, modem science fictional robots can scarcely appear without those Laws.
However, I have firmly resisted the actual quotation of the Three Laws by any other writer. Take the Laws for granted, is my attitude in this matter, but dont recite them. The concepts are everyones but the words are mine.

The Laws Of Humanics

My first three robot novels were, essentially, murder mysteries, with Elijah Baley as the detective. Of these first three, the second novel, The Naked Sun, was a locked-room mystery, in the sense that the murdered person was found with no weapon on the site and yet no weapon could have been removed either.
I managed to produce a satisfactory solution but I did not do that sort of thing again.
The fourth robot novel, Robots and Empire, was not primarily a murder mystery. Elijah Baley had died a natural death at a good, old age, the book veered toward the Foundation universe so that it was clear that both my notable series, the Robot series and the Foundation series, were going to be fused into a broader whole. (No, I didnt do this for some arbitrary reason. The necessities arising out of writing sequels in the 1980s to tales originally written in the 19408 and 1950s forced my hand.)
In Robots and Empire, my robot character, Giskard, of whom I was very fond, began to concern himself with the Laws of Humanics, which, I indicated, might eventually serve as the basis for the science of psychohistory, which plays such a large role in the Foundation series.
Strictly speaking, the Laws of Humanics should be a description, in concise form, of how human beings actually behave. No such description exists, of course. Even psychologists, who study the matter scientifically (at least, I hope they do) cannot present any laws but can only make lengthy and diffuse descriptions of what people seem to do. And none of them are prescriptive. When a psychologist says that people respond in this way to a stimulus of that sort, he merely means that some do at some times. Others may do it at other times, or may not do it at all.
If we have to wait for actual laws prescribing human behavior in order to establish psychohistory (and surely we must) then I suppose we will have to wait a long time.
Well, then, what are we going to do about the Laws of Humanics? I suppose what we can do is to start in a very small way, and then later slowly build it up, if we can.
Thus, in Robots and Empire, it is a robot, Giskard, who raises the question of the Laws of Humanics. Being a robot, he must view everything from the standpoint of the Three Laws of Roboticsthese robotic laws being truly prescriptive, since robots are forced to obey them and cannot disobey them.
The Three Laws of Robotics are:
1A robot may not injure a human being, or, through inaction, allow a human being to come to harm.
2A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.
3A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.
Well, then, it seems to me that a robot could not help but think that human beings ought to behave in such a way as to make it easier for robots to obey those laws.
In fact, it seems to me that ethical human beings should be as anxious to make life easier for robots as the robots themselves would. I took up this matter in my story The Bicentennial Man, which was published in 1976. In it, I had a human character say in part:
If a man has the right to give a robot any order that does not involve harm to a human being, he should have the decency never to give a robot any order that involves harm to a robot, unless human safety absolutely requires it. With great power goes great responsibility, and if the robots have Three Laws to protect men, is it too much to ask that men have a law or two to protect robots?
For instance, the First Law is in two parts. The first part, A robot may not injure a human being, is absolute and nothing need be done about that. The second part, or, through inaction, allow a human being to come to harm, leaves things open a bit. A human being might be about to come to harm because of some event involving an inanimate object. A heavy weight might be likely to fall upon him, or he may slip and be about to fall into a lake, or anyone of uncountable other misadventures of the sort may be involved. Here the robot simply must try to rescue the human being; pull him from under, steady him on his feet and so on. Or a human being might be threatened by some form of life other than humana lion, for instanceand the robot must come to his defense.
But what if harm to a human being is threatened by the action of another human being? There a robot must decide what to do. Can he save one human being without harming the other? Or if there must be harm, what course of action must he pursue to make it minimal?
It would be a lot easier for the robot, if human beings were as concerned about the welfare of human beings, as robots are expected to be. And, indeed, any reasonable human code of ethics would instruct human beings to care for each other and to do no harm to each other. Which is, after all, the mandate that humans gave robots. Therefore the First Law of Humanics from the robots standpoint is:
1A human being may not injure another human being, or, through inaction, allow a human being to come to harm.
If this law is carried through, the robot will be left guarding the human being from misadventures with inanimate objects and with non-human life, something which poses no ethical dilemmas for it. Of course, the robot must still guard against harm done a human being unwittingly by another human being. It must also stand ready to come to the aid of a threatened human being, if another human being on the scene simply cannot get to the scene of action quickly enough. But then, even a robot may unwittingly harm a human being, and even a robot may not be fast enough to get to the scene of action in time or skilled enough to take the necessary action. Nothing is perfect.
That brings us to the Second Law of Robotics, which compels a robot to obey all orders given it by human beings except where such orders would conflict with the First Law. This means that human beings can give robots any order without limitation as long as it does not involve harm to a human being.
But then a human being might order a robot to do something impossible, or give it an order that might involve a robot in a dilemma that would do damage to its brain. Thus, in my short story Liar!, published in 1940, I had a human being deliberately put a robot into a dilemma where its brain burnt out and ceased to function.
We might even imagine that as a robot becomes more intelligent and self-aware, its brain might become sensitive enough to undergo harm if it were forced to do something needlessly embarrassing or undignified. Consequently, the Second Law of Humanics would be:
2A human being must give orders to a robot that preserve robotic existence, unless such orders cause harm or discomfort to human beings.
The Third Law of Robotics is designed to protect the robot, but from the robotic view it can be seen that it does not go far enough. The robot must sacrifice its existence if the First or Second Law makes that necessary. Where the First Law is concerned, there can be no argument. A robot must give up its existence if that is the only way it can avoid doing harm to a human being or can prevent harm from coming to a human being. If we admit the innate superiority of any human being to any robot (which is something I am a little reluctant to admit, actually), then this is inevitable.
On the other hand, must a robot give up its existence merely in obedience to an order that might be trivial, or even malicious? In The Bicentennial Man, I have some hoodlums deliberately order a robot to take itself apart for the fun of watching that happen. The Third Law of Humanics must therefore be:
3A human being must not harm a robot, or, through inaction, allow a robot to come to harm, unless such harm is needed to keep a human being from harm or to allow a vital order to be carried out.
Of course, we cannot enforce these laws as we can the Robotic Laws. We cannot design human brains as we design robot brains. It is, however, a beginning, and I honestly think that if we are to have power over intelligent robots, we must feel a corresponding responsibility for them, as the human character in my story The Bicentennial Man said.

Cybernetic Organism

A robot is a robot and an organism is an organism.
An organism, as we all know, is built up of cells. From the molecular standpoint, its key molecules are nucleic acids and proteins. These float in a watery medium, and the whole has a bony support system. If is useless to go on with the description, since we are all familiar with organisms and since we are examples of them ourselves.
A robot, on the other hand, is (as usually pictured in science fiction) an object, more or less resembling a human being, constructed out of strong, rust-resistant metal. Science fiction writers are generally chary of describing the robotic details too closely since they are not usually essential to the story and the writers are generally at a loss how to do so.
The impression one gets from the stories, however, is that a robot is wired, so that it has wires through which electricity flows rather than tubes through which blood flows. The ultimate source of power is either unnamed, or is assumed to partake of the nature of nuclear power.
What of the robotic brain?
When I wrote my first few robot stories in 1939 and 1940, I imagined a positronic brain of a spongy type of platinum-iridium alloy. It was platinum-iridium because that is a particularly inert metal and is least likely to undergo chemical changes. It was spongy so that it would offer an enormous surface on which electrical patterns could be formed and un-formed. It was positronic because four years before my first robot story, the positron had been discovered as a reverse kind of electron, so that positronic in place of electronic had a delightful science-fiction sound.
Nowadays, of course, my positronic platinum-iridium brain is hopelessly archaic. Even ten years after its invention it became outmoded. By the end of the 1940s, we came to realize that a robots brain must be a kind of computer. Indeed, if a robot were to be as complex as the robots in my most recent novels, the robot brain-computer must be every bit as complex as the human brain. It must be made of tiny microchips no larger than, and as complex as, brain cells.
But now let us try to imagine something that is neither organism nor robot, but a combination of the two. Perhaps we can think of it as an organism-robot or orbot. That would clearly be a poor name, for it is only robot with the first two letters transposed. To say orgabot, instead, is to be stuck with a rather ugly word.
We might call it a robot-organism, or a robotanism, which, again, is ugly or roborg. To my ears, roborg doesnt sound bad, but we cant have that. Something else has arisen.
The science of computers was given the name cybernetics by Norbert Weiner a generation ago, so that if we consider something that is part robot and part organism and remember that a robot is cybernetic in nature, we might think of the mixture as a cybernetic organism, or a cyborg. In fact, that is the name that has stuck and is used.
To see what a cyborg might be, lets try starting with a human organism and moving toward a robot; and when we are quite done with that, lets start with a robot and move toward a human being.
To move from a human organism toward a robot, we must begin replacing portions of the human organism with robotic parts. We already do that in some ways. For instance, a good percentage of the original material of my teeth is now metallic, and metal is, of course, the robotic substance par excellence.
The replacements dont have to be metallic, of course. Some parts of my teeth are now ceramic in nature, and cant be told at a glance from the natural dentine. Still, even though dentine is ceramic in appearance and even, to an extent, in chemical structure, it was originally laid down by living material and bears the marks of its origin. The ceramic that has replaced the dentine shows no trace of life, now or ever.
We can go further. My breastbone, which had to be split longitudinally in an operation a few years back is now held together by metallic staples, which have remained in place ever since. My sister-in-law has an artificial hip-joint replacement. There are people who have artificial arms or legs and such non-living limbs are being designed, as time passes on, to be ever more complex and useful. There are people who have lived for days and even months with artificial hearts, and many more people who live for years with pacemakers.
We can imagine, little by little, this part and that part of the human being replaced by inorganic materials and engineering devices. Is there any part which we would find difficult to replace, even in imagination?
I dont think anyone would hesitate there. Replace every part of the human being but onethe limbs, the heart, the liver, the skeleton, and so onand the product would remain human. It would be a human being with artificial parts, but it would be a human being.
But what about the brain?
Surely, if there is one thing that makes us human it is the brain. If there is one thing that makes us a human individual, it is the intensely complex makeup, the emotions, the learning, the memory content of our particular brain. You cant simply replace a brain with a thinking device off some factory shelf. You have to put in something that incorporates all that a natural brain has learned, that possesses all its memory, and that mimics its exact pattern of working.
An artificial limb might not work exactly like a natural one, but might still serve the purpose. The same might be true of an artificial lung, kidney, or liver. An artificial brain, however, must be the precise replica of the brain it replaces, or the human being in question is no longer the same human being.
It is the brain, then, that is the sticking point in going from human organism to robot.
And the reverse?
In The Bicentennial Man, I described the passage of my robot-hero, Andrew Martin, from robot to man. Little by little, he had himself changed, till his every visible part was human in appearance. He displayed an intelligence that was increasingly equivalent (or even superior) to that of a man. He was an artist, a historian, a scientist, an administrator. He forced the passage of laws guaranteeing robotic rights, and achieved respect and admiration in the fullest degree.
Yet at no point could he make himself accepted as a man. The sticking point, here, too, was his robotic brain. He found that he had to deal with that before the final hurdle could be overcome.
Therefore, we come down to the dichotomy, body and brain. The ultimate cyborgs are those in which the body and brain dont match. That means we can have two classes of complete cyborgs:
a) a robotic brain in a human body, or
b) a human brain in a robotic body.
We can take it for granted that in estimating the worth of a human being (or a robot, for that matter) we judge first by superficial appearance.
I can very easily imagine a man seeing a woman of superlative beauty and gazing in awe and wonder at the sight. What a beautiful woman, he will say, or think, and he could easily imagine himself in love with her on the spot. In romances, I believe that happens as a matter of routine. And, of course, a woman seeing a man of superlative beauty is surely likely to react in precisely the same way.
If you fall in love with a striking beauty, you are scarcely likely to spend much time asking if she (or he, of course) has any brains, or possesses a good character, or has good judgment or kindness or warmth. If you find out eventually that good looks are the persons only redeeming quality, you are liable to make excuses and continue to be guided, for a time at least, by the conditioned reflex of erotic response. Eventually, of course, you will tire of good looks without content, but who knows how long that will take?
On the other hand, a person with a large number of good qualities who happened to be distinctly plain might not be likely to entangle you in the first place unless you were intelligent enough to see those good qualities so that you might settle down to a lifetime of happiness.
What I am saying, then, is that a cyborg with a robotic brain in a human body is going to be accepted by most, if not all, people as a human being; while a cyborg with a human brain in a robotic body is going to be accepted by most, if not all, people as a robot. You are, after allat least to most peoplewhat you seem to be.
These two diametrically opposed cyborgs will not, however, pose a problem to human beings to the same degree.
Consider the robotic brain in the human body and ask why the transfer should be made. A robotic brain is better off in a robotic body since a human body is far the more fragile of the two. You might have a young and stalwart human body in which the brain has been damaged by trauma and disease, and you might think, Why waste that magnificent human body? Lets put a robotic brain in it so that it can live out its life.
If you were to do that, the human being that resulted would not be the original. It would be a different individual human being. You would not be conserving an individual but merely a specific mindless body. And a human body, however fine, is (without the brain that goes with it) a cheap thing. Every day, half a million new bodies come into being. There is no need to save anyone of them if the brain is done.
On the other hand, what about a human brain in a robotic body? A human brain doesnt last forever, but it can last up to ninety years without falling into total uselessness. It is not at all unknown to have a ninety-year-old who is still sharp, and capable of rational and worthwhile thought. And yet we also know that many a superlative mind has vanished after twenty or thirty years because the body that housed it (and was worthless in the absence of the mind) had become uninhabitable through trauma or disease. There would be a strong impulse then to transfer a perfectly good (even superior) brain into a robotic body to give it additional decades of useful life.
Thus, when we say cyborg we are very likely to think, just about exclusively, of a human brain in a robotic bodyand we are going to think of that as a robot.
We might argue that a human mind is a human mind, and that it is the mind that counts and not the surrounding support mechanism, and we would be right. Im sure that any rational court would decide that a human-brain cyborg would have all the legal rights of a man. He could vote, he must not be enslaved, and so on.
And yet suppose a cyborg were challenged: Prove that you have a human brain and not a robotic brain, before I let you have human rights.
The easiest way for a cyborg to offer the proof is for him to demonstrate that he is not bound by the Three Laws of Robotics. Since the Three Laws enforce socially acceptable behavior, this means he must demonstrate that he is capable of human (i.e. nasty) behavior. The simplest and most unanswerable argument is simply to knock the challenger down, breaking his jaw in the process, since no robot could do that. (In fact, in my story Evidence, which appeared in 1947, I use this as a way of proving someone is not a robotbut in that case there was a catch.)
But if a cyborg must continually offer violence in order to prove he has a human brain, that will not necessarily win him friends.
For that matter, even if he is accepted as human and allowed to vote and to rent hotel rooms and do all the other things human beings can do, there must nevertheless be some regulations that distinguish between him and complete human beings. The cyborg would be stronger than a man, and his metallic fists could be viewed as lethal weapons. He might still be forbidden to strike a human being, even in self-defense. He couldnt engage in various sports on an equal basis with human beings, and so on.
Ah, but need a human brain be housed in a metallic robotic body? What about housing it in a body made of ceramic and plastic and fiber so that it looks and feels like a human bodyand has a human brain besides?
But you know, I suspect that the cyborg will still have his troubles. Hell be different. No matter how small the difference is, people will seize upon it.
We know that people who have human brains and full human bodies sometimes hate each other because of a slight difference in skin pigmentation, or a slight variation in the shape of the nose, eyes, lips, or hair.
We know that people who show no difference in any of the physical characteristics that have come to represent a cause for hatred, may yet be at daggers-drawn over matters that are not physical at all, but culturaldifferences in religion, or in political outlook, or in place of birth, or in language, or in just the accent of a language.
Lets face it. Cyborgs will have their difficulties, no matter what.

The Sense Of Humor

Would a robot feel a yearning to be human?
You might answer that question with a counter-question. Does a Chevrolet feel a yearning to be a Cadillac?
The counter-question makes the unstated comment that a machine has no yearnings.
But the very point is that a robot is not quite a machine, at least in potentiality. A robot is a machine that is made as much like a human being as it is possible to make it, and somewhere there may be a boundary line that may be crossed.
We can apply this to life. An earthworm doesnt yearn to be a snake; a hippopotamus doesnt yearn to be an elephant. We have no reason to think such creatures are self-conscious and dream of something more than they are. Chimpanzees and gorillas seem to be self-aware, but we have no reason to think that they yearn to be human.
A human being, however, dreams of an afterlife and yearns to become one of the angels. Somewhere, life crossed a boundary line. At some point a species arose that was not only aware of itself but had the capacity to be dissatisfied with itself.
Perhaps a similar boundary line will someday be crossed in the construction of robots.
But if we grant that a robot might someday aspire to humanity, in what way would he so aspire? He might aspire to the possession of the legal and social status that human beings are born to. That was the theme of my story The Bicentennial Man, and in his pursuit of such status, my robot-hero was willing to give up all his robotic qualities, one by one, right down to his immortality.
That story, however, was more philosophical than realistic. What is there about a human being that a robot might properly envywhat human physical or mental characteristic? No sensible robot would envy human fragility, or human incapacity to withstand mild changes in the environment, or human need for sleep, or aptitude for the trivial mistake, or tendency to infectious and degenerative disease, or incapacitation through illogical storms of emotion.
He might, more properly, envy the human capacity for friendship and love, his wide-ranging curiosity, his eagerness for experience. I would like to suggest, though, that a robot who yearned for humanity might well find that what he would most want to understand, and most frustratingly fail to understand, would be the human sense of humor.
The sense of humor is by no means universal among human beings, though it does cut across all cultures. I have known many people who didnt laugh, but who looked at you in puzzlement or perhaps disdain if you tried to be funny. I need go no further than my father, who routinely shrugged off my cleverest sallies as unworthy of the attention of a serious man. (Fortunately, my mother laughed at all my jokes, and most uninhibitedly, or I might have grown up emotionally stunted.)
The curious thing about the sense of humor, however, is that, as far as I have observed, no human being will admit to its lack. People might admit they hate dogs and dislike children, they might cheerfully own up to cheating on their income tax or on their marital partner as a matter of right, and might not object to being considered inhumane or dishonest, through the simple expediency of switching adjectives and calling themselves realistic or businesslike.
However, accuse them of lacking a sense of humor and they will deny it hotly every time, no matter how openly and how often they display such a lack. My father, for instance, always maintained that he had a keen sense of humor and would prove it as soon as he heard a joke worth laughing at (though he never did, in my experience).
Why, then, do people object to being accused of humorlessness? My theory is that people recognize (subliminally, if not openly) that a sense of humor is typically human, more so than any other characteristic, and refuse demotion to subhumanity.
Only once did I take up the matter of a sense of humor in a science-fiction story, and that was in my story Jokester, which first appeared in the December, 1956 issue of Infinity Science Fiction and which was most recently reprinted in my collection The Best Science Fiction of Isaac Asimov (Doubleday, 1986).
The protagonist of the story spent his time telling jokes to a computer (I quoted six of them in the course of the story). A computer, of course, is an immobile robot; or, which is the same thing, a robot is a mobile computer; so the story deals with robots and jokes. Unfortunately, the problem in the story for which a solution was sought was not the nature of humor, but the source of all the jokes one hears. And there is an answer, too, but youll have to read the story for that.
However, I dont just write science fiction. I write whatever it falls into my busy little head to write, and (by some undeserved stroke of good fortune) my various publishers are under the weird impression that it is illegal not to publish any manuscript I hand them. (You can be sure that I never disabuse them of this ridiculous notion.)
Thus, when I decided to write a joke book, I did, and Houghton-Mifflin published it in 1971 under the title of Isaac Asimovs Treasury of Humor. In it, I told 640 jokes that I happened to have as part of my memorized repertoire. (I also have enough for a sequel to be entitled Isaac Asimov Laughs Again, but I cant seem to get around to writing it no matter how long I sit at the keyboard and how quickly I manipulate the keys.) I interspersed those jokes with my own theories concerning what is funny and how one makes what is funny even funnier.
Mind you, there are as many different theories of humor as there are people who write on the subject, and no two theories are alike. Some are, of course, much stupider than others, and I felt no embarrassment whatever in adding my own thoughts on the subject to the general mountain of commentary.
It is my feeling, to put it as succinctly as possible, that the one necessary ingredient in every successful joke is a sudden alteration in point of view. The more radical the alteration, the more suddenly it is demanded, the more quickly it is seen, the louder the laugh and the greater the joy.
Let me give you an example with a joke that is one of the few I made up myself:

Jim comes into a bar and finds his best friend, Bill, at a corner table gravely nursing a glass of beer and wearing a look of solemnity on his face. Jim sits down at the table and says sympathetically, Whats the matter, Bill?
Bill sighs, and says, My wife ran off yesterday with my best friend.
Jim says, in a shocked voice, What are you talking about, Bill? Im your best friend.
To which Bin answers softly, Not anymore.

I trust you see the change in point of view. The natural supposition is that poor Bill is sunk in gloom over a tragic loss. It is only with the last three words that you realize, quite suddenly, that he is, in actual fact, delighted. And the average human male is sufficiently ambivalent about his wife (however beloved she might be) to greet this particular change in point of view with delight.
Now, if a robot is designed to have a brain that responds to logic only (and of what use would any other kind of robot brain be to humans who are hoping to employ robots for their own purposes?), a sudden change in point of view would be hard to achieve. It would imply that the rules of logic were wrong in the first place or were capable of a flexibility that they obviously dont have. In addition, it would be dangerous to build ambivalence into a robot brain. What we want from him is decision and not the to-be-or-not-to-be of a Hamlet.
Imagine, then, telling a robot the joke I have just given you, and imagine the robot staring at you solemnly after you are done, and questioning you, thus.
Robot: But why is Jim no longer Bills best friend? You have not described Jim as doing anything that would cause Bill to be angry with him or disappointed in him.
You: Well, no, its not that Jim has done anything. Its that someone else has done something for Bill that was so wonderful, that he has been promoted over Jims head and has instantly become Bills new best friend.
Robot: But who has done this? You: The man who ran away with Bills wife, of course. Robot (after a thoughtful pause): But that cant be so. Bill must have felt profound affection for his wife and a great sadness over her loss. Is that not how human males feel about their wives, and how they would react to their loss?
You: In theory, yes. However, it turns out that Bill strongly disliked his wife and was glad someone had run off with her.
Robot (after another thoughtful pause): But you did not say that was so.
You: I know. Thats what makes it funny. I led you in one direction and then suddenly let you know that was the wrong direction.
Robot: Is it funny to mislead a person?
You (giving up): Well, lets get on with building this house.
In fact, some jokes actually depend on the illogical responses of human beings. Consider this one:

The inveterate horse player paused before taking his place at the betting windows, and offered up a fervent prayer to his Maker.
Blessed Lord, he murmured with mountain-moving sincerity. I know you dont approve of my gambling, but just this once, Lord, just this once, please let me break even. I need the money so badly.

If you were so foolish as to tell this joke to a robot, he would immediately say, But to break even means that he would leave the races with precisely the amount of money he had when he entered. Isnt that so?
Yes, thats so.
Then, if he needs the money so badly, all he need do is not bet at all, and it would be just as though he had broken even.
Yes, but he has this unreasoning need to gamble.
You mean even if he loses.
Yes.
But that makes no sense.
But the point of the joke is that the gambler doesnt understand this.
You mean its funny if a person lacks any sense of logic and is possessed of not even the simplest understanding?
And what can you do but turn back to building the house again?
But tell me, is this so different from dealing with the ordinary humorless human being? I once told my father this joke:

Mrs. Jones, the landlady, woke up in the middle of the night because there were strange noises outside her door. She looked out, and there was Robinson, one of her boarders, forcing a frightened horse up the stairs.
She shrieked, What are you doing, Mr. Robinson?
He said, Putting the horse in the bathroom.
For goodness sake, why?
Well, old Higginbotham is such a wise guy. Whatever I tell him, he answers, I know. I know, in such a superior way. Well, in the morning, hell go to the bathroom and hell come out yelling, Theres a horse in the bathroom. And Ill yawn and say, I know, I know. 

And what was my fathers response? He said, Isaac, Isaac. Youre a city boy, so you dont understand. You cant push a horse up the stairs if he doesnt want to go.
Personally, I thought that was funnier than the joke.
Anyway, I dont see why we should particularly want a robot to have a sense of humor, but the point is that the robot himself might want to have oneand how do we give it to him?

Robots In Combination

I have been inventing stories about robots now for very nearly half a century. In that time, I have rung almost every conceivable change upon the theme.
Mind you, it was not my intention to compose an encyclopedia of robot nuances; it was not even my intention to write about them for half a century. It just happened that I survived that long and maintained my interest in the concept. And it also just happened that in attempting to think of new story ideas involving robots, I ended up thinking about nearly everything.
For instance, in the sixth volume of the Robot City series, there are the chemfets, which have been introduced into the heros body in order to replicate and, eventually, give him direct psycho-electronic control over the core computer, and hence all the robots of Robot City.
Well, in my book Foundations Edge (Doubleday, 1982), my hero, Golan Trevize, before taking off in a spaceship, makes contact with an advanced computer by placing his hands on an indicated place on the desk before him.
And as he and the computer held hands, their thinking merged...
...he saw the room with complete claritynot just in the direction in which he was looking, but all around and above and below.
He saw every room in the spaceship, and he saw outside as well. The sun had risen...but he could look at it directly without being dazzled...
He felt the gentle wind and its temperature, and the sounds of the world about him. He detected the planets magnetic field and the tiny electrical charges on the wall of the ship.
He became aware of the controls of the ship...He knew...that if he wanted to lift the ship, or turn it, or accelerate, or make use of any of its abilities, the process was the same as that of performing the analogous process to his body. He had but to use his will.
That was as close as I could come to picturing the result of a mind-computer interface, and now, in connection with this new book, I cant help thinking of it further.
I suppose that the first time human beings learned how to form an interface between the human mind and another sort of intelligence was when they tamed the horse and learned how to use it as a form of transportation. This reached its highest point when human beings rode horses directly, and when a pull at a rein, the touch of a spur, a squeeze of the knees, or just a cry, could make the horse react in accordance with the human will.
It is no wonder that primitive Greeks seeing horsemen invade the comparatively broad Thessalian plains (the part of Greece most suitable to horsemanship) thought they were seeing a single animal with a human torso and a horses body. Thus was invented the centaur.
Again, there are trick drivers. There are expert stunt men who can make an automobile do marvelous things. One might expect that a New Guinea native who had never seen or heard of an automobile before might believe that such stunts were being carried through by a strange and Monstrous living organism that had, as part of its structure, a portion with a human appearance within its stomach.
But a person plus a horse is but an imperfect fusion of intelligence, and a person plus an automobile is but an extension of human muscles by mechanical linkages. A horse can easily disobey signals, or even run away in uncontrollable panic. And an automobile can break down or skid at an inconvenient moment.
The fusion of human and computer, however, ought to be a much closer approach to the ideal. It may be an extension of the mind itself as I tried to make plain in Foundations Edge, a multiplication and intensification of sense-perception, an incredible extension of the will.
Under such circumstances, might not the fusion represent, in a very real sense, a single organism, a kind of cybernetic centaur? And once such a union is established, would the human fraction wish to break it? Would he not feel such a break to be an unbearable loss and be unable to live with the impoverishment of mind and will he would then have to face? In my novel, Golan Trevize could break away from the computer at will and suffered no ill effects as a result, but perhaps that is not realistic.

Another issue that appears now and then in the Robot City series concerns the interaction of robot and robot.
This has not played a part in most of my stories, simply because I generally had a single robot character of importance in any given story and I dealt entirely with the matter of the interaction between that single robot and various human beings.
Consider robots in combination.
The First Law states that a robot cannot injure a human being or, through inaction, allow a human being to come to harm.
But suppose two robots are involved, and that one of them, through inadvertence, lack of knowledge, or special circumstances, is engaged in a course of action (quite innocently) that will clearly injure a human beingand suppose the second robot, with greater knowledge or insight, is aware of this. Would he not be required by the First Law to stop the first robot from committing the injury? If there were no other way, would he not be required by the First Law to destroy the first robot without hesitation or regret?
Thus, in my book Robots and Empire (Doubleday, 1985), a robot is introduced to whom human beings have been defined as those speaking with a certain accent. The heroine of the book does not speak with that accent and therefore the robot feels free to kill her. That robot is promptly destroyed by a second robot.
The situation is similar for the Second Law, in which robots are forced to obey orders given them by human beings provided those orders do not violate the First Law.
If, of two robots, one through inadvertence or lack of understanding does not obey an order, the second must either carry through the order itself, or force the first to do so.
Thus, in an intense scene in Robots and Empire, the villainess gives one robot a direct order. The robot hesitates because the order may cause harm to the heroine. For a while, then, there is a confrontation in which the villainess reinforces her own order while a second robot tries to reason the first robot into a greater realization of the harm that will be done to the heroine. Here we have a case where one robot urges another to obey the Second Law in a truer manner, and to withstand a human being in so doing.

It is the Third Law, however, that brings up the knottiest problem where robots in combination are concerned.
The Third Law states that a robot must protect its own existence, where that is consistent with the First and Second Laws.
But what if two robots are concerned? Is each merely concerned with its own existence, as a literal reading of the Third Law would make it seem? Or would each robot feel the need for helping the other maintain its own existence?
As I said, this problem never arose with me as long as I dealt with only one robot per story. (Sometimes there were other robots but they were distinctly subsidiary charactersmerely spear-carriers, so to speak.)
However, first in The Robots of Dawn (Doubleday, 1983), and then in its sequel Robots and Empire, I had two robots of equal importance. One of these was R. Daneel Olivaw, a humaniform robot (who could not easily be told from a human being) who had earlier appeared in The Caves of Steel (Ooubleday, 1954), and in its sequel, The Naked Sun (Ooubleday, 1957). The other was R. Giskard Reventlov, who had a more orthodox metallic appearance. Both robots were advanced to the point where their minds were of human complexity.
It was these two robots who were engaged in the struggle with the villainess, the Lady Vasilia. It was Giskard who (such were the exigencies of the plot) was being ordered by Vasilia to leave the service of Gladia (the heroine) and enter her own. And it was Daneel who tenaciously argued the point that Giskard ought to remain with Gladia. Giskard has the ability to exert a limited mental control over human beings, and Daneel points out that Vasilia ought to be controlled for Gladias safety. He even argues the good of humanity in the abstract (the Zeroth Law) in favor of such an action.
Daneels arguments weaken the effect of Vasilias orders, but not sufficiently. Giskard is made to hesitate, but cannot be forced to take action.
Vasilia, however, decides that Daneel is too dangerous; if he continues to argue, he might force Giskard his way. She therefore orders her own robots to inactivate Daneel and further orders Daneel not to resist. Daneel must obey the order and Vasilias robots advance to the task.
It is then that Giskard acts. Her four robots are inactivated and Vasilia herself crumples into a forgetful sleep. Later Daneel asks Giskard to explain what happened.
Giskard says, When she ordered the robots to dismantle you, friend Daneel, and showed a clear emotion of pleasure at the prospect, your need, added to what the concept of the Zeroth Law had already done, superseded the Second Law and rivaled the First Law. It was the combination of the Zeroth Law, psychohistory, my loyalty to Lady Gladia, and your need that dictated my action.
Daneel now argues that his own need (he being merely a robot) ought not to have influenced Giskard at all. Giskard obviously agrees, yet he says:
It is a strange thing, friend Daneel. I do not know how it came about...At the moment when the robots advanced toward you and Lady Vasilia expressed her savage pleasure, my positronic pathway pattern re-formed in an anomalous fashion. For a moment, I thought of youas a human beingand I reacted accordingly.
Daneel said, That was wrong.
Giskard said, I know that. And yetand yet, if it were to happen again, I believe the same anomalous change would take place again.
And Daneel cannot help but feel that if the situation were reversed, he, too, would act in the same way.
In other words, the robots had reached a stage of complexity where they had begun to lose the distinction between robots and human beings, where they could see each other as friends, and have the urge to save each others existence.


Isaac Asimov is the author of over 400 books, including three Hugo Award-winners and numerous bestsellers as well as countless stories and scientific essays. He was awarded the honor of Grand Master of Science Fiction by the Science Fiction Writers of America in 1985, and he is the man who coined the words robotics, positronic, and psychohistory. He and his wife live in New York City.

Ralph McQuarrie is known worldwide as the most influential designer of science fiction films. He has been responsible for the look of such movies as Star Wars, The Empire Strikes Back, Close Encounters of the Third Kind, and Star Trek IV. He has illustrated Isaac Asimovs bestseller Robot Dreams.
